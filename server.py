import os, subprocess, hashlib, urllib.parse, unicodedata, threading, time, json, re, sys, traceback, shutil, requests, random, mimetypes, sqlite3, gzip
from flask import Flask, jsonify, send_from_directory, request, Response, redirect, send_file, make_response
from flask_cors import CORS
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from collections import deque
from io import BytesIO

app = Flask(__name__)
CORS(app)

# MIME ÌÉÄÏûÖ Ï∂îÍ∞Ä Îì±Î°ù
if not mimetypes.types_map.get('.mkv'): mimetypes.add_type('video/x-matroska', '.mkv')
if not mimetypes.types_map.get('.ts'): mimetypes.add_type('video/mp2t', '.ts')
if not mimetypes.types_map.get('.tp'): mimetypes.add_type('video/mp2t', '.tp')

# --- [ÏµúÏ†ÅÌôî: Gzip ÏïïÏ∂ï Ìï®Ïàò Ï∂îÍ∞Ä] ---
def gzip_response(data):
    content = gzip.compress(json.dumps(data, ensure_ascii=False).encode('utf-8'))
    response = make_response(content)
    response.headers['Content-Encoding'] = 'gzip'
    response.headers['Content-Type'] = 'application/json'
    response.headers['Content-Length'] = len(content)
    return response

# --- [1. ÏÑ§Ï†ï Î∞è Í≤ΩÎ°ú] ---
MY_IP = "192.168.0.2"
DATA_DIR = "/volume2/video/thumbnails"
DB_FILE = "/volume2/video/video_metadata.db"
TMDB_CACHE_DIR = "/volume2/video/tmdb_cache"
HLS_ROOT = "/dev/shm/videoplayer_hls"
CACHE_VERSION = "137.19" # Î°úÍπÖ Í∞ïÌôî Î≤ÑÏ†Ñ

# [ÏàòÏ†ï] Ï†àÎåÄ Í≤ΩÎ°úÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÌååÏùº ÏÉùÏÑ± Î≥¥Ïû•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
FAILURE_LOG_PATH = os.path.join(SCRIPT_DIR, "metadata_failures.txt")
SUCCESS_LOG_PATH = os.path.join(SCRIPT_DIR, "metadata_success.txt")

TMDB_MEMORY_CACHE = {}
TMDB_API_KEY = "eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiI3OGNiYWQ0ZjQ3NzcwYjYyYmZkMTcwNTA2NDIwZDQyYyIsIm5iZiI6MTY1MzY3NTU4MC45MTUsInN1YiI6IjYyOTExNjNjMTI0MjVjMDA1MjI0ZGQzNCIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.3YU0WuIx_WDo6nTRKehRtn4N5I4uCgjI1tlpkqfsUhk".strip()
TMDB_BASE_URL = "https://api.themoviedb.org/3"

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(TMDB_CACHE_DIR, exist_ok=True)
if os.path.exists(HLS_ROOT): shutil.rmtree(HLS_ROOT, ignore_errors=True)
os.makedirs(HLS_ROOT, exist_ok=True)

PARENT_VIDEO_DIR = "/volume2/video/GDS3/GDRIVE/VIDEO"
PATH_MAP = {
    "Ïô∏Íµ≠TV": (os.path.join(PARENT_VIDEO_DIR, "Ïô∏Íµ≠TV"), "ftv"),
    "Íµ≠ÎÇ¥TV": (os.path.join(PARENT_VIDEO_DIR, "Íµ≠ÎÇ¥TV"), "ktv"),
    "ÏòÅÌôî": (os.path.join(PARENT_VIDEO_DIR, "ÏòÅÌôî"), "movie"),
    "Ïï†ÎãàÎ©îÏù¥ÏÖò": (os.path.join(PARENT_VIDEO_DIR, "ÏùºÎ≥∏ Ïï†ÎãàÎ©îÏù¥ÏÖò"), "anim_all"),
    "Î∞©ÏÜ°Ï§ë": (os.path.join(PARENT_VIDEO_DIR, "Î∞©ÏÜ°Ï§ë"), "air")
}

EXCLUDE_FOLDERS = ["ÏÑ±Ïù∏", "19Í∏à", "Adult", "@eaDir", "#recycle"]
VIDEO_EXTS = ('.mp4', '.mkv', '.avi', '.wmv', '.flv', '.ts', '.tp', '.m4v', '.m2ts', '.mov')
FFMPEG_PATH = "ffmpeg"
for p in ["/usr/local/bin/ffmpeg", "/var/packages/ffmpeg/target/bin/ffmpeg", "/usr/bin/ffmpeg"]:
    if os.path.exists(p):
        FFMPEG_PATH = p
        break

HOME_RECOMMEND = []
IS_METADATA_RUNNING = False
_FAST_CATEGORY_CACHE = {}
_SECTION_CACHE = {} # Ïπ¥ÌÖåÍ≥†Î¶¨ ÏÑπÏÖò Í≤∞Í≥º Ï∫êÏãú Ï∂îÍ∞Ä
_DETAIL_CACHE = deque(maxlen=200)

THUMB_SEMAPHORE = threading.Semaphore(4)
THUMB_EXECUTOR = ThreadPoolExecutor(max_workers=8)

def log(tag, msg):
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] [{tag}] {msg}", flush=True)

def log_matching_failure(orig, cleaned, reason):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with open(FAILURE_LOG_PATH, "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] [FAIL] ORIG: {orig} | CLEANED: {cleaned} | REASON: {reason}\n")
    except Exception as e:
        log("LOG_ERROR", f"Ïã§Ìå® Î°úÍ∑∏ ÌååÏùº Ïì∞Í∏∞ Ï§ë ÏóêÎü¨: {str(e)}")

def log_matching_success(orig, cleaned, matched, tmdb_id):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with open(SUCCESS_LOG_PATH, "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] [OK] ORIG: {orig} | CLEANED: {cleaned} | MATCHED: {matched} | ID: {tmdb_id}\n")
    except: pass

def nfc(text):
    return unicodedata.normalize('NFC', text) if text else ""

def nfd(text):
    return unicodedata.normalize('NFD', text) if text else ""

# --- [DB Í¥ÄÎ¶¨] ---
def get_db():
    conn = sqlite3.connect(DB_FILE, timeout=60)
    conn.row_factory = sqlite3.Row
    return conn

def init_db():
    conn = get_db()
    cursor = conn.cursor()
    cursor.execute('CREATE TABLE IF NOT EXISTS series (path TEXT PRIMARY KEY, category TEXT, name TEXT, posterPath TEXT, year TEXT, overview TEXT, rating TEXT, seasonCount INTEGER, genreIds TEXT, genreNames TEXT, director TEXT, actors TEXT, failed INTEGER DEFAULT 0, tmdbId TEXT)')
    cursor.execute('CREATE TABLE IF NOT EXISTS episodes (id TEXT PRIMARY KEY, series_path TEXT, title TEXT, videoUrl TEXT, thumbnailUrl TEXT, overview TEXT, air_date TEXT, season_number INTEGER, episode_number INTEGER, FOREIGN KEY (series_path) REFERENCES series (path) ON DELETE CASCADE)')
    cursor.execute('CREATE TABLE IF NOT EXISTS tmdb_cache (h TEXT PRIMARY KEY, data TEXT)')
    cursor.execute('CREATE TABLE IF NOT EXISTS server_config (key TEXT PRIMARY KEY, value TEXT)')

    cursor.execute('CREATE INDEX IF NOT EXISTS idx_series_category ON series(category)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_series_name ON series(name)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_series_tmdbId ON series(tmdbId)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_episodes_series ON episodes(series_path)')

    def add_col_if_missing(table, col, type):
        cursor.execute(f"PRAGMA table_info({table})")
        cols = [c[1] for c in cursor.fetchall()]
        if col not in cols:
            log("DB", f"Ïª¨Îüº Ï∂îÍ∞Ä: {table}.{col}")
            cursor.execute(f"ALTER TABLE {table} ADD COLUMN {col} {type}")

    add_col_if_missing('series', 'tmdbId', 'TEXT')
    add_col_if_missing('series', 'genreNames', 'TEXT')
    add_col_if_missing('series', 'director', 'TEXT')
    add_col_if_missing('series', 'actors', 'TEXT')
    add_col_if_missing('series', 'cleanedName', 'TEXT')
    add_col_if_missing('series', 'yearVal', 'TEXT')

    add_col_if_missing('episodes', 'overview', 'TEXT')
    add_col_if_missing('episodes', 'air_date', 'TEXT')
    add_col_if_missing('episodes', 'season_number', 'INTEGER')
    add_col_if_missing('episodes', 'episode_number', 'INTEGER')

    cursor.execute('CREATE INDEX IF NOT EXISTS idx_series_cleanedName ON series(cleanedName)')

    conn.commit()
    conn.close()
    log("DB", "ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî Î∞è ÏµúÏ†ÅÌôî ÏôÑÎ£å")

# --- [Ïú†Ìã∏Î¶¨Ìã∞] ---
def get_real_path(path):
    if not path or os.path.exists(path): return path
    if os.path.exists(nfc(path)): return nfc(path)
    if os.path.exists(nfd(path)): return nfd(path)
    return path

def migrate_json_to_db():
    if not os.path.exists(TMDB_CACHE_DIR): return
    conn = get_db()
    row = conn.execute("SELECT value FROM server_config WHERE key = 'json_migration_done'").fetchone()
    if row and row['value'] == 'true':
        conn.close()
        return
    files = [f for f in os.listdir(TMDB_CACHE_DIR) if f.endswith(".json")]
    if not files:
        conn.close()
        return
    log("MIGRATE", f"JSON Ï∫êÏãú {len(files)}Í∞ú DB Ïù¥Í¥Ä Ï§ë...")
    for idx, f in enumerate(files):
        h = f.replace(".json", "")
        try:
            with open(os.path.join(TMDB_CACHE_DIR, f), 'r', encoding='utf-8') as file:
                data = json.load(file)
                conn.execute('INSERT OR REPLACE INTO tmdb_cache (h, data) VALUES (?, ?)', (h, json.dumps(data)))
        except: pass
        if (idx + 1) % 2000 == 0:
            conn.commit()
            log("MIGRATE", f"ÏßÑÌñâ Ï§ë... ({idx+1}/{len(files)})")
    conn.execute("INSERT OR REPLACE INTO server_config (key, value) VALUES ('json_migration_done', 'true')")
    conn.commit()
    conn.close()
    log("MIGRATE", "Ïù¥Í¥Ä ÏôÑÎ£å")

# --- [Ï†ïÍ∑úÏãù Î∞è ÌÅ¥Î¶¨Îãù ÎåÄÌè≠ Í∞ïÌôî] ---
REGEX_EXT = re.compile(r'\.[a-zA-Z0-9]{2,4}$')
REGEX_YEAR = re.compile(r'\((19|20)\d{2}\)|(?<!\d)(19|20)\d{2}(?!\d)')
REGEX_CH_PREFIX = re.compile(r'^\[(?:KBS|SBS|MBC|tvN|JTBC|OCN|Mnet|TVÏ°∞ÏÑ†|Ï±ÑÎÑêA|MBN|ENA|KBS2|KBS1|CH\d+|TV|Netflix|Disney\+|AppleTV|NET|Wavve|Tving|Coupang)\]\s*')
REGEX_TECHNICAL_TAGS = re.compile(r'(?i)[.\s_-](?!(?:\d+\b))(\d{3,4}p|2160p|FHD|QHD|UHD|4K|Bluray|Blu-ray|WEB-DL|WEBRip|HDRip|BDRip|DVDRip|H\.?26[45]|x26[45]|HEVC|AVC|AAC\d?|DTS-?H?D?|AC3|DDP\d?|DD\+\d?|Dual|Atmos|REPACK|10bit|REMUX|FLAC|xvid|DivX|MKV|MP4|AVI|HDR(?:10)?(?:\+)?|Vision|Dolby|NF|AMZN|HMAX|DSNP|AppleTV?|Disney|PCOK|playWEB|ATVP|HULU|HDTV|HD|KBS|SBS|MBC|TVN|JTBC|NEXT|ST|SW|KL|YT|MVC|KN|FLUX|hallowed|PiRaTeS|Jadewind|Movie|pt\s*\d+|KOREAN|KOR|ITALIAN|JAPANESE|JPN|CHINESE|CHN|ENGLISH|ENG|USA|HK|TW|FRENCH|GERMAN|SPANISH|THAI|VIETNAMESE|WEB|DL|TVRip|HDR10Plus|IMAX|Unrated|REMASTERED|Criterion|NonDRM|BRRip|1080i|720i|Íµ≠Ïñ¥|Mandarin|Cantonese|FanSub|VFQ|VF|2CH|5\.1CH|8m|2398|PROPER|PROMO|LIMITED|RM4K|DC|THEATRICAL|EXTENDED|FINAL|DUB|KORDUB|JAPDUB|ENGDUB|ARROW|EDITION|SPECIAL|COLLECTION|RETAIL|TVING|WAVVE|Coupang|CP|B-Global|TrueHD|E-AC3|EAC3|DV|Dual-Audio|Multi-Audio|Multi-Sub)(\b|$|[.\s_-])')

REGEX_EP_MARKER_STRICT = re.compile(r'(?i)(?:^|[.\s_-]|[Í∞Ä-Ìû£\u3040-\u30ff\u4e00-\u9fff])(?:Á¨¨?\s*S(\d+)E(\d+)(?:[-~]E?\d+)?|Á¨¨?\s*S(\d+)|Á¨¨?\s*E(\d+)(?:[-~]\d+)?|\d+\s*(?:Ìôî|Ìöå|Í∏∞|Î∂Ä|Ë©±)|Season\s*\d+|Part\s*\d+|pt\s*\d+|Episode\s*\d+|Disk\s*\d+|Disc\s*\d+|CD\s*\d+|ÏãúÏ¶å\s*\d+|[ÏÉÅÌïò]Î∂Ä|ÏµúÏ¢ÖÌôî|\d{6}|\d{8})')

REGEX_DATE_YYMMDD = re.compile(r'(?<!\d)\d{6}(?!\d)')
REGEX_FORBIDDEN_CONTENT = re.compile(r'(?i)(Storyboard|Behind the Scenes|Making of|Deleted Scenes|Alternate Scenes|Gag Reel|Gag Menu|Digital Hits|Trailer|Bonus|Extras|Gallery|Production|Visual Effects|VFX|Îì±Í∏âÍ≥†ÏßÄ|ÏòàÍ≥†Ìé∏|Í∞úÎ¥âÎ≤ÑÏ†Ñ|Ïù∏ÌÑ∞Î∑∞|ÏÇ≠Ï†úÏû•Î©¥|(?<!\S)[ÏÉÅÌïò](?!\S))')
REGEX_FORBIDDEN_TITLE = re.compile(r'(?i)^\s*(Season\s*\d+|Part\s*\d+|EP\s*\d+|\d+Ìôî|\d+Ìöå|\d+Í∏∞|ÏãúÏ¶å\s*\d+|S\d+|E\d+|Disk\s*\d+|Disc\s*\d+|CD\s*\d+|Specials?|Extras?|Bonus|ÎØ∏Î∂ÑÎ•ò|Í∏∞ÌÉÄ|ÏÉà\s*Ìè¥Îçî|VIDEO|GDS3|GDRIVE|NAS|share|ÏòÅÌôî|Ïô∏Íµ≠TV|Íµ≠ÎÇ¥TV|Ïï†ÎãàÎ©îÏù¥ÏÖò|Î∞©ÏÜ°Ï§ë|Ï†úÎ™©|UHD|ÏµúÏã†|ÏµúÏã†Ïûë|ÏµúÏã†ÏòÅÌôî|4K|1080P|720P)\s*$', re.I)

REGEX_BRACKETS = re.compile(r'\[.*?(?:\]|$)|\(.*?(?:\)|$)|\{.*?(?:\)|$)|\{.*?(?:\)|$)|\„Äê.*?(?:\„Äë|$)|\„Äé.*?(?:\„Äè|$)|\„Äå.*?(?:\„Äç|$)|\Ôºà.*?(?:\Ôºâ|$)')
REGEX_TMDB_HINT = re.compile(r'\{tmdb[\s-]*(\d+)\}')
REGEX_JUNK_KEYWORDS = re.compile(r'(?i)\s*(?:ÎçîÎπô|ÏûêÎßâ|Í∑πÏû•Ìåê|BD|TV|Web|OAD|OVA|ONA|Full|Î¨¥ÏÇ≠Ï†ú|Í∞êÎèÖÌåê|ÌôïÏû•Ìåê|ÏùµÏä§ÌÖêÎîîÎìú|Îì±Í∏âÍ≥†ÏßÄ|ÏòàÍ≥†Ìé∏|(?<!\S)[ÏÉÅÌïò](?!\S))\s*')

REGEX_SPECIAL_CHARS = re.compile(r'[\[\]()_\-\.!#@*‚Äª√ó,~:;„Äê„Äë„Äé„Äè„Äå„Äç"\'ÔºàÔºâ]')
REGEX_LEADING_INDEX = re.compile(r'^\s*(\d{1,5}(?:\s+|[.\s_-]+|(?=[Í∞Ä-Ìû£a-zA-Z])))|^\s*(\d{1,5}\. )')
REGEX_SPACES = re.compile(r'\s+')

def clean_title_complex(title):
    if not title: return "", None
    orig_title = nfc(title)

    if REGEX_FORBIDDEN_CONTENT.search(orig_title):
        return "", None

    cleaned = REGEX_EXT.sub('', orig_title)
    cleaned = REGEX_CH_PREFIX.sub('', cleaned)
    cleaned = REGEX_TMDB_HINT.sub('', cleaned)
    if '.' in cleaned:
        cleaned = cleaned.replace('.', ' ')

    ep_match = REGEX_EP_MARKER_STRICT.search(cleaned)
    if ep_match:
        if ep_match.start() < 5:
            cleaned = cleaned[ep_match.end():].strip()
        else:
            cleaned = cleaned[:ep_match.start()].strip()

    tech_match = REGEX_TECHNICAL_TAGS.search(cleaned)
    if tech_match:
        cleaned = cleaned[:tech_match.start()].strip()

    cleaned = re.sub(r'([Í∞Ä-Ìû£\u3040-\u30ff\u4e00-\u9fff])([a-zA-Z])', r'\1 \2', cleaned)
    cleaned = re.sub(r'([a-zA-Z])([Í∞Ä-Ìû£\u3040-\u30ff\u4e00-\u9fff])', r'\1 \2', cleaned)
    cleaned = re.sub(r'([Í∞Ä-Ìû£\u3040-\u30ff\u4e00-\u9fff\w])(\d+)', r'\1 \2', cleaned)
    cleaned = re.sub(r'(\d+)([Í∞Ä-Ìû£\u3040-\u30ff\u4e00-\u9fff\w])', r'\1 \2', cleaned)

    cleaned = REGEX_DATE_YYMMDD.sub(' ', cleaned)
    year_match = REGEX_YEAR.search(cleaned)
    year = year_match.group().replace('(', '').replace(')', '') if year_match else None
    cleaned = REGEX_YEAR.sub(' ', cleaned)

    if len(cleaned.strip()) < 2:
        brackets = re.findall(r'\[(.*?)\]|\((.*?)\)|Ôºà(.*?)Ôºâ', orig_title)
        for b in brackets:
            inner = (b[0] or b[1] or b[2]).strip()
            if len(inner) >= 2 and not REGEX_TECHNICAL_TAGS.search(inner) and not REGEX_FORBIDDEN_TITLE.match(inner):
                cleaned = inner
                break

    cleaned = REGEX_BRACKETS.sub(' ', cleaned)
    cleaned = cleaned.replace("(ÏûêÎßâ)", "").replace("(ÎçîÎπô)", "").replace("[ÏûêÎßâ]", "").replace("[ÎçîÎπô]", "").replace("ÔºàÏûêÎßâÔºâ", "").replace("ÔºàÎçîÎπôÔºâ", "")
    cleaned = REGEX_JUNK_KEYWORDS.sub(' ', cleaned)
    cleaned = REGEX_SPECIAL_CHARS.sub(' ', cleaned)
    cleaned = REGEX_LEADING_INDEX.sub('', cleaned)
    cleaned = re.sub(r'([Í∞Ä-Ìû£a-zA-Z\u3040-\u30ff\u4e00-\u9fff])(\d+)$', r'\1 \2', cleaned)
    cleaned = REGEX_SPACES.sub(' ', cleaned).strip()

    if len(cleaned) < 1:
        return "", None
    return nfc(cleaned), year

def extract_episode_numbers(filename):
    match = REGEX_EP_MARKER_STRICT.search(filename)
    if match:
        s, e = match.group(1), match.group(2)
        if s and e: return int(s), int(e)
        s_only = match.group(3)
        if s_only: return int(s_only), 1
        e_only = match.group(4)
        if e_only: return 1, int(e_only)
    return 1, None

def natural_sort_key(s):
    if s is None: return []
    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\d+)', nfc(str(s)))]

def extract_tmdb_id(title):
    match = REGEX_TMDB_HINT.search(nfc(title))
    return int(match.group(1)) if match else None

def simple_similarity(s1, s2):
    s1, s2 = s1.lower().replace(" ", ""), s2.lower().replace(" ", "")
    if s1 == s2: return 1.0
    if s1 in s2 or s2 in s1: return 0.8
    return 0.0

# --- [TMDB API Î≥¥ÏôÑ: ÏßÄÎä•Ìòï Ïû¨Í≤ÄÏÉâ Î∞è Îû≠ÌÇπ ÏãúÏä§ÌÖú] ---
def get_tmdb_info_server(title, ignore_cache=False):
    if not title: return {"failed": True}
    hint_id = extract_tmdb_id(title)
    ct, year = clean_title_complex(title)
    if not ct or REGEX_FORBIDDEN_TITLE.match(ct):
        return {"failed": True, "forbidden": True}

    cache_key = f"{ct}_{year}" if year else ct
    h = hashlib.md5(nfc(cache_key).encode()).hexdigest()

    if not ignore_cache and h in TMDB_MEMORY_CACHE:
        return TMDB_MEMORY_CACHE[h]

    if not ignore_cache:
        try:
            conn = get_db()
            row = conn.execute('SELECT data FROM tmdb_cache WHERE h = ?', (h,)).fetchone()
            conn.close()
            if row:
                data = json.loads(row['data'])
                TMDB_MEMORY_CACHE[h] = data
                return data
        except: pass

    log("TMDB", f"üîç ÏßÄÎä•Ìòï Í≤ÄÏÉâ ÏãúÏûë: '{ct}'" + (f" ({year})" if year else ""))
    headers = {"Authorization": f"Bearer {TMDB_API_KEY}"}
    base_params = {"include_adult": "true", "region": "KR"}

    def perform_search(query, lang=None, m_type='multi', search_year=None):
        if not query or len(query) < 2: return []
        params = {**base_params, "query": query}
        if lang: params["language"] = lang
        if search_year:
            params['year' if m_type != 'tv' else 'first_air_date_year'] = search_year
        try:
            r = requests.get(f"{TMDB_BASE_URL}/search/{m_type}", params=params, headers=headers, timeout=10)
            return r.json().get('results', []) if r.status_code == 200 else []
        except: return []

    def rank_results(results, target_title, target_year):
        if not results: return None
        scored = []
        for res in results:
            if res.get('media_type') == 'person': continue
            score = 0
            res_title = res.get('title') or res.get('name') or ""
            res_year = (res.get('release_date') or res.get('first_air_date') or "").split('-')[0]

            sim = simple_similarity(target_title, res_title)
            score += sim * 60

            if target_year and res_year:
                if target_year == res_year: score += 30
                elif abs(int(target_year) - int(res_year)) <= 1: score += 15
            elif not target_year:
                score += 10

            score += min(res.get('popularity', 0) / 10, 10)
            if res.get('poster_path'): score += 5

            scored.append((score, res))

        scored.sort(key=lambda x: x[0], reverse=True)
        return scored[0][1] if scored and scored[0][0] > 30 else None

    try:
        best_match = None
        if hint_id:
            log("TMDB", f"üí° ÌûåÌä∏ ID ÏÇ¨Ïö©: {hint_id}")
            for mt in ['movie', 'tv']:
                resp = requests.get(f"{TMDB_BASE_URL}/{mt}/{hint_id}", params={"language": "ko-KR", **base_params}, headers=headers, timeout=10)
                if resp.status_code == 200:
                    best_match = resp.json()
                    best_match['media_type'] = mt
                    break

        if not best_match:
            results = perform_search(ct, "ko-KR", "multi", year)
            best_match = rank_results(results, ct, year)

            if not best_match and year:
                log("TMDB", f"üîÑ Ïó∞ÎèÑ Ï†úÏô∏ Ïû¨Í≤ÄÏÉâ: '{ct}'")
                results = perform_search(ct, "ko-KR", "multi", None)
                best_match = rank_results(results, ct, year)

            if not best_match:
                parts = re.split(r'[-:]', ct)
                if len(parts) > 1 and len(parts[0].strip()) >= 2:
                    main_title = parts[0].strip()
                    log("TMDB", f"üîÑ Ï£ºÏ†úÎ™© Í≤ÄÏÉâ: '{main_title}'")
                    results = perform_search(main_title, "ko-KR", "multi", year)
                    best_match = rank_results(results, main_title, year)

            if not best_match:
                cjk_only = "".join(re.findall(r'[Í∞Ä-Ìû£\u3040-\u30ff\u4e00-\u9fff\s]+', ct)).strip()
                if cjk_only and len(cjk_only) >= 2 and cjk_only != ct:
                    results = perform_search(cjk_only, "ko-KR", "multi", year)
                    best_match = rank_results(results, cjk_only, year)

            if not best_match:
                en_only = "".join(re.findall(r'[a-zA-Z\s]+', ct)).strip()
                if en_only and len(en_only) >= 3:
                    results = perform_search(en_only, "ko-KR", "multi", year)
                    best_match = rank_results(results, en_only, year)

        if best_match:
            m_type, t_id = best_match.get('media_type') or ('movie' if best_match.get('title') else 'tv'), best_match.get('id')
            log("TMDB", f"‚úÖ Îß§Ïπ≠ ÏÑ±Í≥µ: '{ct}' -> {m_type}:{t_id}")
            log_matching_success(title, ct, best_match.get('title') or best_match.get('name'), f"{m_type}:{t_id}")
            d_resp = requests.get(f"{TMDB_BASE_URL}/{m_type}/{t_id}?language=ko-KR&append_to_response=content_ratings,credits", headers=headers, timeout=10).json()

            yv = (d_resp.get('release_date') or d_resp.get('first_air_date') or "").split('-')[0]
            rating = None
            if 'content_ratings' in d_resp:
                res_r = d_resp['content_ratings'].get('results', [])
                kr = next((r['rating'] for r in res_r if r.get('iso_3166_1') == 'KR'), None)
                if kr: rating = f"{kr}+" if kr.isdigit() else kr

            genre_names = [g['name'] for g in d_resp.get('genres', [])] if d_resp.get('genres') else []
            cast_data = d_resp.get('credits', {}).get('cast', [])
            actors = [{"name": c['name'], "profile": c['profile_path'], "role": c['character']} for c in cast_data[:10]]
            crew_data = d_resp.get('credits', {}).get('crew', [])
            director = next((c['name'] for c in crew_data if c.get('job') == 'Director'), "")

            info = {
                "tmdbId": f"{m_type}:{t_id}",
                "genreIds": [g['id'] for g in d_resp.get('genres', [])] if d_resp.get('genres') else [],
                "genreNames": genre_names,
                "director": director,
                "actors": actors,
                "posterPath": d_resp.get('poster_path'),
                "year": yv,
                "overview": d_resp.get('overview'),
                "rating": rating,
                "seasonCount": d_resp.get('number_of_seasons'),
                "failed": False
            }

            if m_type == 'tv':
                info['seasons_data'] = {}
                s_count = d_resp.get('number_of_seasons') or 1
                for s_num in range(1, s_count + 1):
                    s_resp = requests.get(f"{TMDB_BASE_URL}/tv/{t_id}/season/{s_num}?language=ko-KR", headers=headers, timeout=10).json()
                    if 'episodes' in s_resp:
                        for ep in s_resp['episodes']:
                            info['seasons_data'][f"{s_num}_{ep['episode_number']}"] = {"overview": ep.get('overview'), "air_date": ep.get('air_date')}

            TMDB_MEMORY_CACHE[h] = info
            try:
                conn = get_db()
                conn.execute('INSERT OR REPLACE INTO tmdb_cache (h, data) VALUES (?, ?)', (h, json.dumps(info)))
                conn.commit()
                conn.close()
            except: pass
            return info
        else:
            log("TMDB", f"‚ùå Í≤ÄÏÉâ Í≤∞Í≥º ÏóÜÏùå: '{ct}'")
            log_matching_failure(title, ct, "NOT_FOUND_IN_TMDB")
            try:
                conn = get_db()
                conn.execute('INSERT OR REPLACE INTO tmdb_cache (h, data) VALUES (?, ?)', (h, json.dumps({"failed": True})))
                conn.commit()
                conn.close()
            except: pass
    except:
        log("TMDB", f"‚ö†Ô∏è ÏóêÎü¨ Î∞úÏÉù: {traceback.format_exc()}")
        log_matching_failure(title, ct, f"API_ERROR: {str(sys.exc_info()[1])}")
    return {"failed": True}

# --- [Ïä§Ï∫î Î∞è ÌÉêÏÉâ] ---
def scan_recursive_to_db(bp, prefix, category):
    log("SCAN", f"üìÇ '{category}' ÌÉêÏÉâ Ï§ë: {bp}")
    base = nfc(get_real_path(bp))
    all_files = []
    stack = [base]
    visited = set()
    while stack:
        curr = stack.pop()
        real_curr = os.path.realpath(curr)
        if real_curr in visited: continue
        visited.add(real_curr)
        try:
            with os.scandir(curr) as it:
                for entry in it:
                    if entry.is_dir():
                        if not any(ex in entry.name for ex in EXCLUDE_FOLDERS) and not entry.name.startswith('.'):
                            stack.append(entry.path)
                    elif entry.is_file() and entry.name.lower().endswith(VIDEO_EXTS):
                        all_files.append(nfc(entry.path))
        except: pass

    conn = get_db()
    cursor = conn.cursor()
    cursor.execute('SELECT id, series_path FROM episodes WHERE series_path LIKE ?', (f"{category}/%",))
    db_data = {row['id']: row['series_path'] for row in cursor.fetchall()}
    current_ids = set()
    total = len(all_files)

    for idx, fp in enumerate(all_files):
        mid = hashlib.md5(fp.encode()).hexdigest()
        current_ids.add(mid)
        rel = nfc(os.path.relpath(fp, base))
        name = os.path.splitext(os.path.basename(fp))[0]
        spath = f"{category}/{rel}"

        ct, yr = clean_title_complex(name)
        cursor.execute('INSERT OR IGNORE INTO series (path, category, name, cleanedName, yearVal) VALUES (?, ?, ?, ?, ?)', (spath, category, name, ct, yr))

        if mid not in db_data:
            cursor.execute('INSERT OR REPLACE INTO episodes (id, series_path, title, videoUrl, thumbnailUrl) VALUES (?, ?, ?, ?, ?)', (mid, spath, os.path.basename(fp), f"/video_serve?type={prefix}&path={urllib.parse.quote(rel)}", f"/thumb_serve?type={prefix}&id={mid}&path={urllib.parse.quote(rel)}"))
        elif db_data[mid] != spath:
            cursor.execute('UPDATE episodes SET series_path = ? WHERE id = ?', (spath, mid))

        if (idx + 1) % 2000 == 0:
            conn.commit()
            log("SCAN", f"‚è≥ ÏßÑÌñâ Ï§ë... ({idx+1}/{total})")

    for rid in (set(db_data.keys()) - current_ids):
        cursor.execute('DELETE FROM episodes WHERE id = ?', (rid,))
    cursor.execute('DELETE FROM series WHERE path NOT IN (SELECT DISTINCT series_path FROM episodes) AND category = ?', (category,))
    conn.commit()
    conn.close()
    log("SCAN", f"‚úÖ '{category}' Ïä§Ï∫î ÏôÑÎ£å ({total}Í∞ú)")

def perform_full_scan():
    log("SYSTEM", f"üöÄ Ï†ÑÏ≤¥ Ïä§Ï∫î ÏãúÏûë (v{CACHE_VERSION})")
    pk = [("ÏòÅÌôî", "movies"), ("Ïô∏Íµ≠TV", "foreigntv"), ("Íµ≠ÎÇ¥TV", "koreantv"), ("Ïï†ÎãàÎ©îÏù¥ÏÖò", "animations_all"), ("Î∞©ÏÜ°Ï§ë", "air")]
    conn = get_db()
    rows = conn.execute("SELECT key FROM server_config WHERE key LIKE 'scan_done_%' AND value = 'true'").fetchall()
    done = [r['key'].replace('scan_done_', '') for r in rows]
    conn.close()

    for label, ck in pk:
        if ck in done: continue
        path, prefix = PATH_MAP[label]
        if os.path.exists(path):
            scan_recursive_to_db(path, prefix, ck)
            conn = get_db()
            conn.execute("INSERT OR REPLACE INTO server_config (key, value) VALUES (?, 'true')", (f'scan_done_{ck}',))
            conn.commit()
            conn.close()
            build_all_caches()

    conn = get_db()
    conn.execute('INSERT OR REPLACE INTO server_config (key, value) VALUES (?, ?)', ('last_scan_version', CACHE_VERSION))
    conn.execute("DELETE FROM server_config WHERE key LIKE 'scan_done_%'")
    conn.commit()
    conn.close()
    threading.Thread(target=fetch_metadata_async, daemon=True).start()

def fetch_metadata_async(force_all=False):
    global IS_METADATA_RUNNING
    if IS_METADATA_RUNNING:
        log("METADATA", "Ïù¥ÎØ∏ ÌîÑÎ°úÏÑ∏Ïä§Í∞Ä Ïã§Ìñâ Ï§ëÏûÖÎãàÎã§. Ï§ëÎã®Ìï©ÎãàÎã§.")
        return
    IS_METADATA_RUNNING = True
    log("METADATA", f"‚öôÔ∏è Î≥ëÎ†¨ Îß§Ïπ≠ ÌîÑÎ°úÏÑ∏Ïä§ ÏãúÏûë (force_all={force_all})")
    try:
        conn = get_db()
        # [Î°úÍ∑∏ Í∞ïÌôî] ÏûëÏóÖ ÏãúÏûë Ï†Ñ ÌòÑÏû¨ DB ÌÜµÍ≥Ñ ÌååÏïÖ
        cursor = conn.cursor()
        t_all = cursor.execute("SELECT COUNT(*) FROM series").fetchone()[0]
        t_ok = cursor.execute("SELECT COUNT(*) FROM series WHERE tmdbId IS NOT NULL").fetchone()[0]
        t_fail = cursor.execute("SELECT COUNT(*) FROM series WHERE failed = 1").fetchone()[0]
        t_wait = cursor.execute("SELECT COUNT(*) FROM series WHERE tmdbId IS NULL AND failed = 0").fetchone()[0]
        t_rate = (t_ok / t_all * 100) if t_all > 0 else 0

        log("METADATA_STATS", f"üìä [ÏûëÏóÖ Ï†Ñ ÌÜµÍ≥Ñ] Ï†ÑÏ≤¥: {t_all} | ÏÑ±Í≥µ: {t_ok} | Ïã§Ìå®: {t_fail} | ÎåÄÍ∏∞: {t_wait} | ÏÑ±Í≥µÎ•†: {round(t_rate, 2)}%")

        if force_all:
            log("METADATA", "üßπ Ïã§Ìå®Ìïú Ìï≠Î™© Ï¥àÍ∏∞Ìôî Î∞è Ïû¨ÏãúÎèÑ Ï§ÄÎπÑ (ÏÑ±Í≥µ Ìï≠Î™© Î≥¥Ï°¥)...")
            conn.execute('UPDATE series SET failed=0 WHERE tmdbId IS NULL')
            conn.commit()

        uncleaned_names_rows = conn.execute('SELECT name FROM series WHERE cleanedName IS NULL AND tmdbId IS NULL AND failed = 0 GROUP BY name').fetchall()
        if uncleaned_names_rows:
            log("METADATA", f"üß™ ÎàÑÎùΩÎêú Ìï≠Î™© Ï†úÎ™© Ï†ïÏ†ú Ï§ë ({len(uncleaned_names_rows)}Í∞ú Í≥†Ïú† Ï†úÎ™©)...")
            cursor = conn.cursor()
            for idx, r in enumerate(uncleaned_names_rows):
                name = r['name']
                ct, yr = clean_title_complex(name)
                cursor.execute('UPDATE series SET cleanedName=?, yearVal=? WHERE name=? AND cleanedName IS NULL', (ct, yr, name))
                if (idx + 1) % 2000 == 0: conn.commit()
            conn.commit()

        log("METADATA", "üìÇ DBÏóêÏÑú Îß§Ïπ≠ÎêòÏßÄ ÏïäÏùÄ ÏûëÌíà Î™©Î°ù Î∂àÎü¨Ïò§Îäî Ï§ë...")
        all_names_rows = conn.execute('SELECT name FROM series WHERE tmdbId IS NULL AND failed = 0').fetchall()

        if not all_names_rows:
            conn.close()
            log("METADATA", "‚úÖ Îß§Ïπ≠ ÎåÄÏÉÅ ÏóÜÏùå (Î™®Îì† ÏûëÌíàÏù¥ Ïù¥ÎØ∏ Îß§Ïπ≠ÎêòÏóàÍ±∞ÎÇò Ïã§Ìå® Ï≤òÎ¶¨Îê®)")
            IS_METADATA_RUNNING = False
            build_all_caches()
            return

        group_rows = conn.execute('''
            SELECT cleanedName, yearVal, MIN(name) as sample_name, GROUP_CONCAT(name, '|') as orig_names
            FROM series
            WHERE tmdbId IS NULL AND failed = 0 AND cleanedName IS NOT NULL
            GROUP BY cleanedName, yearVal
        ''').fetchall()
        conn.close()

        tasks = []
        for gr in group_rows:
            tasks.append({
                'clean_title': gr['cleanedName'],
                'year': gr['yearVal'],
                'sample_name': gr['sample_name'],
                'orig_names': gr['orig_names'].split('|')
            })

        total = len(tasks)
        log("METADATA", f"üìä Í∑∏Î£πÌôî ÏôÑÎ£å: {total}Í∞úÏùò Í≥†Ïú† ÏûëÌíà ÏãùÎ≥ÑÎê®")

        def process_one(task):
            info = get_tmdb_info_server(task['sample_name'], ignore_cache=force_all)
            return (task, info)

        batch_size = 50
        total_success = 0
        total_fail = 0
        for i in range(0, total, batch_size):
            batch = tasks[i:i+batch_size]
            log("METADATA", f"üì¶ Î∞∞Ïπò Ï≤òÎ¶¨ Ï§ë ({i+1}~{min(i+batch_size, total)} / {total})")
            results = []
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_task = {executor.submit(process_one, t): t for t in batch}
                for future in as_completed(future_to_task):
                    results.append(future.result())

            log("METADATA", f"üíæ Î∞∞Ïπò {i//batch_size + 1} Í≤∞Í≥º DB Î∞òÏòÅ Ï§ë...")
            conn = get_db()
            cursor = conn.cursor()
            batch_success = 0
            batch_fail = 0
            for task, info in results:
                orig_names = task['orig_names']
                if info.get('failed'):
                    batch_fail += 1
                    cursor.executemany('UPDATE series SET failed=1 WHERE name=?', [(n,) for n in orig_names])
                else:
                    batch_success += 1
                    up = (
                        info.get('posterPath'), info.get('year'), info.get('overview'),
                        info.get('rating'), info.get('seasonCount'),
                        json.dumps(info.get('genreIds', [])),
                        json.dumps(info.get('genreNames', []), ensure_ascii=False),
                        info.get('director'),
                        json.dumps(info.get('actors', []), ensure_ascii=False),
                        info.get('tmdbId')
                    )
                    cursor.executemany('UPDATE series SET posterPath=?, year=?, overview=?, rating=?, seasonCount=?, genreIds=?, genreNames=?, director=?, actors=?, tmdbId=?, failed=0 WHERE name=?', [(*up, name) for name in orig_names])

                    if 'seasons_data' in info:
                        eps_to_update = []
                        for name in orig_names:
                            cursor.execute('SELECT id, title FROM episodes WHERE series_path IN (SELECT path FROM series WHERE name = ?)', (name,))
                            eps_to_update.extend(cursor.fetchall())

                        ep_batch = []
                        for ep_row in eps_to_update:
                            sn, EN = extract_episode_numbers(ep_row['title'])
                            if EN:
                                ei = info['seasons_data'].get(f"{sn}_{EN}")
                                if ei:
                                    ep_batch.append((ei.get('overview'), ei.get('air_date'), sn, EN, ep_row['id']))
                        if ep_batch:
                            cursor.executemany('UPDATE episodes SET overview=?, air_date=?, season_number=?, episode_number=? WHERE id=?', ep_batch)

            conn.commit()
            conn.close()
            total_success += batch_success
            total_fail += batch_fail

            log("METADATA", f"üìà ÏßÑÌñâ ÏÉÅÌô©: ‚úÖ Ïù¥Î≤à Î∞∞Ïπò ÏÑ±Í≥µ {batch_success} / ‚ùå Ïã§Ìå® {batch_fail} (ÎàÑÏ†Å: ‚úÖ {total_success} / ‚ùå {total_fail})")

            if (i // batch_size) % 10 == 0:
                log("METADATA", "‚ôªÔ∏è Ï§ëÍ∞Ñ Ï∫êÏãú Í∞±Ïã† Ï§ë...")
                build_all_caches()

        build_all_caches()

        # [Î°úÍ∑∏ Í∞ïÌôî] ÏûëÏóÖ Ï¢ÖÎ£å ÌõÑ ÏµúÏ¢Ö ÌÜµÍ≥Ñ Ï∂úÎ†•
        conn = get_db()
        cursor = conn.cursor()
        f_ok = cursor.execute("SELECT COUNT(*) FROM series WHERE tmdbId IS NOT NULL").fetchone()[0]
        f_rate = (f_ok / t_all * 100) if t_all > 0 else 0
        conn.close()

        log("METADATA_STATS", f"üéä ÏûëÏóÖ Ï¢ÖÎ£å! ÏÑ±Í≥µ: {t_ok} -> {f_ok} (Í∞úÏÑ†: +{f_ok - t_ok}) | ÏµúÏ¢Ö ÏÑ±Í≥µÎ•†: {round(f_rate, 2)}%")
        log("METADATA", f"üèÅ ÏµúÏ¢Ö ÏôÑÎ£å: Ï¥ù ‚úÖ {total_success}Í∞ú Ïã†Í∑ú Îß§Ïπ≠, ‚ùå {total_fail}Í∞ú Ïã§Ìå® Ïú†ÏßÄ")
    except:
        log("METADATA", f"‚ö†Ô∏è ÏπòÎ™ÖÏ†Å ÏóêÎü¨ Î∞úÏÉù: {traceback.format_exc()}")
    finally:
        IS_METADATA_RUNNING = False
        log("METADATA", "üèÅ Î≥ëÎ†¨ Îß§Ïπ≠ ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å")

def get_sections_for_category(cat, kw=None):
    cache_key = f"sections_{cat}_{kw}"
    if cache_key in _SECTION_CACHE:
        return _SECTION_CACHE[cache_key]
    base_list = _FAST_CATEGORY_CACHE.get(cat, [])
    if not base_list: return []
    if kw and kw not in ["Ï†ÑÏ≤¥", "All"]:
        search_kw = kw.strip().lower()
        target_list = [i for i in base_list if search_kw in i['path'].lower() or search_kw in i['name'].lower()]
    else:
        target_list = base_list
    if not target_list: return []
    sections = [{"title": "Ï†ÑÏ≤¥ Î™©Î°ù", "items": target_list[:400]}]
    _SECTION_CACHE[cache_key] = sections
    return sections

@app.route('/category_sections')
def get_category_sections():
    cat = request.args.get('cat', 'movies')
    kw = request.args.get('kw')
    return gzip_response(get_sections_for_category(cat, kw))

@app.route('/home')
def get_home():
    return gzip_response(HOME_RECOMMEND)

@app.route('/list')
def get_list():
    p = request.args.get('path', '')
    m = {"ÏòÅÌôî": "movies", "Ïô∏Íµ≠TV": "foreigntv", "Íµ≠ÎÇ¥TV": "koreantv", "Ïï†ÎãàÎ©îÏù¥ÏÖò": "animations_all", "Î∞©ÏÜ°Ï§ë": "air", "movies": "movies", "foreigntv": "foreigntv", "koreantv": "koreantv", "animations_all": "animations_all", "air": "air"}
    cat = "movies"
    for label, code in m.items():
        if label in p:
            cat = code
            break
    bl = _FAST_CATEGORY_CACHE.get(cat, [])
    kw = request.args.get('keyword')
    lim = int(request.args.get('limit', 1000))
    off = int(request.args.get('offset', 0))
    res = [item for item in bl if not kw or nfc(kw).lower() in item['path'].lower() or nfc(kw).lower() in item['name'].lower()] if kw and kw not in ["Ï†ÑÏ≤¥", "All"] else bl
    return gzip_response(res[off:off+lim])

@app.route('/api/series_detail')
def get_series_detail_api():
    path = request.args.get('path')
    if not path: return gzip_response([])
    for c_path, data in _DETAIL_CACHE:
        if c_path == path: return gzip_response(data)
    conn = get_db()
    row = conn.execute('SELECT * FROM series WHERE path = ?', (path,)).fetchone()
    if not row:
        conn.close()
        return gzip_response([])
    series = dict(row)
    for col in ['genreIds', 'genreNames', 'actors']:
        if series.get(col):
            try: series[col] = json.loads(series[col])
            except: series[col] = []
    if series.get('tmdbId'):
        cursor = conn.execute("SELECT e.* FROM episodes e JOIN series s ON e.series_path = s.path WHERE s.tmdbId = ?", (series['tmdbId'],))
    else:
        cursor = conn.execute("SELECT * FROM episodes WHERE series_path = ?", (path,))
    eps = []
    seen = set()
    for r in cursor.fetchall():
        if r['videoUrl'] not in seen:
            eps.append(dict(r))
            seen.add(r['videoUrl'])
    series['movies'] = sorted(eps, key=lambda x: natural_sort_key(x['title']))
    conn.close()

    for ep in series['movies']:
        THUMB_EXECUTOR.submit(pre_generate_individual_task, ep['thumbnailUrl'])

    _DETAIL_CACHE.append((path, series))
    return gzip_response(series)

def pre_generate_individual_task(ep_thumb_url):
    try:
        u = urllib.parse.urlparse(ep_thumb_url)
        q = urllib.parse.parse_qs(u.query)
        if 'path' in q and 'type' in q and 'id' in q:
            _generate_thumb_file(q['path'][0], q['type'][0], q['id'][0], q.get('t', ['300'])[0], q.get('w', ['320'])[0])
    except: pass

@app.route('/search')
def search_videos():
    q = request.args.get('q', '').lower()
    if not q: return jsonify([])
    conn = get_db()
    cursor = conn.execute('SELECT s.*, e.id as ep_id, e.videoUrl, e.thumbnailUrl, e.title FROM series s LEFT JOIN episodes e ON s.path = e.series_path WHERE (s.path LIKE ? OR s.name LIKE ?) GROUP BY s.path ORDER BY s.name ASC', (f'%{q}%', f'%{q}%'))
    rows = []
    for row in cursor.fetchall():
        item = dict(row)
        item['movies'] = [{"id": item.pop('ep_id'), "videoUrl": item.pop('videoUrl'), "thumbnailUrl": item.pop('thumbnailUrl'), "title": item.pop('title')}] if item.get('ep_id') else []
        for col in ['genreIds', 'genreNames', 'actors']:
            if item.get(col):
                try: item[col] = json.loads(item[col])
                except: item[col] = []
        rows.append(item)
    conn.close()
    return gzip_response(rows)

@app.route('/rescan_broken')
def rescan_broken():
    threading.Thread(target=perform_full_scan, daemon=True).start()
    return jsonify({"status": "success"})

@app.route('/rematch_metadata')
def rescan_metadata():
    if IS_METADATA_RUNNING:
        return jsonify({"status": "error", "message": "Metadata process is already running."})
    threading.Thread(target=fetch_metadata_async, args=(True,), daemon=True).start()
    return jsonify({"status": "success", "message": "Scanning for new or failed metadata in background."})

@app.route('/retry_failed_metadata')
def retry_failed_metadata():
    if IS_METADATA_RUNNING:
        return jsonify({"status": "error", "message": "Metadata process is already running."})
    conn = get_db()
    conn.execute('UPDATE series SET failed = 0 WHERE failed = 1')
    conn.commit()
    conn.close()
    threading.Thread(target=fetch_metadata_async, daemon=False).start()
    return jsonify({"status": "success", "message": "Retrying failed metadata with improved regex."})

@app.route('/video_serve')
def video_serve():
    path, prefix = request.args.get('path'), request.args.get('type')
    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
        return send_file(get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path)))), conditional=True)
    except:
        return "Not Found", 404

def _generate_thumb_file(path_raw, prefix, tid, t, w):
    tp = os.path.join(DATA_DIR, f"seek_{tid}_{t}_{w}.jpg")
    if os.path.exists(tp): return tp

    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
        vp = get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path_raw))))
        if not os.path.exists(vp): return None

        with THUMB_SEMAPHORE:
            if os.path.exists(tp): return tp
            subprocess.run([
                FFMPEG_PATH, "-y",
                "-ss", t,
                "-noaccurate_seek",
                "-i", vp,
                "-frames:v", "1",
                "-map", "0:v:0",
                "-an", "-sn",
                "-q:v", "7",
                "-vf", f"scale={w}:-1:flags=fast_bilinear",
                "-threads", "1",
                tp
            ], timeout=15, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        return tp if os.path.exists(tp) else None
    except:
        return None

@app.route('/thumb_serve')
def thumb_serve():
    path, prefix, tid = request.args.get('path'), request.args.get('type'), request.args.get('id')
    t = request.args.get('t', default="300")
    w = request.args.get('w', default="480")

    tp = _generate_thumb_file(path, prefix, tid, t, w)
    if tp and os.path.exists(tp):
        resp = make_response(send_file(tp, mimetype='image/jpeg'))
        resp.headers['Cache-Control'] = 'public, max-age=31536000, immutable'
        return resp
    return "Not Found", 404

@app.route('/api/status')
def get_server_status():
    try:
        conn = get_db()
        eps = conn.execute("SELECT COUNT(*) FROM episodes").fetchone()[0]
        ser = conn.execute("SELECT COUNT(*) FROM series").fetchone()[0]
        mtch = conn.execute("SELECT COUNT(*) FROM series WHERE tmdbId IS NOT NULL").fetchone()[0]
        fail = conn.execute("SELECT COUNT(*) FROM series WHERE failed = 1").fetchone()[0]
        conn.close()
        return jsonify({
            "total_episodes": eps,
            "total_series": ser,
            "matched_series": mtch,
            "failed_series": fail,
            "success_rate": f"{round(mtch/ser*100, 1)}%" if ser > 0 else "0%"
        })
    except:
        return jsonify({"error": traceback.format_exc()})

def build_all_caches():
    global _SECTION_CACHE
    _SECTION_CACHE = {}
    _rebuild_fast_memory_cache()
    build_home_recommend()

def _rebuild_fast_memory_cache():
    global _FAST_CATEGORY_CACHE
    temp = {}
    conn = get_db()
    log("CACHE", "‚öôÔ∏è Í≤ΩÎüâ Î©îÎ™®Î¶¨ Ï∫êÏãú ÎπåÎìú ÏãúÏûë")
    for cat in ["movies", "foreigntv", "koreantv", "animations_all", "air"]:
        rows_dict = {}
        all_rows = conn.execute('SELECT path, name, posterPath, year, rating, genreIds, genreNames, director, actors, tmdbId, cleanedName, yearVal, overview FROM series WHERE category = ? ORDER BY name ASC', (cat,)).fetchall()
        for row in all_rows:
            path, name, poster, year, rating, g_ids, g_names, director, actors, t_id, c_name, y_val, overview = row
            if c_name is not None:
                ct, yr = c_name, y_val
            else:
                ct, yr = clean_title_complex(name)

            group_key = f"tmdb:{t_id}" if t_id else f"name:{ct}_{yr}"
            if group_key not in rows_dict:
                try: genre_list = json.loads(g_names) if g_names else []
                except: genre_list = []
                try: genre_ids = json.loads(g_ids) if g_ids else []
                except: genre_ids = []
                try: actors_list = json.loads(actors) if actors else []
                except: actors_list = []
                rows_dict[group_key] = {
                    "path": path, "name": name, "posterPath": poster,
                    "year": year, "rating": rating, "genreIds": genre_ids, "genreNames": genre_list,
                    "director": director, "actors": actors_list, "tmdbId": t_id, "overview": overview, "movies": []
                }
        temp[cat] = list(rows_dict.values())
    conn.close()
    _FAST_CATEGORY_CACHE = temp
    log("CACHE", "‚úÖ Í≤ΩÎüâ Î©îÎ™®Î¶¨ Ï∫êÏãú ÎπåÎìú ÏôÑÎ£å")

def build_home_recommend():
    global HOME_RECOMMEND
    try:
        m = _FAST_CATEGORY_CACHE.get('movies', [])
        k = _FAST_CATEGORY_CACHE.get('koreantv', [])
        a = _FAST_CATEGORY_CACHE.get('air', [])
        combined = m + k
        unique_map = {}
        for item in combined:
            uid = item.get('tmdbId') or item.get('path')
            if uid not in unique_map:
                unique_map[uid] = item
        unique_hot_list = list(unique_map.values())
        hot_picks = random.sample(unique_hot_list, min(100, len(unique_hot_list))) if unique_hot_list else []
        seen_ids = { (p.get('tmdbId') or p.get('path')) for p in hot_picks }
        airing_picks = []
        for item in a:
            uid = item.get('tmdbId') or item.get('path')
            if uid not in seen_ids:
                airing_picks.append(item)
                if len(airing_picks) >= 100: break
        HOME_RECOMMEND = [
            {"title": "ÏßÄÍ∏à Í∞ÄÏû• Ìï´Ìïú Ïù∏Í∏∞Ïûë", "items": hot_picks},
            {"title": "Ïã§ÏãúÍ∞Ñ Î∞©ÏòÅ Ï§ë", "items": airing_picks}
        ]
        log("CACHE", f"üè† Ìôà Ï∂îÏ≤ú ÎπåÎìú ÏôÑÎ£å ({len(hot_picks)} / {len(airing_picks)})")
    except:
        traceback.print_exc()

def background_init_tasks():
    build_all_caches()

if __name__ == '__main__':
    init_db()
    threading.Thread(target=background_init_tasks, daemon=True).start()
    app.run(host='0.0.0.0', port=5000, threaded=True)
