import os, subprocess, hashlib, urllib.parse, unicodedata, threading, time, json, re, sys, traceback, shutil, requests, random, mimetypes, sqlite3
from flask import Flask, jsonify, send_from_directory, request, Response, redirect, send_file
from flask_cors import CORS
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from collections import deque

app = Flask(__name__)
CORS(app)

# MIME íƒ€ì… ì¶”ê°€ ë“±ë¡
if not mimetypes.types_map.get('.mkv'): mimetypes.add_type('video/x-matroska', '.mkv')
if not mimetypes.types_map.get('.ts'): mimetypes.add_type('video/mp2t', '.ts')
if not mimetypes.types_map.get('.tp'): mimetypes.add_type('video/mp2t', '.tp')

# --- [1. ì„¤ì • ë° ê²½ë¡œ] ---
MY_IP = "192.168.0.2"
DATA_DIR = "/volume2/video/thumbnails"
CACHE_FILE = "/volume2/video/video_cache.json"
DB_FILE = "/volume2/video/video_metadata.db"
TMDB_CACHE_DIR = "/volume2/video/tmdb_cache"
HLS_ROOT = "/dev/shm/videoplayer_hls"
CACHE_VERSION = "10.5" # ì˜í™” í´ë” êµ¬ì¡° ê°œì„  ë²„ì „

# TMDB ê´€ë ¨ ì „ì—­ ë©”ëª¨ë¦¬ ìºì‹œ
TMDB_MEMORY_CACHE = {}

# TMDB API KEY
TMDB_API_KEY = "eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiI3OGNiYWQ0ZjQ3NzcwYjYyYmZkMTcwNTA2NDIwZDQyYyIsIm5iZiI6MTY1MzY3NTU4MC45MTUsInN1YiI6IjYyOTExNjNjMTI0MjVjMDA1MjI0ZGQzNCIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.3YU0WuIx_WDo6nTRKehRtn4N5I4uCgjI1tlpkqfsUhk".strip()
TMDB_BASE_URL = "https://api.themoviedb.org/3"

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(TMDB_CACHE_DIR, exist_ok=True)
if os.path.exists(HLS_ROOT): shutil.rmtree(HLS_ROOT, ignore_errors=True)
os.makedirs(HLS_ROOT, exist_ok=True)

PARENT_VIDEO_DIR = "/volume2/video/GDS3/GDRIVE/VIDEO"
PATH_MAP = {
    "ì™¸êµ­TV": (os.path.join(PARENT_VIDEO_DIR, "ì™¸êµ­TV"), "ftv"),
    "êµ­ë‚´TV": (os.path.join(PARENT_VIDEO_DIR, "êµ­ë‚´TV"), "ktv"),
    "ì˜í™”": (os.path.join(PARENT_VIDEO_DIR, "ì˜í™”"), "movie"),
    "ì• ë‹ˆë©”ì´ì…˜": (os.path.join(PARENT_VIDEO_DIR, "ì¼ë³¸ ì• ë‹ˆë©”ì´ì…˜"), "anim_all"),
    "ë°©ì†¡ì¤‘": (os.path.join(PARENT_VIDEO_DIR, "ë°©ì†¡ì¤‘"), "air")
}

EXCLUDE_FOLDERS = ["ì„±ì¸", "19ê¸ˆ", "Adult", "@eaDir", "#recycle"]
VIDEO_EXTS = ('.mp4', '.mkv', '.avi', '.wmv', '.flv', '.ts', '.tp', '.m4v', '.m2ts', '.mov')
FFMPEG_PATH = "ffmpeg"
for p in ["/usr/local/bin/ffmpeg", "/var/packages/ffmpeg/target/bin/ffmpeg", "/usr/bin/ffmpeg"]:
    if os.path.exists(p): FFMPEG_PATH = p; break

# ë©”ëª¨ë¦¬ ìƒì˜ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ (DB ì¡°íšŒ í›„ ê°±ì‹ )
HOME_RECOMMEND = []

# ë§¤ì¹­ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ í”Œë˜ê·¸
IS_METADATA_RUNNING = False

def log(msg):
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {msg}", flush=True)

def nfc(text): return unicodedata.normalize('NFC', text) if text else ""
def nfd(text): return unicodedata.normalize('NFD', text) if text else ""

# --- [DB ê´€ë¦¬] ---
def get_db():
    conn = sqlite3.connect(DB_FILE)
    conn.row_factory = sqlite3.Row
    return conn

def init_db():
    conn = get_db()
    cursor = conn.cursor()
    # Series í…Œì´ë¸”
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS series (
            path TEXT PRIMARY KEY,
            category TEXT,
            name TEXT,
            posterPath TEXT,
            year TEXT,
            overview TEXT,
            rating TEXT,
            seasonCount INTEGER,
            genreIds TEXT,
            failed INTEGER DEFAULT 0
        )
    ''')
    # Episodes í…Œì´ë¸”
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS episodes (
            id TEXT PRIMARY KEY,
            series_path TEXT,
            title TEXT,
            videoUrl TEXT,
            thumbnailUrl TEXT,
            FOREIGN KEY (series_path) REFERENCES series (path) ON DELETE CASCADE
        )
    ''')
    # ì¸ë±ìŠ¤ ì¶”ê°€
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_series_category ON series(category)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_episodes_series ON episodes(series_path)')
    conn.commit()
    conn.close()
    log("ğŸ—„ï¸ [DB] ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

def migrate_json_to_sqlite():
    if not os.path.exists(CACHE_FILE): return
    log("ğŸšš [MIGRATE] JSON ë°ì´í„°ë¥¼ SQLiteë¡œ ì´ê´€ì„ ì‹œë„í•©ë‹ˆë‹¤...")
    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)

        conn = get_db()
        cursor = conn.cursor()

        series_count = 0
        episode_count = 0
        for key in ["air", "movies", "foreigntv", "koreantv", "animations_all"]:
            category_items = data.get(key, [])
            log(f"  ğŸ“‚ [MIGRATE] '{key}' ì¹´í…Œê³ ë¦¬ ì´ê´€ ì¤‘ ({len(category_items)}ê°œ ì‹œë¦¬ì¦ˆ)")
            for cat in category_items:
                cursor.execute('''
                    INSERT OR REPLACE INTO series (path, category, name, posterPath, year, overview, rating, seasonCount, genreIds, failed)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    cat.get('path'), key, cat.get('name'), cat.get('posterPath'),
                    cat.get('year'), cat.get('overview'), cat.get('rating'),
                    cat.get('seasonCount'), json.dumps(cat.get('genreIds', [])),
                    1 if cat.get('failed') else 0
                ))
                series_count += 1

                for m in cat.get('movies', []):
                    cursor.execute('''
                        INSERT OR REPLACE INTO episodes (id, series_path, title, videoUrl, thumbnailUrl)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (m.get('id'), cat.get('path'), m.get('title'), m.get('videoUrl'), m.get('thumbnailUrl')))
                    episode_count += 1

        conn.commit()
        conn.close()
        log(f"âœ… [MIGRATE] ì´ê´€ ì™„ë£Œ: ì‹œë¦¬ì¦ˆ {series_count}ê°œ, ì—í”¼ì†Œë“œ {episode_count}ê°œ")
        os.rename(CACHE_FILE, CACHE_FILE + ".bak")
        log(f"ğŸ“¦ [MIGRATE] ê¸°ì¡´ JSON íŒŒì¼ì„ '{CACHE_FILE}.bak'ìœ¼ë¡œ ë°±ì—…í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        log(f"âŒ [MIGRATE] ì´ê´€ ì‹¤íŒ¨: {str(e)}")
        traceback.print_exc()

# --- [ì •ê·œì‹ ë° í´ë¦¬ë‹] ---
REGEX_EXT = re.compile(r'\.[a-zA-Z0-9]{2,4}$')
REGEX_YEAR = re.compile(r'\((19|20)\d{2}\)|(?<!\d)(19|20)\d{2}(?!\d)')
REGEX_EP_MARKER = re.compile(r'(?i)(?:^|[.\s_]|(?<=[ê°€-í£]))(?:S\d+E\d+|S\d+|E\d+|\d+\s*(?:í™”|íšŒ|ê¸°)|Season\s*\d+|Part\s*\d+|Disk\s*\d+|Disc\s*\d+|CD\s*\d+).*')
# [ìˆ˜ì •] ê¸ˆì§€ëœ ì œëª© í´ë”ì— 'ì œëª©', 'UHD', 'ìµœì‹ ' ë“± ì¶”ê°€í•˜ì—¬ í•´ë‹¹ í´ë”ê°€ í•˜ë‚˜ì˜ ì‹œë¦¬ì¦ˆë¡œ ë¬¶ì´ì§€ ì•Šê²Œ í•¨
REGEX_FORBIDDEN_TITLE = re.compile(r'(?i)^\s*(Season\s*\d+|Part\s*\d+|EP\s*\d+|\d+í™”|\d+íšŒ|\d+ê¸°|ì‹œì¦Œ\s*\d+|S\d+|E\d+|Disk\s*\d+|Disc\s*\d+|CD\s*\d+|Specials?|Extras?|Bonus|ë¯¸ë¶„ë¥˜|ê¸°íƒ€|ìƒˆ\s*í´ë”|VIDEO|GDS3|GDRIVE|NAS|share|ì˜í™”|ì™¸êµ­TV|êµ­ë‚´TV|ì• ë‹ˆë©”ì´ì…˜|ë°©ì†¡ì¤‘|ì œëª©|UHD|ìµœì‹ |ìµœì‹ ì‘|ìµœì‹ ì˜í™”|4K|1080P|720P)\s*$', re.I)

def natural_sort_key(s):
    if s is None: return []
    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\d+)', nfc(str(s)))]

def clean_title_complex(title):
    if not title: return "", None
    title = nfc(title)
    title = re.sub(r'^\d+[\s.]+(?=.+)', '', title).strip()
    cleaned = REGEX_EXT.sub('', title)
    year_match = REGEX_YEAR.search(cleaned)
    year = year_match.group().replace('(', '').replace(')', '') if year_match else None
    cleaned = REGEX_YEAR.sub(' ', cleaned)
    cleaned = REGEX_EP_MARKER.sub(' ', cleaned)
    cleaned = re.sub(r'\[.*?\]|\(.*?\)', ' ', cleaned)
    cleaned = re.sub(r'(?<!\d)\.|\.(?!\d)', ' ', cleaned)
    cleaned = re.sub(r'[\_\-!?ã€ã€‘ã€ã€ã€Œã€"\'#@*â€»:]', ' ', cleaned)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned, year

# --- [ìœ í‹¸ë¦¬í‹°] ---
def load_tmdb_memory_cache():
    if not os.path.exists(TMDB_CACHE_DIR): return
    for f in os.listdir(TMDB_CACHE_DIR):
        if f.endswith(".json"):
            try:
                with open(os.path.join(TMDB_CACHE_DIR, f), 'r', encoding='utf-8') as file:
                    data = json.load(file)
                    if not data.get('failed'): TMDB_MEMORY_CACHE[f.replace(".json", "")] = data
            except: pass

def get_real_path(path):
    if not path or os.path.exists(path): return path
    if os.path.exists(nfc(path)): return nfc(path)
    if os.path.exists(nfd(path)): return nfd(path)
    return path

def resolve_nas_path(app_path):
    app_path = nfc(urllib.parse.unquote(app_path or ""))
    parts = app_path.split('/')
    if parts and parts[0] in PATH_MAP:
        base_dir, type_code = PATH_MAP[parts[0]]
        return get_real_path(os.path.join(base_dir, "/".join(parts[1:]))), type_code
    return None, None

def get_meaningful_name(path):
    curr = nfc(path)
    while True:
        name = os.path.basename(curr)
        if not name: break
        if not REGEX_FORBIDDEN_TITLE.match(name) and name.lower() not in ["video", "share"]: return name
        parent = os.path.dirname(curr)
        if parent == curr: break
        curr = parent
    return os.path.basename(path)

def get_series_root_path(path, rel_base):
    curr = nfc(path); rel_base = nfc(rel_base)
    while True:
        name = os.path.basename(curr)
        if not name or curr == rel_base: break
        if not REGEX_FORBIDDEN_TITLE.match(name) and name.lower() not in ["video", "share"]: return nfc(os.path.relpath(curr, rel_base))
        parent = os.path.dirname(curr)
        if parent == curr: break
        curr = parent
    return nfc(os.path.relpath(path, rel_base))

# --- [TMDB ë° ë©”íƒ€ë°ì´í„°] ---
def get_tmdb_info_server(title, ignore_cache=False):
    if not title: return {"failed": True}
    h = hashlib.md5(nfc(title).encode()).hexdigest(); cp = os.path.join(TMDB_CACHE_DIR, f"{h}.json")
    if not ignore_cache and h in TMDB_MEMORY_CACHE: return TMDB_MEMORY_CACHE[h]
    if not ignore_cache and os.path.exists(cp):
        try:
            with open(cp, 'r', encoding='utf-8') as f:
                data = json.load(f); TMDB_MEMORY_CACHE[h] = data; return data
        except: pass
    ct, year = clean_title_complex(title)
    if not ct or REGEX_FORBIDDEN_TITLE.match(ct): return {"failed": True, "forbidden": True}
    params = {"query": ct, "language": "ko-KR", "include_adult": "true", "region": "KR"}
    if year: params["year"] = year
    headers = {"Authorization": f"Bearer {TMDB_API_KEY}"}
    try:
        resp = requests.get(f"{TMDB_BASE_URL}/search/multi", params=params, headers=headers, timeout=5).json()
        results = [r for r in resp.get('results', []) if r.get('media_type') in ['movie', 'tv']]
        if results:
            best = results[0]; m_type, t_id = best.get('media_type'), best.get('id')
            d_resp = requests.get(f"{TMDB_BASE_URL}/{m_type}/{t_id}?language=ko-KR&append_to_response=content_ratings", params=params, headers=headers, timeout=5).json()
            year_val = (d_resp.get('release_date') or d_resp.get('first_air_date') or "").split('-')[0]
            rating = None
            if 'content_ratings' in d_resp:
                kr = next((r['rating'] for r in d_resp['content_ratings'].get('results', []) if r.get('iso_3166_1') == 'KR'), None)
                if kr: rating = f"{kr}+" if kr.isdigit() else kr
            info = {"genreIds": [g['id'] for g in d_resp.get('genres', [])], "posterPath": d_resp.get('poster_path'), "year": year_val, "overview": d_resp.get('overview'), "rating": rating, "seasonCount": d_resp.get('number_of_seasons'), "failed": False}
            TMDB_MEMORY_CACHE[h] = info
            with open(cp, 'w', encoding='utf-8') as f: json.dump(info, f, ensure_ascii=False)
            return info
    except: pass
    return {"failed": True}

# --- [ìŠ¤ìº” ë° íƒìƒ‰ ë¡œì§ (SQLite)] ---
def scan_recursive_to_db(bp, prefix, category):
    log(f"  ğŸ“‚ '{category}' ì¹´í…Œê³ ë¦¬ ìŠ¤ìº” ì‹œì‘: {bp}")
    base = nfc(get_real_path(bp)); exts = VIDEO_EXTS; all_files = []
    stack = [base]
    visited_dirs = set()

    # 1ë‹¨ê³„: íŒŒì¼ ì‹œìŠ¤í…œ ë’¤ì§€ê¸° (ì‹¬ë³¼ë¦­ ë§í¬ì— ì˜í•œ ì¤‘ë³µ íƒìƒ‰ ë°©ì§€)
    find_count = 0
    while stack:
        curr = stack.pop()
        real_curr = os.path.realpath(curr)
        if real_curr in visited_dirs: continue
        visited_dirs.add(real_curr)

        try:
            with os.scandir(curr) as it:
                for entry in sorted(list(it), key=lambda e: natural_sort_key(e.name)):
                    if entry.is_dir():
                        if not any(ex in entry.name for ex in EXCLUDE_FOLDERS) and not entry.name.startswith('.'):
                            stack.append(entry.path)
                    elif entry.is_file() and entry.name.lower().endswith(exts):
                        all_files.append(nfc(entry.path))
                        find_count += 1
                        if find_count % 1000 == 0:
                            log(f"    ğŸ” íŒŒì¼ íƒìƒ‰ ì¤‘... í˜„ì¬ {find_count}ê°œ ë°œê²¬")
        except: pass

    log(f"  ğŸ” '{category}' íƒìƒ‰ ì™„ë£Œ (ì´ {len(all_files)}ê°œ). ì´ì œ DB ì •ë³´ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.")
    conn = get_db()
    cursor = conn.cursor()

    series_map = {}
    db_update_count = 0
    for fp in all_files:
        dp = nfc(os.path.dirname(fp)); rel_path = get_series_root_path(dp, base)

        # [ìˆ˜ì •] ì˜í™” ì¹´í…Œê³ ë¦¬ ë¡œì§ ê°•í™”: ê° íŒŒì¼ì„ ê°œë³„ ì‹œë¦¬ì¦ˆ(ì˜í™”)ë¡œ ì¸ì‹í•˜ë„ë¡ ì²˜ë¦¬
        if category == 'movies':
            clean_name, _ = clean_title_complex(os.path.basename(fp))
            # í´ë”ê°€ 'ì œëª©', 'ìµœì‹ ' ë“± genericí•œ ê²½ìš° íŒŒì¼ëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ìƒì„±
            parent_folder = os.path.basename(dp)
            if REGEX_FORBIDDEN_TITLE.match(parent_folder):
                full_series_path = f"{category}/{rel_path}/{clean_name}".replace("//", "/")
                name = clean_name
            else:
                full_series_path = f"{category}/{rel_path}"
                name = get_meaningful_name(dp)
        else:
            full_series_path = f"{category}/{rel_path}"
            name = get_meaningful_name(dp)

        if full_series_path not in series_map:
            cursor.execute('INSERT OR IGNORE INTO series (path, category, name) VALUES (?, ?, ?)', (full_series_path, category, name))
            series_map[full_series_path] = True

        movie_id = hashlib.md5(fp.encode()).hexdigest()
        cursor.execute('''
            INSERT OR REPLACE INTO episodes (id, series_path, title, videoUrl, thumbnailUrl)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            movie_id, full_series_path, os.path.basename(fp),
            f"/video_serve?type={prefix}&path={urllib.parse.quote(os.path.relpath(fp, base))}",
            f"/thumb_serve?type={prefix}&id={movie_id}&path={urllib.parse.quote(os.path.relpath(fp, base))}"
        ))
        db_update_count += 1
        if db_update_count % 1000 == 0:
            log(f"    â³ DB ì—…ë°ì´íŠ¸ ì§„í–‰ ì¤‘... ({db_update_count}/{len(all_files)})")
            conn.commit()

    conn.commit()
    conn.close()
    log(f"  âœ… '{category}' ëª¨ë“  DB ê°±ì‹  ì™„ë£Œ.")

def perform_full_scan(cache_keys=None):
    keys = cache_keys if cache_keys else [("ì• ë‹ˆë©”ì´ì…˜", "animations_all"), ("ì™¸êµ­TV", "foreigntv"), ("êµ­ë‚´TV", "koreantv"), ("ì˜í™”", "movies"), ("ë°©ì†¡ì¤‘", "air")]
    log(f"ğŸ”„ [SCAN] NAS ì „ì²´ ì¬ìŠ¤ìº” ì‹œì‘: {keys}")

    # êµ¬í˜• ì¹´í…Œê³ ë¦¬ ëª…ì¹­(ì˜ˆ: movie)ìœ¼ë¡œ ì¸í•œ ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ í˜„ì¬ í™œì„± ì¹´í…Œê³ ë¦¬ ì™¸ì—ëŠ” ì •ë¦¬
    active_cats = [k[1] for k in keys]
    conn = get_db()
    conn.execute(f"DELETE FROM series WHERE category NOT IN ({','.join(['?']*len(active_cats))})", active_cats)
    conn.commit()
    conn.close()

    for label, cache_key in keys:
        path, prefix = PATH_MAP[label]
        scan_recursive_to_db(path, prefix, cache_key)

    log("ğŸ§  [SCAN] ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ê°±ì‹  ì¤‘...")
    build_home_recommend()
    log("ğŸ [SCAN] ì „ì²´ ìŠ¤ìº” ì‘ì—… ì™„ë£Œ")
    threading.Thread(target=fetch_metadata_async, daemon=True).start()

def fetch_metadata_async(force_all=False):
    global IS_METADATA_RUNNING
    if IS_METADATA_RUNNING:
        log("âš ï¸ [METADATA] ì´ë¯¸ ë§¤ì¹­ ì‘ì—…ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì¤‘ë³µ ì‹¤í–‰ì„ ë°©ì§€í•©ë‹ˆë‹¤.")
        return

    IS_METADATA_RUNNING = True
    log("ğŸš€ [METADATA] ë°±ê·¸ë¼ìš´ë“œ TMDB ë§¤ì¹­ ì‹œì‘")
    try:
        conn = get_db()
        cursor = conn.cursor()

        cursor.execute('SELECT COUNT(*) as cnt FROM series')
        total_in_db = cursor.fetchone()['cnt']
        cursor.execute('SELECT COUNT(*) as cnt FROM series WHERE posterPath IS NOT NULL OR failed = 1')
        already_completed = cursor.fetchone()['cnt']

        if force_all:
            cursor.execute('SELECT path, name FROM series')
        else:
            cursor.execute('SELECT path, name FROM series WHERE posterPath IS NULL AND failed = 0')

        tasks = cursor.fetchall()
        conn.close()

        total_tasks = len(tasks)
        if total_tasks == 0:
            log("ğŸ [METADATA] ë§¤ì¹­í•  ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            IS_METADATA_RUNNING = False
            return

        log(f"ğŸ“Š [METADATA] ì´ {total_tasks}ê°œì˜ ì‹ ê·œ í•­ëª©ì„ TMDBì™€ ë§¤ì¹­í•©ë‹ˆë‹¤. (í˜„ì¬ ì „ì²´ ì™„ë£Œ: {already_completed}/{total_in_db})")

        count = 0
        success_count = 0
        fail_count = 0
        start_time = time.time()

        for row in tasks:
            path, name = row['path'], row['name']
            info = get_tmdb_info_server(name, ignore_cache=force_all)

            conn = get_db()
            cursor = conn.cursor()
            cursor.execute('''
                UPDATE series SET
                    posterPath = ?, year = ?, overview = ?, rating = ?,
                    seasonCount = ?, genreIds = ?, failed = ?
                WHERE path = ?
            ''', (
                info.get('posterPath'), info.get('year'), info.get('overview'),
                info.get('rating'), info.get('seasonCount'),
                json.dumps(info.get('genreIds', [])),
                1 if info.get('failed') else 0,
                path
            ))
            conn.commit()
            conn.close()

            count += 1
            if not info.get('failed'): success_count += 1
            else: fail_count += 1

            if count % 10 == 0 or count == total_tasks:
                elapsed = time.time() - start_time
                speed = count / elapsed if elapsed > 0 else 0
                remaining = (total_tasks - count) / speed if speed > 0 else 0
                current_total_progress = already_completed + count
                percent = (current_total_progress / total_in_db * 100) if total_in_db > 0 else 0
                log(f"  â³ ì§„í–‰ì¤‘: {current_total_progress}/{total_in_db} ({percent:.1f}%) - [ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {fail_count}]")

            time.sleep(0.05)
        log(f"ğŸ [METADATA] ëª¨ë“  ì‘ì—… ì™„ë£Œ")
        build_home_recommend()
    finally:
        IS_METADATA_RUNNING = False

def build_home_recommend():
    global HOME_RECOMMEND
    log("ğŸ  [HOME] ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ êµ¬ì¶• ì¤‘...")
    try:
        conn = get_db()
        cursor = conn.cursor()

        def get_series_with_first_movie(sql_filter):
            # [ìˆ˜ì •] ì˜í™” ì¤‘ë³µ ì œê±° ê¸°ì¤€ ê°•í™”: posterPathê°€ ê°™ê±°ë‚˜ nameì´ ê°™ìœ¼ë©´ í•˜ë‚˜ë¡œ ê·¸ë£¹í™”
            group_by_clause = "GROUP BY COALESCE(s.posterPath, s.name)" if "movies" in sql_filter or "1=1" in sql_filter else "GROUP BY s.path"
            sql = f'''
                SELECT s.*, e.id as movie_id, e.title as movie_title, e.videoUrl, e.thumbnailUrl
                FROM series s
                LEFT JOIN (
                    SELECT * FROM episodes GROUP BY series_path
                ) e ON s.path = e.series_path
                WHERE {sql_filter}
                {group_by_clause}
                ORDER BY RANDOM() LIMIT 20
            '''
            cursor.execute(sql)
            results = []
            for row in cursor.fetchall():
                item = dict(row)
                if item.get('genreIds'): item['genreIds'] = json.loads(item['genreIds'])
                cursor.execute('SELECT * FROM episodes WHERE series_path = ?', (item['path'],))
                item['movies'] = [dict(r) for r in cursor.fetchall()]
                results.append(item)
            return results

        all_p = get_series_with_first_movie("1=1")
        m = get_series_with_first_movie("category = 'movies'")
        kf = get_series_with_first_movie("category IN ('koreantv', 'foreigntv')")

        conn.close()
        HOME_RECOMMEND = [
            {"title": "ì§€ê¸ˆ ê°€ì¥ í•«í•œ ì¸ê¸°ì‘", "items": all_p},
            {"title": "ë°©ê¸ˆ ì˜¬ë¼ì˜¨ ìµœì‹  ì˜í™”", "items": m},
            {"title": "ì§€ê¸ˆ ì¸ê¸° ìˆëŠ” ì‹œë¦¬ì¦ˆ", "items": kf}
        ]
        log(f"ğŸ  [HOME] ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ê°±ì‹  ì™„ë£Œ")
    except Exception as e:
        log(f"âŒ [HOME] ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")

# --- [API ì—”ë“œí¬ì¸íŠ¸] ---
@app.route('/home')
def get_home(): return jsonify(HOME_RECOMMEND)

def get_series_list_api(category, filter_keyword=None):
    conn = get_db()
    cursor = conn.cursor()
    # [ìˆ˜ì •] ì˜í™” ì¹´í…Œê³ ë¦¬ëŠ” í¬ìŠ¤í„° ê²½ë¡œê°€ ê°™ê±°ë‚˜ ì´ë¦„ì´ ê°™ìœ¼ë©´ í•˜ë‚˜ë¡œ ë¬¶ì–´ ì¤‘ë³µ ë…¸ì¶œ ë°©ì§€
    group_by = "GROUP BY COALESCE(s.posterPath, s.name)" if category == "movies" else "GROUP BY s.path"

    query = f'''
        SELECT s.* FROM series s WHERE s.category = ?
    '''
    params = [category]
    if filter_keyword:
        query += ' AND (s.path LIKE ? OR s.name LIKE ?)'
        params.extend([f'%{filter_keyword}%', f'%{filter_keyword}%'])

    query += f' {group_by}'

    cursor.execute(query, params)
    rows = []
    for row in cursor.fetchall():
        item = dict(row)
        if item.get('genreIds'): item['genreIds'] = json.loads(item['genreIds'])
        cursor.execute("SELECT * FROM episodes WHERE series_path = ?", (item['path'],))
        item['movies'] = [dict(r) for r in cursor.fetchall()]
        rows.append(item)

    conn.close()
    return sorted(rows, key=lambda x: natural_sort_key(x['name']))

@app.route('/list')
def get_list_api():
    path = request.args.get('path', '')
    # ì ‘ë‘ì‚¬ ì œê±° (ì˜í™”/movies/Path -> movies/Path)
    for prefix in ["ì˜í™”/", "ì™¸êµ­TV/", "êµ­ë‚´TV/", "ì• ë‹ˆë©”ì´ì…˜/", "ë°©ì†¡ì¤‘/"]:
        if path.startswith(prefix):
            path = path[len(prefix):]
            break

    if not path or path in ["movies", "foreigntv", "koreantv", "animations_all", "air"]:
        return jsonify(get_series_list_api(path or "movies"))

    conn = get_db(); cursor = conn.cursor()
    cursor.execute('SELECT * FROM series WHERE path = ?', (path,))
    row = cursor.fetchone()
    if not row:
        conn.close(); return jsonify([])

    series = dict(row)
    if series.get('genreIds'): series['genreIds'] = json.loads(series['genreIds'])
    cursor.execute('SELECT * FROM episodes WHERE series_path = ?', (path,))
    series['movies'] = [dict(r) for r in cursor.fetchall()]
    conn.close()
    return jsonify([series])

def get_series_list_filtered(category, filter_keyword=None):
    synonyms = {
        "ë¯¸êµ­": ["ë¯¸êµ­", "ë¯¸ë“œ", "us"], "ì¤‘êµ­": ["ì¤‘êµ­", "ì¤‘ë“œ", "cn"], "ì¼ë³¸": ["ì¼ë³¸", "ì¼ë“œ", "jp"],
        "ê¸°íƒ€": ["ê¸°íƒ€", "etc"], "ë‹¤í": ["ë‹¤í", "docu"], "ë“œë¼ë§ˆ": ["ë“œë¼ë§ˆ"], "ì‹œíŠ¸ì½¤": ["ì‹œíŠ¸ì½¤"],
        "ì˜ˆëŠ¥": ["ì˜ˆëŠ¥"], "êµì–‘": ["êµì–‘"], "uhd": ["uhd", "4k"],
        "latest": ["latest", "ìµœì‹ ", "ìµœì‹ ì‘", "ìµœì‹ ì˜í™”", "ìµœê·¼"], "title": ["title", "ì œëª©"]
    }

    conn = get_db()
    cursor = conn.cursor()
    # [ìˆ˜ì •] ì¤‘ë³µ ì œê±° ê¸°ì¤€ ê°•í™”
    group_by = "GROUP BY COALESCE(s.posterPath, s.name)" if category == "movies" else "GROUP BY s.path"

    filter_clause = ""
    params = [category]
    if filter_keyword:
        targets = synonyms.get(filter_keyword, [filter_keyword])
        filter_parts = []
        for t in targets:
            filter_parts.append("(s.path LIKE ? OR s.name LIKE ? OR s.path LIKE ? OR s.path LIKE ?)")
            params.extend([f'%/{t}/%', f'%{t}%', f'%/{t}', f'{t}/%'])
        filter_clause = " AND (" + " OR ".join(filter_parts) + ")"

    query = f'''
        SELECT s.* FROM series s WHERE s.category = ? {filter_clause} {group_by}
    '''

    cursor.execute(query, params)
    rows = []
    for row in cursor.fetchall():
        item = dict(row)
        if item.get('genreIds'): item['genreIds'] = json.loads(item['genreIds'])
        cursor.execute("SELECT * FROM episodes WHERE series_path = ?", (item['path'],))
        item['movies'] = [dict(r) for r in cursor.fetchall()]
        rows.append(item)

    conn.close()
    return sorted(rows, key=lambda x: natural_sort_key(x['name']))

@app.route('/air')
def get_air_all(): return jsonify(get_series_list_api("air"))
@app.route('/air_animations')
def get_air_animations(): return jsonify(get_series_list_api("air", "ì• ë‹ˆë©”ì´ì…˜"))
@app.route('/air_dramas')
def get_air_dramas(): return jsonify(get_series_list_api("air", "ë“œë¼ë§ˆ"))

@app.route('/foreigntv')
def get_ftv(): return jsonify(get_series_list_api("foreigntv"))
@app.route('/ftv_us')
def get_ftv_us(): return jsonify(get_series_list_filtered("foreigntv", "ë¯¸êµ­"))
@app.route('/ftv_cn')
def get_ftv_cn(): return jsonify(get_series_list_filtered("foreigntv", "ì¤‘êµ­"))
@app.route('/ftv_jp')
def get_ftv_jp(): return jsonify(get_series_list_filtered("foreigntv", "ì¼ë³¸"))
@app.route('/ftv_docu')
def get_ftv_docu(): return jsonify(get_series_list_filtered("foreigntv", "ë‹¤í"))
@app.route('/ftv_etc')
def get_ftv_etc(): return jsonify(get_series_list_filtered("foreigntv", "ê¸°íƒ€"))

@app.route('/koreantv')
def get_ktv(): return jsonify(get_series_list_api("koreantv"))
@app.route('/ktv_drama')
def get_ktv_drama(): return jsonify(get_series_list_filtered("koreantv", "ë“œë¼ë§ˆ"))
@app.route('/ktv_sitcom')
def get_ktv_sitcom(): return jsonify(get_series_list_filtered("koreantv", "ì‹œíŠ¸ì½¤"))
@app.route('/ktv_variety')
def get_ktv_variety(): return jsonify(get_series_list_filtered("koreantv", "ì˜ˆëŠ¥"))
@app.route('/ktv_edu')
def get_ktv_edu(): return jsonify(get_series_list_filtered("koreantv", "êµì–‘"))
@app.route('/ktv_docu')
def get_ktv_docu(): return jsonify(get_series_list_filtered("koreantv", "ë‹¤íë©˜í„°ë¦¬"))

@app.route('/animations_all')
def get_anim(): return jsonify(get_series_list_api("animations_all"))
@app.route('/anim_raftel')
def get_anim_r(): return jsonify(get_series_list_filtered("animations_all", "ë¼í”„í…”"))
@app.route('/anim_series')
def get_anim_s(): return jsonify(get_series_list_filtered("animations_all", "ì‹œë¦¬ì¦ˆ"))

@app.route('/movies')
def get_movies(): return jsonify(get_series_list_api("movies"))
@app.route('/movies_uhd')
def get_movies_uhd(): return jsonify(get_series_list_filtered("movies", "uhd"))
@app.route('/movies_latest')
def get_movies_latest(): return jsonify(get_series_list_filtered("movies", "latest"))
@app.route('/movies_title')
def get_movies_title(): return jsonify(get_series_list_filtered("movies", "title"))

@app.route('/rescan_broken')
def rescan_broken():
    log("âš ï¸ ì˜í™”/ë°©ì†¡ì¤‘ ì¹´í…Œê³ ë¦¬ ì¦‰ì‹œ ì¬íƒìƒ‰ ìš”ì²­ ìˆ˜ì‹ ")
    threading.Thread(target=perform_full_scan, args=([("ì˜í™”", "movies"), ("ë°©ì†¡ì¤‘", "air")],), daemon=True).start()
    return jsonify({"status": "success", "message": "ì˜í™”/ë°©ì†¡ì¤‘ ì¹´í…Œê³ ë¦¬ ì¬íƒìƒ‰ ì‹œì‘"})

@app.route('/rematch_metadata')
def rescan_metadata():
    log("âš ï¸ TMDB ë©”íƒ€ë°ì´í„° ì „ì²´ ì¬ë§¤ì¹­ ìš”ì²­ ìˆ˜ì‹ ")
    threading.Thread(target=fetch_metadata_async, args=(True,), daemon=True).start()
    return jsonify({"status": "success", "message": "TMDB ë©”íƒ€ë°ì´í„° ì „ì²´ ì¬ë§¤ì¹­ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)"})

@app.route('/api/series_detail')
def get_series_detail_api():
    path = request.args.get('path')
    if not path: return jsonify(None)

    clean_path = path
    for prefix in ["ì˜í™”/", "ì™¸êµ­TV/", "êµ­ë‚´TV/", "ì• ë‹ˆë©”ì´ì…˜/", "ë°©ì†¡ì¤‘/"]:
        if clean_path.startswith(prefix):
            clean_path = clean_path[len(prefix):]
            break

    conn = get_db(); cursor = conn.cursor()
    cursor.execute('SELECT * FROM series WHERE path = ?', (clean_path,))
    row = cursor.fetchone()
    if not row:
        conn.close(); return jsonify(None)
    series = dict(row)
    if series.get('genreIds'): series['genreIds'] = json.loads(series['genreIds'])

    if series.get('posterPath'):
        cursor.execute("SELECT e.* FROM episodes e JOIN series s ON e.series_path = s.path WHERE s.posterPath = ?", (series['posterPath'],))
    else:
        cursor.execute("SELECT e.* FROM episodes e JOIN series s ON e.series_path = s.path WHERE s.name = ?", (series['name'],))

    eps = []
    seen = set()
    for r in cursor.fetchall():
        if r['videoUrl'] not in seen:
            eps.append(dict(r))
            seen.add(r['videoUrl'])
    series['movies'] = sorted(eps, key=lambda x: natural_sort_key(x['title']))
    conn.close()
    return jsonify(series)

@app.route('/search')
def search_videos():
    q = request.args.get('q', '').lower()
    if not q: return jsonify([])
    conn = get_db(); cursor = conn.cursor()
    query = f'''
        SELECT s.* FROM series s
        WHERE s.name LIKE ? OR s.path LIKE ?
        GROUP BY COALESCE(s.posterPath, s.name)
    '''
    cursor.execute(query, (f'%{q}%', f'%{q}%'))
    rows = []
    for row in cursor.fetchall():
        item = dict(row)
        if item.get('genreIds'): item['genreIds'] = json.loads(item['genreIds'])
        cursor.execute("SELECT * FROM episodes WHERE series_path = ?", (item['path'],))
        item['movies'] = [dict(r) for r in cursor.fetchall()]
        rows.append(item)
    conn.close()
    return jsonify(rows)

@app.route('/video_serve')
def video_serve():
    path, prefix = request.args.get('path'), request.args.get('type')
    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
        return send_file(get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path)))), conditional=True)
    except: return "Not Found", 404

@app.route('/thumb_serve')
def thumb_serve():
    path, prefix, tid, t = request.args.get('path'), request.args.get('type'), request.args.get('id'), request.args.get('t', default="300")
    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
        vp = get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path))))
        if os.path.isdir(vp):
            fs = sorted([f for f in os.listdir(vp) if f.lower().endswith(VIDEO_EXTS)])
            vp = os.path.join(vp, fs[0]) if fs else vp
        tp = os.path.join(DATA_DIR, f"seek_{tid}_{t}.jpg")
        if not os.path.exists(tp):
            subprocess.run([FFMPEG_PATH, "-y", "-ss", t, "-i", vp, "-vframes", "1", "-q:v", "5", "-vf", "scale=320:-1", tp], timeout=15)
        return send_file(tp, mimetype='image/jpeg') if os.path.exists(tp) else ("Not Found", 404)
    except: return "Not Found", 404

# --- [ìµœì í™” ì „ìš© ê³ ì† API ì¶”ê°€ (ê·œì¹™ ì¤€ìˆ˜)] ---
# ê¸°ì¡´ ë¡œì§ì„ ë³´ì¡´í•˜ë©°, ëŒ€ìš©ëŸ‰ ì¹´í…Œê³ ë¦¬ ë¡œë”© ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì—í”¼ì†Œë“œ ìµœì†Œí™” ë° í˜ì´ì§• ê¸°ëŠ¥ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
CAT_MAP_V2 = { "ì˜í™”": "movies", "ì™¸êµ­TV": "foreigntv", "êµ­ë‚´TV": "koreantv", "ì• ë‹ˆë©”ì´ì…˜": "animations_all", "ë°©ì†¡ì¤‘": "air" }

@app.route('/api/fast_list')
def get_fast_list():
    path_arg = request.args.get('path', 'movies')
    limit = int(request.args.get('limit', 100))
    offset = int(request.args.get('offset', 0))

    # ì¹´í…Œê³ ë¦¬ì™€ í•„í„° í‚¤ì›Œë“œ ë¶„ë¦¬
    target_cat = "movies"
    filter_q = ""
    for kor, eng in CAT_MAP_V2.items():
        if path_arg.startswith(kor):
            target_cat = eng
            filter_q = path_arg[len(kor):].strip("/")
            break
        elif path_arg.startswith(eng):
            target_cat = eng
            filter_q = path_arg[len(eng):].strip("/")
            break

    conn = get_db(); cursor = conn.cursor()
    params = [target_cat]

    # [ìˆ˜ì •] í•„í„°ë§ì„ 'í¬í•¨' ê²€ìƒ‰(%keyword%)ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ëˆ„ë½ ë°©ì§€
    filter_clause = ""
    if filter_q:
        filter_clause = " AND (s.path LIKE ? OR s.name LIKE ?)"
        params.extend([f'%{filter_q}%', f'%{filter_q}%'])

    # [ìˆ˜ì •] ì˜í™”ëŠ” ì¤‘ë³µ ì œê±°(í¬ìŠ¤í„° ê¸°ì¤€), TV/ì• ë‹ˆ ë“±ì€ í´ë”ë³„ ë…¸ì¶œ(ê¸°ì¡´ ê·œì¹™ ì¤€ìˆ˜)
    group_by = "GROUP BY COALESCE(s.posterPath, s.name)" if target_cat == "movies" else "GROUP BY s.path"

    query = f"SELECT * FROM series s WHERE category = ? {filter_clause} {group_by} ORDER BY name ASC LIMIT ? OFFSET ?"
    params.extend([limit, offset])

    cursor.execute(query, params); rows = []
    for row in cursor.fetchall():
        item = dict(row)
        if item.get('genreIds'): item['genreIds'] = json.loads(item['genreIds'])
        cursor.execute("SELECT * FROM episodes WHERE series_path = ? LIMIT 1", (item['path'],))
        ep = cursor.fetchone()
        item['movies'] = [dict(ep)] if ep else []
        rows.append(item)
    conn.close()
    return jsonify(rows)

@app.route('/api/fast_detail')
def get_fast_detail():
    path = request.args.get('path', '')
    for kor, eng in CAT_MAP_V2.items():
        if path.startswith(kor):
            path = path.replace(kor, eng, 1).strip("/")
            break
    conn = get_db(); cursor = conn.cursor()
    cursor.execute("SELECT * FROM series WHERE path = ?", (path,))
    row = cursor.fetchone()
    if not row: conn.close(); return jsonify(None)
    series = dict(row); items = []
    if series.get('genreIds'): series['genreIds'] = json.loads(series['genreIds'])
    # ìƒì„¸ í™”ë©´ìš© ì—í”¼ì†Œë“œ í•©ì‚° ë¡œì§
    sql = "SELECT e.* FROM episodes e JOIN series s ON e.series_path = s.path WHERE "
    sql += "s.posterPath = ?" if series.get('posterPath') else "s.name = ?"
    cursor.execute(sql, (series.get('posterPath') or series['name'],))
    seen = set()
    for r in cursor.fetchall():
        if r['videoUrl'] not in seen: items.append(dict(r)); seen.add(r['videoUrl'])
    series['movies'] = sorted(items, key=lambda x: natural_sort_key(x['title']))
    conn.close()
    return jsonify(series)

if __name__ == '__main__':
    log(f"ğŸ“º NAS Server ì‹œì‘ (SQLite ê¸°ë°˜ ì¤‘ë³µ ë° êµ¬ì¡° ìµœì í™” ë²„ì „)")
    init_db()
    migrate_json_to_sqlite()
    load_tmdb_memory_cache()
    build_home_recommend()
    threading.Thread(target=perform_full_scan, daemon=True).start()
    app.run(host='0.0.0.0', port=5000, threaded=True)
