import os, subprocess, hashlib, urllib.parse, unicodedata, threading, time, json, re, sys, traceback, shutil, requests, random, mimetypes
from flask import Flask, jsonify, send_from_directory, request, Response, redirect, send_file
from flask_cors import CORS
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from collections import deque

app = Flask(__name__)
CORS(app)

# MIME ÌÉÄÏûÖ Ï∂îÍ∞Ä Îì±Î°ù (Ïû¨ÏÉù Í¥ÄÎ†® Î¨∏Ï†ú Î∞©ÏßÄ)
if not mimetypes.types_map.get('.mkv'): mimetypes.add_type('video/x-matroska', '.mkv')
if not mimetypes.types_map.get('.ts'): mimetypes.add_type('video/mp2t', '.ts')
if not mimetypes.types_map.get('.tp'): mimetypes.add_type('video/mp2t', '.tp')

# --- [1. ÏÑ§Ï†ï Î∞è Í≤ΩÎ°ú] ---
MY_IP = "192.168.0.2"
DATA_DIR = "/volume2/video/thumbnails"
CACHE_FILE = "/volume2/video/video_cache.json"
TMDB_CACHE_DIR = "/volume2/video/tmdb_cache"
HLS_ROOT = "/dev/shm/videoplayer_hls"
CACHE_VERSION = "9.7" # Í∏∞Ï°¥ Î≤ÑÏ†Ñ Ïú†ÏßÄ

# TMDB Í¥ÄÎ†® Ï†ÑÏó≠ Î©îÎ™®Î¶¨ Ï∫êÏãú (Ï∂îÍ∞Ä)
TMDB_MEMORY_CACHE = {}

# TMDB API KEY
TMDB_API_KEY = "eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiI3OGNiYWQ0ZjQ3NzcwYjYyYmZkMTcwNTA2NDIwZDQyYyIsIm5iZiI6MTY1MzY3NTU4MC45MTUsInN1YiI6IjYyOTExNjNjMTI0MjVjMDA1MjI0ZGQzNCIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.3YU0WuIx_WDo6nTRKehRtn4N5I4uCgjI1tlpkqfsUhk".strip()
TMDB_BASE_URL = "https://api.themoviedb.org/3"

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(TMDB_CACHE_DIR, exist_ok=True)
if os.path.exists(HLS_ROOT): shutil.rmtree(HLS_ROOT, ignore_errors=True)
os.makedirs(HLS_ROOT, exist_ok=True)

PARENT_VIDEO_DIR = "/volume2/video/GDS3/GDRIVE/VIDEO"
PATH_MAP = {
    "Ïô∏Íµ≠TV": (os.path.join(PARENT_VIDEO_DIR, "Ïô∏Íµ≠TV"), "ftv"),
    "Íµ≠ÎÇ¥TV": (os.path.join(PARENT_VIDEO_DIR, "Íµ≠ÎÇ¥TV"), "ktv"),
    "ÏòÅÌôî": (os.path.join(PARENT_VIDEO_DIR, "ÏòÅÌôî"), "movie"),
    "Ïï†ÎãàÎ©îÏù¥ÏÖò": (os.path.join(PARENT_VIDEO_DIR, "ÏùºÎ≥∏ Ïï†ÎãàÎ©îÏù¥ÏÖò"), "anim_all"),
    "Î∞©ÏÜ°Ï§ë": (os.path.join(PARENT_VIDEO_DIR, "Î∞©ÏÜ°Ï§ë"), "air")
}

EXCLUDE_FOLDERS = ["ÏÑ±Ïù∏", "19Í∏à", "Adult", "@eaDir", "#recycle"]
VIDEO_EXTS = ('.mp4', '.mkv', '.avi', '.wmv', '.flv', '.ts', '.tp', '.m4v', '.m2ts', '.mov')
FFMPEG_PATH = "ffmpeg"
for p in ["/usr/local/bin/ffmpeg", "/var/packages/ffmpeg/target/bin/ffmpeg", "/usr/bin/ffmpeg"]:
    if os.path.exists(p): FFMPEG_PATH = p; break

GLOBAL_CACHE = {
    "air": [], "movies": [], "foreigntv": [], "koreantv": [],
    "animations_all": [], "search_index": [], "home_recommend": [], "version": CACHE_VERSION
}

def log(msg):
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {msg}", flush=True)

def nfc(text): return unicodedata.normalize('NFC', text) if text else ""
def nfd(text): return unicodedata.normalize('NFD', text) if text else ""

# --- [Ï†ïÍ∑úÏãù Î∞è ÌÅ¥Î¶¨Îãù] ---
REGEX_EXT = re.compile(r'\.[a-zA-Z0-9]{2,4}$')
REGEX_YEAR = re.compile(r'\((19|20)\d{2}\)|(?<!\d)(19|20)\d{2}(?!\d)')
REGEX_EP_MARKER = re.compile(r'(?i)(?:^|[.\s_]|(?<=[Í∞Ä-Ìû£]))(?:S\d+E\d+|S\d+|E\d+|\d+\s*(?:Ìôî|Ìöå|Í∏∞)|Season\s*\d+|Part\s*\d+|Disk\s*\d+|Disc\s*\d+|CD\s*\d+).*')
REGEX_FORBIDDEN_TITLE = re.compile(r'(?i)^\s*(Season\s*\d+|Part\s*\d+|EP\s*\d+|\d+Ìôî|\d+Ìöå|\d+Í∏∞|ÏãúÏ¶å\s*\d+|S\d+|E\d+|Disk\s*\d+|Disc\s*\d+|CD\s*\d+|Specials?|Extras?|Bonus|ÎØ∏Î∂ÑÎ•ò|Í∏∞ÌÉÄ|ÏÉà\s*Ìè¥Îçî|VIDEO|GDS3|GDRIVE|NAS|share)\s*$', re.I)

def natural_sort_key(s):
    if s is None: return []
    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\d+)', nfc(str(s)))]

def clean_title_complex(title):
    if not title: return "", None
    title = nfc(title)

    # [ÏàòÏ†ï] Ï†úÎ™© ÏïûÏùò Ïù∏Îç±Ïä§ Ïà´ÏûêÎßå Ï†ïÍµêÌïòÍ≤å Ï†úÍ±∞
    # Ï°∞Í±¥: Ïà´Ïûê Îí§Ïóê Î∞òÎìúÏãú Í≥µÎ∞±(' ')Ïù¥ÎÇò ÎßàÏπ®Ìëú('.')Í∞Ä Ïò§Í≥† Í∑∏ Îí§Ïóê Îã§Î•∏ Í∏ÄÏûêÍ∞Ä ÏûàÎäî Í≤ΩÏö∞Îßå Îß§Ïπ≠
    # '007', '2.5', '300' Ï≤òÎüº Í≥µÎ∞± ÏóÜÏù¥ Ïà´ÏûêÎÇò ÏÜåÏàòÏ†êÏúºÎ°úÎßå Îêú Ï†úÎ™©ÏùÄ Î≥¥Ìò∏Ìï®
    title = re.sub(r'^\d+[\s.]+(?=.+)', '', title).strip()

    cleaned = REGEX_EXT.sub('', title)
    year_match = REGEX_YEAR.search(cleaned)
    year = year_match.group().replace('(', '').replace(')', '') if year_match else None
    cleaned = REGEX_YEAR.sub(' ', cleaned)
    cleaned = REGEX_EP_MARKER.sub(' ', cleaned)
    cleaned = re.sub(r'\[.*?\]|\(.*?\)', ' ', cleaned)
    cleaned = re.sub(r'(?<!\d)\.|\.(?!\d)', ' ', cleaned)
    cleaned = re.sub(r'[\_\-!?„Äê„Äë„Äé„Äè„Äå„Äç"\'#@*‚Äª:]', ' ', cleaned)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned, year

# --- [Ï∂îÍ∞Ä Ïú†Ìã∏Î¶¨Ìã∞] ---
def load_tmdb_memory_cache():
    """ÏÑúÎ≤Ñ ÏãúÏûë Ïãú TMDB Ï∫êÏãúÎ•º Î©îÎ™®Î¶¨Î°ú Î°úÎìú"""
    if not os.path.exists(TMDB_CACHE_DIR): return
    for f in os.listdir(TMDB_CACHE_DIR):
        if f.endswith(".json"):
            try:
                with open(os.path.join(TMDB_CACHE_DIR, f), 'r', encoding='utf-8') as file:
                    data = json.load(file)
                    if not data.get('failed'): TMDB_MEMORY_CACHE[f.replace(".json", "")] = data
            except: pass

def get_real_path(path):
    if not path or os.path.exists(path): return path
    if os.path.exists(nfc(path)): return nfc(path)
    if os.path.exists(nfd(path)): return nfd(path)
    return path

def resolve_nas_path(app_path):
    app_path = nfc(urllib.parse.unquote(app_path or ""))
    parts = app_path.split('/')
    if parts and parts[0] in PATH_MAP:
        base_dir, type_code = PATH_MAP[parts[0]]
        return get_real_path(os.path.join(base_dir, "/".join(parts[1:]))), type_code
    return None, None

def get_meaningful_name(path):
    curr = nfc(path)
    while True:
        name = os.path.basename(curr)
        if not name: break
        if not REGEX_FORBIDDEN_TITLE.match(name) and name.lower() not in ["video", "share"]: return name
        parent = os.path.dirname(curr)
        if parent == curr: break
        curr = parent
    return os.path.basename(path)

def get_series_root_path(path, rel_base):
    curr = nfc(path); rel_base = nfc(rel_base)
    while True:
        name = os.path.basename(curr)
        if not name or curr == rel_base: break
        if not REGEX_FORBIDDEN_TITLE.match(name) and name.lower() not in ["video", "share"]: return nfc(os.path.relpath(curr, rel_base))
        parent = os.path.dirname(curr)
        if parent == curr: break
        curr = parent
    return nfc(os.path.relpath(path, rel_base))

# --- [Î©îÎ™®Î¶¨ Ïã§ÏãúÍ∞Ñ ÌÜµÌï© Î°úÏßÅ] ---
def merge_folders_to_series_in_memory(items):
    if not items: return []
    merged = {}
    for item in items:
        raw_name = item.get('name', 'Unknown')
        pure_name, _ = clean_title_complex(raw_name)
        if not pure_name: pure_name = raw_name

        if pure_name not in merged:
            merged[pure_name] = item.copy()
            merged[pure_name]['name'] = pure_name
            if 'movies' not in merged[pure_name]: merged[pure_name]['movies'] = []
        else:
            if 'movies' in item and item['movies']:
                merged[pure_name]['movies'].extend(item['movies'])
            if not merged[pure_name].get('posterPath') and item.get('posterPath'):
                merged[pure_name].update({k: v for k, v in item.items() if k != 'movies' and k != 'path'})

    result = list(merged.values())
    for r in result:
        if 'movies' in r:
            seen_ids = set()
            unique_movies = []
            for m in r['movies']:
                if m['id'] not in seen_ids:
                    unique_movies.append(m)
                    seen_ids.add(m['id'])
            r['movies'] = sorted(unique_movies, key=lambda x: natural_sort_key(x.get('title', '')))
    return sorted(result, key=lambda x: natural_sort_key(x.get('name', '')))

# --- [TMDB Î∞è Î©îÌÉÄÎç∞Ïù¥ÌÑ∞] ---
def get_tmdb_info_server(title, ignore_cache=False):
    if not title: return {"failed": True}
    h = hashlib.md5(nfc(title).encode()).hexdigest(); cp = os.path.join(TMDB_CACHE_DIR, f"{h}.json")
    if not ignore_cache and h in TMDB_MEMORY_CACHE: return TMDB_MEMORY_CACHE[h]
    if not ignore_cache and os.path.exists(cp):
        try:
            with open(cp, 'r', encoding='utf-8') as f:
                data = json.load(f); TMDB_MEMORY_CACHE[h] = data; return data
        except: pass
    ct, year = clean_title_complex(title)
    if not ct or REGEX_FORBIDDEN_TITLE.match(ct): return {"failed": True, "forbidden": True}
    params = {"query": ct, "language": "ko-KR", "include_adult": "true", "region": "KR"}
    if year: params["year"] = year
    headers = {"Authorization": f"Bearer {TMDB_API_KEY}"}
    try:
        resp = requests.get(f"{TMDB_BASE_URL}/search/multi", params=params, headers=headers, timeout=5).json()
        results = [r for r in resp.get('results', []) if r.get('media_type') in ['movie', 'tv']]
        if results:
            best = results[0]; m_type, t_id = best.get('media_type'), best.get('id')
            d_resp = requests.get(f"{TMDB_BASE_URL}/{m_type}/{t_id}?language=ko-KR&append_to_response=content_ratings", params=params, headers=headers, timeout=5).json()
            year_val = (d_resp.get('release_date') or d_resp.get('first_air_date') or "").split('-')[0]
            rating = None
            if 'content_ratings' in d_resp:
                kr = next((r['rating'] for r in d_resp['content_ratings'].get('results', []) if r.get('iso_3166_1') == 'KR'), None)
                if kr: rating = f"{kr}+" if kr.isdigit() else kr
            info = {"genreIds": [g['id'] for g in d_resp.get('genres', [])], "posterPath": d_resp.get('poster_path'), "year": year_val, "overview": d_resp.get('overview'), "rating": rating, "seasonCount": d_resp.get('number_of_seasons'), "failed": False}
            TMDB_MEMORY_CACHE[h] = info
            with open(cp, 'w', encoding='utf-8') as f: json.dump(info, f, ensure_ascii=False)
            return info
    except: pass
    return {"failed": True}

# --- [Ïä§Ï∫î Î∞è ÌÉêÏÉâ] ---
def scan_recursive(bp, prefix, display_name=None):
    series_map = {}
    base = nfc(get_real_path(bp)); exts = VIDEO_EXTS; all_files = []
    stack = [base]
    while stack:
        curr = stack.pop()
        try:
            with os.scandir(curr) as it:
                for entry in sorted(list(it), key=lambda e: natural_sort_key(e.name)):
                    if entry.is_dir():
                        if not any(ex in entry.name for ex in EXCLUDE_FOLDERS) and not entry.name.startswith('.'): stack.append(entry.path)
                    elif entry.is_file() and entry.name.lower().endswith(exts): all_files.append(nfc(entry.path))
        except: pass
    all_files.sort(key=natural_sort_key)
    for fp in all_files:
        dp = nfc(os.path.dirname(fp)); rel_path = get_series_root_path(dp, base)
        if rel_path not in series_map:
            name = get_meaningful_name(dp); full_path = display_name if rel_path == "." else f"{display_name}/{rel_path}"
            series_map[rel_path] = {"name": name, "path": full_path, "movies": [], "genreIds": [], "posterPath": None}
        movie_id = hashlib.md5(fp.encode()).hexdigest()
        series_map[rel_path]["movies"].append({"id": movie_id, "title": os.path.basename(fp), "videoUrl": f"/video_serve?type={prefix}&path={urllib.parse.quote(os.path.relpath(fp, base))}", "thumbnailUrl": f"/thumb_serve?type={prefix}&id={movie_id}&path={urllib.parse.quote(os.path.relpath(fp, base))}"})
    return list(series_map.values())

def fetch_metadata_async(force_all=False):
    log("üöÄ [METADATA] Î∞±Í∑∏ÎùºÏö¥Îìú Îß§Ïπ≠ ÏãúÏûë")
    tasks = []
    for k in ["animations_all", "foreigntv", "koreantv", "movies", "air"]:
        for cat in GLOBAL_CACHE.get(k, []):
            if force_all or (not cat.get('posterPath') and not cat.get('failed')):
                tasks.append(cat)

    total = len(tasks)
    count = 0
    for cat in tasks:
        info = get_tmdb_info_server(cat['name'], ignore_cache=force_all)
        cat.update(info)
        count += 1
        if count % 10 == 0:
            log(f"  ‚è≥ Îß§Ïπ≠ Ï§ë... ({count}/{total})")
            save_cache()
        time.sleep(0.1)
    log("üèÅ [METADATA] Î™®Îì† ÏûëÏóÖ ÏôÑÎ£å")

def build_home_recommend():
    m, a, k, f = GLOBAL_CACHE["movies"], GLOBAL_CACHE["animations_all"], GLOBAL_CACHE["koreantv"], GLOBAL_CACHE["foreigntv"]
    all_p = list(m + a + k + f); random.shuffle(all_p)
    GLOBAL_CACHE["home_recommend"] = [{"title": "ÏßÄÍ∏à Í∞ÄÏû• Ìï´Ìïú Ïù∏Í∏∞Ïûë", "items": all_p[:20]}, {"title": "Î∞©Í∏à Ïò¨ÎùºÏò® ÏµúÏã† ÏòÅÌôî", "items": m[:20]}, {"title": "ÏßÄÍ∏à Ïù∏Í∏∞ ÏûàÎäî ÏãúÎ¶¨Ï¶à", "items": (k + f)[:20]}, {"title": "Ï∂îÏ≤ú Ïï†ÎãàÎ©îÏù¥ÏÖò", "items": a[:20]}]

def perform_full_scan(cache_keys=None):
    keys = cache_keys if cache_keys else [("Ïï†ÎãàÎ©îÏù¥ÏÖò", "animations_all"), ("Ïô∏Íµ≠TV", "foreigntv"), ("Íµ≠ÎÇ¥TV", "koreantv"), ("ÏòÅÌôî", "movies"), ("Î∞©ÏÜ°Ï§ë", "air")]
    log(f"üîÑ NAS Î∂ÄÎ∂Ñ/Ï†ÑÏó≠ Ïä§Ï∫î ÏãúÏûë: {keys}")
    for label, cache_key in keys:
        path, prefix = PATH_MAP[label]
        GLOBAL_CACHE[cache_key] = scan_recursive(path, prefix, display_name=label)
    # Ïä§Ï∫î ÏßÅÌõÑ Î©îÎ™®Î¶¨ ÌÜµÌï©
    for k in ["foreigntv", "koreantv", "animations_all"]:
        GLOBAL_CACHE[k] = merge_folders_to_series_in_memory(GLOBAL_CACHE[k])
    build_home_recommend(); save_cache()
    threading.Thread(target=fetch_metadata_async, daemon=True).start()

def save_cache():
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f: json.dump(GLOBAL_CACHE, f, ensure_ascii=False)
    except: pass

def load_cache():
    if not os.path.exists(CACHE_FILE): return False
    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            d = json.load(f)
            if d.get("version") == CACHE_VERSION:
                GLOBAL_CACHE.update(d)
                log("üß† 9.7 Ï∫êÏãú Î°úÎìú ÏôÑÎ£å. Î©îÎ™®Î¶¨ Ïã§ÏãúÍ∞Ñ ÌÜµÌï© Ï§ë...")
                for k in ["foreigntv", "koreantv", "animations_all"]: GLOBAL_CACHE[k] = merge_folders_to_series_in_memory(GLOBAL_CACHE[k])
                return True
    except: pass
    return False

@app.route('/home')
def get_home(): return jsonify(GLOBAL_CACHE.get("home_recommend", []))

def process_api(data, filter_keyword=None):
    pool = data
    if filter_keyword:
        synonyms = {"ÎØ∏Íµ≠": ["ÎØ∏Íµ≠", "ÎØ∏Îìú", "us"], "Ï§ëÍµ≠": ["Ï§ëÍµ≠", "Ï§ëÎìú", "cn"], "ÏùºÎ≥∏": ["ÏùºÎ≥∏", "ÏùºÎìú", "jp"], "Í∏∞ÌÉÄ": ["Í∏∞ÌÉÄ", "etc"], "Îã§ÌÅê": ["Îã§ÌÅê", "docu"], "ÎìúÎùºÎßà": ["ÎìúÎùºÎßà"], "ÏãúÌä∏ÏΩ§": ["ÏãúÌä∏ÏΩ§"], "ÏòàÎä•": ["ÏòàÎä•"], "ÍµêÏñë": ["ÍµêÏñë"]}
        targets = [nfc(t).lower() for t in synonyms.get(filter_keyword, [filter_keyword])]
        pool = [c for c in data if any(t in nfc(c.get('path', '')).lower() or t in nfc(c.get('name', '')).lower() for t in targets)]
    limit = request.args.get('limit', type=int, default=5000); offset = request.args.get('offset', type=int, default=0)
    res = pool[offset:offset+limit]
    if request.args.get('lite') == 'true':
        return [{"name": c.get('name'), "path": c.get('path'), "genreIds": c.get('genreIds'), "posterPath": c.get('posterPath'), "year": c.get('year'), "overview": c.get('overview'), "rating": c.get('rating'), "seasonCount": c.get('seasonCount'), "failed": c.get('failed')} for c in res]
    return res

@app.route('/foreigntv')
def get_ftv(): return jsonify(process_api(GLOBAL_CACHE["foreigntv"]))
@app.route('/ftv_us')
def get_ftv_us(): return jsonify(process_api(GLOBAL_CACHE["foreigntv"], "ÎØ∏Íµ≠"))
@app.route('/ftv_cn')
def get_ftv_cn(): return jsonify(process_api(GLOBAL_CACHE["foreigntv"], "Ï§ëÍµ≠"))
@app.route('/ftv_jp')
def get_ftv_jp(): return jsonify(process_api(GLOBAL_CACHE["foreigntv"], "ÏùºÎ≥∏"))
@app.route('/ftv_docu')
def get_ftv_docu(): return jsonify(process_api(GLOBAL_CACHE["foreigntv"], "Îã§ÌÅê"))
@app.route('/ftv_etc')
def get_ftv_etc(): return jsonify(process_api(GLOBAL_CACHE["foreigntv"], "Í∏∞ÌÉÄ"))

@app.route('/koreantv')
def get_ktv(): return jsonify(process_api(GLOBAL_CACHE["koreantv"]))
@app.route('/ktv_drama')
def get_ktv_drama(): return jsonify(process_api(GLOBAL_CACHE["koreantv"], "ÎìúÎùºÎßà"))
@app.route('/ktv_sitcom')
def get_ktv_sitcom(): return jsonify(process_api(GLOBAL_CACHE["koreantv"], "ÏãúÌä∏ÏΩ§"))
@app.route('/ktv_variety')
def get_ktv_variety(): return jsonify(process_api(GLOBAL_CACHE["koreantv"], "ÏòàÎä•"))
@app.route('/ktv_edu')
def get_ktv_edu(): return jsonify(process_api(GLOBAL_CACHE["koreantv"], "ÍµêÏñë"))
@app.route('/ktv_docu')
def get_ktv_docu(): return jsonify(process_api(GLOBAL_CACHE["koreantv"], "Îã§ÌÅêÎ©òÌÑ∞Î¶¨"))

@app.route('/animations_all')
def get_anim(): return jsonify(process_api(GLOBAL_CACHE["animations_all"]))
@app.route('/anim_raftel')
def get_anim_r(): return jsonify(process_api(GLOBAL_CACHE["animations_all"], "ÎùºÌîÑÌÖî"))
@app.route('/anim_series')
def get_anim_s(): return jsonify(process_api(GLOBAL_CACHE["animations_all"], "ÏãúÎ¶¨Ï¶à"))

@app.route('/movies')
@app.route('/movies_all')
@app.route('/movies_latest')
@app.route('/movies_uhd')
@app.route('/movies_title')
def get_movies():
    pool = GLOBAL_CACHE["movies"]
    if "uhd" in request.path: pool = [c for c in pool if "uhd" in (c.get('path') or "").lower() or "4k" in (c.get('path') or "").lower()]
    elif "title" in request.path: pool = sorted(pool, key=lambda x: natural_sort_key(x.get('name', '')))
    return jsonify(process_api(pool))

@app.route('/rescan_broken')
def rescan_broken():
    threading.Thread(target=perform_full_scan, args=([("ÏòÅÌôî", "movies"), ("Î∞©ÏÜ°Ï§ë", "air")],), daemon=True).start()
    return jsonify({"status": "success", "message": "ÏòÅÌôî/Î∞©ÏÜ°Ï§ë Ïπ¥ÌÖåÍ≥†Î¶¨ Ïû¨ÌÉêÏÉâ ÏãúÏûë"})

@app.route('/rematch_metadata')
def rescan_metadata():
    threading.Thread(target=fetch_metadata_async, args=(True,), daemon=True).start()
    return jsonify({"status": "success", "message": "TMDB Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤¥ Ïû¨Îß§Ïπ≠ ÏãúÏûë (Î∞±Í∑∏ÎùºÏö¥Îìú)"})

@app.route('/search')
def search_videos():
    q = request.args.get('q', '').lower(); res = []
    for k in ["movies", "animations_all", "foreigntv", "koreantv", "air"]:
        for cat in GLOBAL_CACHE.get(k, []):
            if q in cat['name'].lower() or q in cat.get('path','').lower(): res.append(cat.copy())
    return jsonify(process_api(res))

@app.route('/list')
def get_list():
    path = request.args.get('path'); real_path, type_code = resolve_nas_path(path)
    if not real_path or not os.path.exists(real_path): return jsonify([])
    cat_prefix = nfc(path.split('/')[0]); base_dir = PATH_MAP[cat_prefix][0]; res, movies = [], []
    try:
        for entry in sorted(os.listdir(real_path), key=natural_sort_key):
            fe = os.path.join(real_path, entry)
            if os.path.isdir(fe) and not any(ex in entry for ex in EXCLUDE_FOLDERS):
                item = {"name": nfc(entry), "path": f"{cat_prefix}/{nfc(os.path.relpath(fe, base_dir))}"}
                h = hashlib.md5(nfc(get_meaningful_name(fe)).encode()).hexdigest(); item.update(TMDB_MEMORY_CACHE.get(h, {}))
                res.append(item)
            elif entry.lower().endswith(VIDEO_EXTS):
                movie_id = hashlib.md5(fe.encode()).hexdigest()
                movies.append({"id": movie_id, "title": entry, "videoUrl": f"/video_serve?type={type_code}&path={urllib.parse.quote(os.path.relpath(fe, base_dir))}", "thumbnailUrl": f"/thumb_serve?type={type_code}&id={movie_id}&path={urllib.parse.quote(os.path.relpath(fe, base_dir))}"})
    except: pass
    if movies: res.append({"name": os.path.basename(real_path), "path": path, "movies": sorted(movies, key=lambda x: natural_sort_key(x['title']))})
    return jsonify(res)

@app.route('/video_serve')
def video_serve():
    path, prefix = request.args.get('path'), request.args.get('type')
    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
        return send_file(get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path)))), conditional=True)
    except: return "Not Found", 404

@app.route('/thumb_serve')
def thumb_serve():
    path, prefix, tid = request.args.get('path'), request.args.get('type'), request.args.get('id')
    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix); vp = get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path))))
        if os.path.isdir(vp):
            fs = sorted([f for f in os.listdir(vp) if f.lower().endswith(VIDEO_EXTS)]); vp = os.path.join(vp, fs[0]) if fs else vp
        tp = os.path.join(DATA_DIR, f"seek_300_{tid}")
        if not os.path.exists(tp): subprocess.run([FFMPEG_PATH, "-y", "-ss", "300", "-i", vp, "-vframes", "1", "-q:v", "5", "-vf", "scale=320:-1", tp], timeout=10)
        return send_file(tp, mimetype='image/jpeg') if os.path.exists(tp) else ("Not Found", 404)
    except: return "Not Found", 404

if __name__ == '__main__':
    log(f"üì∫ NAS Server v{CACHE_VERSION} ÏãúÏûë (9.7 Ï∫êÏãú Î≥¥Ï°¥ Î™®Îìú)")
    load_tmdb_memory_cache()
    if not load_cache(): perform_full_scan()
    else: log("‚úÖ Í∏∞Ï°¥ 9.7 Ï∫êÏãú Î°úÎìú Î∞è Î©îÎ™®Î¶¨ ÌÜµÌï© ÏôÑÎ£å")
    app.run(host='0.0.0.0', port=5000, threaded=True)
