import os, subprocess, hashlib, urllib.parse, unicodedata, threading, time, json, re, sys, traceback, shutil, requests, random, mimetypes, sqlite3, gzip
from flask import Flask, jsonify, send_from_directory, request, Response, redirect, send_file, make_response
from flask_cors import CORS
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from collections import deque
from io import BytesIO

app = Flask(__name__)
CORS(app)

# MIME íƒ€ì… ì¶”ê°€ ë“±ë¡
if not mimetypes.types_map.get('.mkv'): mimetypes.add_type('video/x-matroska', '.mkv')
if not mimetypes.types_map.get('.ts'): mimetypes.add_type('video/mp2t', '.ts')
if not mimetypes.types_map.get('.tp'): mimetypes.add_type('video/mp2t', '.tp')

# --- [ìµœì í™”: Gzip ì••ì¶• í•¨ìˆ˜ ì¶”ê°€] ---
def gzip_response(data):
    content = gzip.compress(json.dumps(data, ensure_ascii=False).encode('utf-8'))
    response = make_response(content)
    response.headers['Content-Encoding'] = 'gzip'
    response.headers['Content-Type'] = 'application/json'
    response.headers['Content-Length'] = len(content)
    return response

# --- [1. ì„¤ì • ë° ê²½ë¡œ] ---
MY_IP = "192.168.0.2"
DATA_DIR = "/volume2/video/thumbnails"
DB_FILE = "/volume2/video/video_metadata.db"
TMDB_CACHE_DIR = "/volume2/video/tmdb_cache"
HLS_ROOT = "/dev/shm/videoplayer_hls"
CACHE_VERSION = "137.5" # ë²„ì „ ì—…ê·¸ë ˆì´ë“œ

# [ìˆ˜ì •] ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ìƒì„± ë³´ì¥
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
FAILURE_LOG_PATH = os.path.join(SCRIPT_DIR, "metadata_failures.txt")

TMDB_MEMORY_CACHE = {}
TMDB_API_KEY = "eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiI3OGNiYWQ0ZjQ3NzcwYjYyYmZkMTcwNTA2NDIwZDQyYyIsIm5iZiI6MTY1MzY3NTU4MC45MTUsInN1YiI6IjYyOTExNjNjMTI0MjVjMDA1MjI0ZGQzNCIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.3YU0WuIx_WDo6nTRKehRtn4N5I4uCgjI1tlpkqfsUhk".strip()
TMDB_BASE_URL = "https://api.themoviedb.org/3"

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(TMDB_CACHE_DIR, exist_ok=True)
if os.path.exists(HLS_ROOT): shutil.rmtree(HLS_ROOT, ignore_errors=True)
os.makedirs(HLS_ROOT, exist_ok=True)

PARENT_VIDEO_DIR = "/volume2/video/GDS3/GDRIVE/VIDEO"
PATH_MAP = {
    "ì™¸êµ­TV": (os.path.join(PARENT_VIDEO_DIR, "ì™¸êµ­TV"), "ftv"),
    "êµ­ë‚´TV": (os.path.join(PARENT_VIDEO_DIR, "êµ­ë‚´TV"), "ktv"),
    "ì˜í™”": (os.path.join(PARENT_VIDEO_DIR, "ì˜í™”"), "movie"),
    "ì• ë‹ˆë©”ì´ì…˜": (os.path.join(PARENT_VIDEO_DIR, "ì¼ë³¸ ì• ë‹ˆë©”ì´ì…˜"), "anim_all"),
    "ë°©ì†¡ì¤‘": (os.path.join(PARENT_VIDEO_DIR, "ë°©ì†¡ì¤‘"), "air")
}

EXCLUDE_FOLDERS = ["ì„±ì¸", "19ê¸ˆ", "Adult", "@eaDir", "#recycle"]
VIDEO_EXTS = ('.mp4', '.mkv', '.avi', '.wmv', '.flv', '.ts', '.tp', '.m4v', '.m2ts', '.mov')
FFMPEG_PATH = "ffmpeg"
for p in ["/usr/local/bin/ffmpeg", "/var/packages/ffmpeg/target/bin/ffmpeg", "/usr/bin/ffmpeg"]:
    if os.path.exists(p):
        FFMPEG_PATH = p
        break

HOME_RECOMMEND = []
IS_METADATA_RUNNING = False
_FAST_CATEGORY_CACHE = {}
_SECTION_CACHE = {} # ì¹´í…Œê³ ë¦¬ ì„¹ì…˜ ê²°ê³¼ ìºì‹œ ì¶”ê°€
_DETAIL_CACHE = deque(maxlen=200)

THUMB_SEMAPHORE = threading.Semaphore(4)

def log(tag, msg):
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] [{tag}] {msg}", flush=True)

def log_matching_failure(orig, cleaned, reason):
    """[ì¶”ê°€] ë§¤ì¹­ ì‹¤íŒ¨ ë‚´ìš©ì„ íŒŒì¼ì— ê¸°ë¡í•˜ì—¬ AI ë¶„ì„ ì§€ì›"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        # íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ ë¡œê·¸ë¡œ í™•ì¸
        with open(FAILURE_LOG_PATH, "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] [FAIL] ORIG: {orig} | CLEANED: {cleaned} | REASON: {reason}\n")
    except Exception as e:
        log("LOG_ERROR", f"ì‹¤íŒ¨ ë¡œê·¸ íŒŒì¼ ì“°ê¸° ì¤‘ ì—ëŸ¬: {str(e)}")

def nfc(text):
    return unicodedata.normalize('NFC', text) if text else ""

def nfd(text):
    return unicodedata.normalize('NFD', text) if text else ""

# --- [DB ê´€ë¦¬] ---
def get_db():
    conn = sqlite3.connect(DB_FILE, timeout=60)
    conn.row_factory = sqlite3.Row
    return conn

def init_db():
    conn = get_db()
    cursor = conn.cursor()
    # í…Œì´ë¸” ìƒì„±
    cursor.execute('CREATE TABLE IF NOT EXISTS series (path TEXT PRIMARY KEY, category TEXT, name TEXT, posterPath TEXT, year TEXT, overview TEXT, rating TEXT, seasonCount INTEGER, genreIds TEXT, genreNames TEXT, director TEXT, actors TEXT, failed INTEGER DEFAULT 0, tmdbId TEXT)')
    cursor.execute('CREATE TABLE IF NOT EXISTS episodes (id TEXT PRIMARY KEY, series_path TEXT, title TEXT, videoUrl TEXT, thumbnailUrl TEXT, overview TEXT, air_date TEXT, season_number INTEGER, episode_number INTEGER, FOREIGN KEY (series_path) REFERENCES series (path) ON DELETE CASCADE)')
    cursor.execute('CREATE TABLE IF NOT EXISTS tmdb_cache (h TEXT PRIMARY KEY, data TEXT)')
    cursor.execute('CREATE TABLE IF NOT EXISTS server_config (key TEXT PRIMARY KEY, value TEXT)')

    # ì¸ë±ìŠ¤ ìƒì„±
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_series_category ON series(category)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_series_name ON series(name)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_series_tmdbId ON series(tmdbId)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_episodes_series ON episodes(series_path)')

    # ëˆ„ë½ëœ ì»¬ëŸ¼ ë™ì  ì¶”ê°€ (PRAGMA ì‚¬ìš©ìœ¼ë¡œ ìµœì í™”)
    def add_col_if_missing(table, col, type):
        cursor.execute(f"PRAGMA table_info({table})")
        cols = [c[1] for c in cursor.fetchall()]
        if col not in cols:
            log("DB", f"ì»¬ëŸ¼ ì¶”ê°€: {table}.{col}")
            cursor.execute(f"ALTER TABLE {table} ADD COLUMN {col} {type}")

    add_col_if_missing('series', 'genreNames', 'TEXT')
    add_col_if_missing('series', 'director', 'TEXT')
    add_col_if_missing('series', 'actors', 'TEXT')
    add_col_if_missing('episodes', 'overview', 'TEXT')
    add_col_if_missing('episodes', 'air_date', 'TEXT')
    add_col_if_missing('episodes', 'season_number', 'INTEGER')
    add_col_if_missing('episodes', 'episode_number', 'INTEGER')

    conn.commit()
    conn.close()
    log("DB", "ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ìµœì í™” ì™„ë£Œ")

# --- [ìœ í‹¸ë¦¬í‹°] ---
def get_real_path(path):
    if not path or os.path.exists(path): return path
    if os.path.exists(nfc(path)): return nfc(path)
    if os.path.exists(nfd(path)): return nfd(path)
    return path

def migrate_json_to_db():
    if not os.path.exists(TMDB_CACHE_DIR): return
    conn = get_db()
    row = conn.execute("SELECT value FROM server_config WHERE key = 'json_migration_done'").fetchone()
    if row and row['value'] == 'true':
        conn.close()
        return
    files = [f for f in os.listdir(TMDB_CACHE_DIR) if f.endswith(".json")]
    if not files:
        conn.close()
        return
    log("MIGRATE", f"JSON ìºì‹œ {len(files)}ê°œ DB ì´ê´€ ì¤‘...")
    for idx, f in enumerate(files):
        h = f.replace(".json", "")
        try:
            with open(os.path.join(TMDB_CACHE_DIR, f), 'r', encoding='utf-8') as file:
                data = json.load(file)
                conn.execute('INSERT OR REPLACE INTO tmdb_cache (h, data) VALUES (?, ?)', (h, json.dumps(data)))
        except: pass
        if (idx + 1) % 2000 == 0:
            conn.commit()
            log("MIGRATE", f"ì§„í–‰ ì¤‘... ({idx+1}/{len(files)})")
    conn.execute("INSERT OR REPLACE INTO server_config (key, value) VALUES ('json_migration_done', 'true')")
    conn.commit()
    conn.close()
    log("MIGRATE", "ì´ê´€ ì™„ë£Œ")

# --- [ì •ê·œì‹ ë° í´ë¦¬ë‹ ëŒ€í­ ê°•í™”] ---
REGEX_EXT = re.compile(r'\.[a-zA-Z0-9]{2,4}$')
REGEX_YEAR = re.compile(r'\((19|20)\d{2}\)|(?<!\d)(19|20)\d{2}(?!\d)')
REGEX_CH_PREFIX = re.compile(r'^\[(?:KBS|SBS|MBC|tvN|JTBC|OCN|Mnet|TVì¡°ì„ |ì±„ë„A|MBN|ENA|KBS2|KBS1|CH\d+|TV)\]\s*')
# ê¸°ìˆ  íƒœê·¸ ë° êµ­ê°€ ì•½ì–´ ë³´ê°• (IMAX, Unrated, Criterion, KOR, JPN ë“± ì¶”ê°€)
REGEX_TECHNICAL_TAGS = re.compile(r'(?i)[.\s_-](?!(?:\d+\b))(\d{3,4}p|FHD|QHD|UHD|4K|Bluray|Blu-ray|WEB-DL|WEBRip|HDRip|BDRip|DVDRip|H\.?26[45]|x26[45]|HEVC|AVC|AAC\d?|DTS-?H?D?|AC3|DDP\d?|DD\+\d?|Dual|Atmos|REPACK|10bit|REMUX|FLAC|xvid|DivX|MKV|MP4|AVI|HDR(?:10)?(?:\+)?|Vision|Dolby|NF|AMZN|HMAX|DSNP|AppleTV?|Disney|PCOK|playWEB|ATVP|HULU|HDTV|HD|KBS|SBS|MBC|TVN|JTBC|NEXT|ST|SW|KL|YT|MVC|KN|FLUX|hallowed|PiRaTeS|Jadewind|Movie|pt\s*\d+|KOREAN|KOR|ITALIAN|JAPANESE|JPN|CHINESE|CHN|ENGLISH|ENG|USA|HK|TW|FRENCH|GERMAN|SPANISH|THAI|VIETNAMESE|WEB|DL|TVRip|HDR10Plus|IMAX|Unrated|REMASTERED|Criterion|NonDRM|BRRip|1080i|720i|å›½è¯­|Mandarin|Cantonese|FanSub|VFQ|VF|2CH|5\.1CH|8m|2398)(\b|$|[.\s_-])')
REGEX_EP_MARKER_STRICT = re.compile(r'(?i)(?:[.\s_-]|(?<=[ê°€-í£]))(?:S(\d+)E(\d+)(?:-E\d+)?|S(\d+)|E(\d+)(?:-E\d+)?|\d+\s*(?:í™”|íšŒ|ê¸°|ë¶€)|Season\s*\d+|Part\s*\d+|pt\s*\d+|Episode\s*\d+|Disk\s*\d+|Disc\s*\d+|CD\s*\d+|ì‹œì¦Œ\s*\d+|[ìƒí•˜]ë¶€|ìµœì¢…í™”|\d{6}|\d{8})')
REGEX_DATE_YYMMDD = re.compile(r'(?<!\d)\d{6}(?!\d)')
# ê¸ˆì§€ ë‹¨ì–´ ë° ë¶€ì† ì˜ìƒ í‚¤ì›Œë“œ ê°•í™” (ë“±ê¸‰ê³ ì§€, ì˜ˆê³ í¸, Making of ë“±)
REGEX_FORBIDDEN_TITLE = re.compile(r'(?i)^\s*(Season\s*\d+|Part\s*\d+|EP\s*\d+|\d+í™”|\d+íšŒ|\d+ê¸°|ì‹œì¦Œ\s*\d+|S\d+|E\d+|Disk\s*\d+|Disc\s*\d+|CD\s*\d+|Specials?|Extras?|Bonus|ë¯¸ë¶„ë¥˜|ê¸°íƒ€|ìƒˆ\s*í´ë”|VIDEO|GDS3|GDRIVE|NAS|share|ì˜í™”|ì™¸êµ­TV|êµ­ë‚´TV|ì• ë‹ˆë©”ì´ì…˜|ë°©ì†¡ì¤‘|ì œëª©|UHD|ìµœì‹ |ìµœì‹ ì‘|ìµœì‹ ì˜í™”|4K|1080P|720P|Digital\s*Hits|Gag\s*Reel|Making\s*of|Behind\s*the\s*Scenes|ë“±ê¸‰ê³ ì§€|ì˜ˆê³ í¸|Trailer)\s*$', re.I)
REGEX_BRACKETS = re.compile(r'\[.*?(?:\]|$)|\(.*?(?:\)|$)|\{.*?(?:\)|$)|\ã€.*?(?:\ã€‘|$)|\ã€.*?(?:\ã€|$)|\ã€Œ.*?(?:\ã€|$)')
REGEX_TMDB_HINT = re.compile(r'\{tmdb[\s-]*(\d+)\}')
REGEX_JUNK_KEYWORDS = re.compile(r'(?i)\s*(?:ë”ë¹™|ìë§‰|ê·¹ì¥íŒ|BD|TV|Web|OAD|OVA|ONA|Full|ë¬´ì‚­ì œ|ê°ë…íŒ|í™•ì¥íŒ|ìµìŠ¤í…ë””ë“œ|ë“±ê¸‰ê³ ì§€|ì˜ˆê³ í¸|(?<!\S)[ìƒí•˜](?!\S))\s*')
REGEX_SPECIAL_CHARS = re.compile(r'[\[\]()_\-!?ã€ã€‘ã€ã€ã€Œã€"\'#@*â€»Ã—,~:;]')
REGEX_LEADING_INDEX = re.compile(r'^(\d+\s+|(?:\d+\.(?!\d)\s*))')
REGEX_SPACES = re.compile(r'\s+')

def clean_title_complex(title):
    if not title: return "", None
    orig_title = nfc(title)
    cleaned = REGEX_EXT.sub('', orig_title)
    cleaned = REGEX_CH_PREFIX.sub('', cleaned)
    cleaned = REGEX_TMDB_HINT.sub('', cleaned)
    if '.' in cleaned:
        cleaned = cleaned.replace('.', ' ')

    ep_match = REGEX_EP_MARKER_STRICT.search(cleaned)
    if ep_match:
        cleaned = cleaned[:ep_match.start()].strip()
    tech_match = REGEX_TECHNICAL_TAGS.search(cleaned)
    if tech_match:
        cleaned = cleaned[:tech_match.start()].strip()

    cleaned = REGEX_DATE_YYMMDD.sub(' ', cleaned)
    year_match = REGEX_YEAR.search(cleaned)
    year = year_match.group().replace('(', '').replace(')', '') if year_match else None
    cleaned = REGEX_YEAR.sub(' ', cleaned)
    cleaned = REGEX_BRACKETS.sub(' ', cleaned)
    cleaned = cleaned.replace("(ìë§‰)", "").replace("(ë”ë¹™)", "").replace("[ìë§‰]", "").replace("[ë”ë¹™]", "")
    cleaned = REGEX_JUNK_KEYWORDS.sub(' ', cleaned)
    cleaned = REGEX_SPECIAL_CHARS.sub(' ', cleaned)
    cleaned = REGEX_LEADING_INDEX.sub('', cleaned)
    cleaned = re.sub(r'([ê°€-í£a-zA-Z])(\d+)$', r'\1 \2', cleaned)
    cleaned = REGEX_SPACES.sub(' ', cleaned).strip()

    if len(cleaned) < 2:
        backup = REGEX_TMDB_HINT.sub('', orig_title)
        backup = REGEX_EXT.sub('', backup).strip()
        if len(backup) >= 2: return backup, year
        return orig_title, year
    return nfc(cleaned), year

def extract_episode_numbers(filename):
    match = REGEX_EP_MARKER_STRICT.search(filename)
    if match:
        s, e = match.group(1), match.group(2)
        if s and e: return int(s), int(e)
        e_only = match.group(4) or match.group(2)
        if e_only: return 1, int(e_only)
    return 1, None

def natural_sort_key(s):
    if s is None: return []
    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\d+)', nfc(str(s)))]

def extract_tmdb_id(title):
    match = REGEX_TMDB_HINT.search(nfc(title))
    return int(match.group(1)) if match else None

# --- [TMDB API ë³´ì™„: ì§€ëŠ¥í˜• ì¬ê²€ìƒ‰] ---
def get_tmdb_info_server(title, ignore_cache=False):
    if not title: return {"failed": True}
    hint_id = extract_tmdb_id(title)
    ct, year = clean_title_complex(title)
    ct = nfc(ct)
    if not ct or REGEX_FORBIDDEN_TITLE.match(ct):
        log("TMDB", f"ê¸ˆì§€ëœ ì œëª© ë˜ëŠ” ë„ˆë¬´ ì§§ìŒ: {title} -> {ct}")
        return {"failed": True, "forbidden": True}

    cache_key = f"{ct}_{year}" if year else ct
    h = hashlib.md5(nfc(cache_key).encode()).hexdigest()

    if not ignore_cache and h in TMDB_MEMORY_CACHE:
        return TMDB_MEMORY_CACHE[h]

    if not ignore_cache:
        try:
            conn = get_db()
            row = conn.execute('SELECT data FROM tmdb_cache WHERE h = ?', (h,)).fetchone()
            conn.close()
            if row:
                data = json.loads(row['data'])
                if not data.get('failed'):
                    TMDB_MEMORY_CACHE[h] = data
                    return data
        except: pass

    log("TMDB", f"ê²€ìƒ‰ ì‹œì‘: '{ct}'" + (f" ({year})" if year else ""))
    headers = {"Authorization": f"Bearer {TMDB_API_KEY}"}
    base_params = {"include_adult": "true", "region": "KR"}

    def perform_search(query, lang=None, m_type='multi'):
        if not query or len(query) < 2: return []
        params = {**base_params, "query": query}
        if lang: params["language"] = lang
        if year: params["year"] = year
        try:
            r = requests.get(f"{TMDB_BASE_URL}/search/{m_type}", params=params, headers=headers, timeout=10)
            if r.status_code != 200: return []
            items = r.json().get('results', [])
            if m_type == 'multi': return [i for i in items if i.get('media_type') in ['movie', 'tv']]
            for i in items: i['media_type'] = m_type
            return items
        except: return []

    try:
        results = []
        if hint_id:
            log("TMDB", f"íŒíŠ¸ ID ì‚¬ìš©: {hint_id}")
            for mt in ['movie', 'tv']:
                resp = requests.get(f"{TMDB_BASE_URL}/{mt}/{hint_id}", params={"language": "ko-KR", **base_params}, headers=headers, timeout=10)
                if resp.status_code == 200:
                    best = resp.json()
                    best['media_type'] = mt
                    results = [best]
                    break

        if not results:
            # 1ë‹¨ê³„: ì •ì œëœ ì œëª©ìœ¼ë¡œ ê²€ìƒ‰
            results = perform_search(ct, "ko-KR", "multi")

            # 2ë‹¨ê³„: ì‹¤íŒ¨ ì‹œ ì œëª© ë¶„í•  ê²€ìƒ‰ (í•œê¸€/ì˜ì–´ í˜¼ìš© ëŒ€ì‘)
            if not results:
                # í•œê¸€ë§Œ ì¶”ì¶œ
                ko_only = "".join(re.findall(r'[ê°€-í£0-9\s]+', ct)).strip()
                if ko_only and len(ko_only) >= 2 and ko_only != ct:
                    log("TMDB", f"ğŸ” ì¬ê²€ìƒ‰ (í•œê¸€ë§Œ): '{ko_only}'")
                    results = perform_search(ko_only, "ko-KR", "multi")

                # ì‹¤íŒ¨ ì‹œ ì˜ì–´ë§Œ ì¶”ì¶œ
                if not results:
                    en_only = "".join(re.findall(r'[a-zA-Z0-9\s]+', ct)).strip()
                    if en_only and len(en_only) >= 3:
                        log("TMDB", f"ğŸ” ì¬ê²€ìƒ‰ (ì˜ì–´ë§Œ): '{en_only}'")
                        results = perform_search(en_only, "ko-KR", "multi")

            # 3ë‹¨ê³„: ì‹œì¦Œì œ ëŒ€ì‘ (ìˆ«ì ì œê±°)
            if not results:
                stripped_ct = re.sub(r'\s+\d+$', '', ct).strip()
                if stripped_ct != ct and len(stripped_ct) >= 2:
                    log("TMDB", f"ğŸ” ì¬ê²€ìƒ‰ (ìˆ«ì ì œê±°): '{stripped_ct}'")
                    results = perform_search(stripped_ct, "ko-KR", "multi")

            if not results: results = perform_search(ct, "ko-KR", "tv")
            if not results: results = perform_search(ct, None, "multi")

        if results:
            best = results[0]
            m_type, t_id = best.get('media_type'), best.get('id')
            log("TMDB", f"âœ… ë§¤ì¹­ ì„±ê³µ: '{ct}' -> {m_type}:{t_id}")
            d_resp = requests.get(f"{TMDB_BASE_URL}/{m_type}/{t_id}?language=ko-KR&append_to_response=content_ratings,credits", headers=headers, timeout=10).json()

            yv = (d_resp.get('release_date') or d_resp.get('first_air_date') or "").split('-')[0]
            rating = None
            if 'content_ratings' in d_resp:
                res_r = d_resp['content_ratings'].get('results', [])
                kr = next((r['rating'] for r in res_r if r.get('iso_3166_1') == 'KR'), None)
                if kr: rating = f"{kr}+" if kr.isdigit() else kr

            genre_names = [g['name'] for g in d_resp.get('genres', [])] if d_resp.get('genres') else []
            cast_data = d_resp.get('credits', {}).get('cast', [])
            actors = [{"name": c['name'], "profile": c['profile_path'], "role": c['character']} for c in cast_data[:10]]
            crew_data = d_resp.get('credits', {}).get('crew', [])
            director = next((c['name'] for c in crew_data if c.get('job') == 'Director'), "")

            info = {
                "tmdbId": f"{m_type}:{t_id}",
                "genreIds": [g['id'] for g in d_resp.get('genres', [])] if d_resp.get('genres') else [],
                "genreNames": genre_names,
                "director": director,
                "actors": actors,
                "posterPath": d_resp.get('poster_path'),
                "year": yv,
                "overview": d_resp.get('overview'),
                "rating": rating,
                "seasonCount": d_resp.get('number_of_seasons'),
                "failed": False
            }

            if m_type == 'tv':
                info['seasons_data'] = {}
                s_count = d_resp.get('number_of_seasons') or 1
                for s_num in range(1, s_count + 1):
                    s_resp = requests.get(f"{TMDB_BASE_URL}/tv/{t_id}/season/{s_num}?language=ko-KR", headers=headers, timeout=10).json()
                    if 'episodes' in s_resp:
                        for ep in s_resp['episodes']:
                            info['seasons_data'][f"{s_num}_{ep['episode_number']}"] = {"overview": ep.get('overview'), "air_date": ep.get('air_date')}

            TMDB_MEMORY_CACHE[h] = info
            try:
                conn = get_db()
                conn.execute('INSERT OR REPLACE INTO tmdb_cache (h, data) VALUES (?, ?)', (h, json.dumps(info)))
                conn.commit()
                conn.close()
            except: pass
            return info
        else:
            log("TMDB", f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: '{ct}'")
            log_matching_failure(title, ct, "NOT_FOUND_IN_TMDB")
    except:
        log("TMDB", f"ì—ëŸ¬ ë°œìƒ: {traceback.format_exc()}")
        log_matching_failure(title, ct, f"API_ERROR: {str(sys.exc_info()[1])}")
    return {"failed": True}

# --- [ìŠ¤ìº” ë° íƒìƒ‰] ---
def scan_recursive_to_db(bp, prefix, category):
    log("SCAN", f"'{category}' íƒìƒ‰ ì¤‘: {bp}")
    base = nfc(get_real_path(bp))
    all_files = []
    stack = [base]
    visited = set()
    while stack:
        curr = stack.pop()
        real_curr = os.path.realpath(curr)
        if real_curr in visited: continue
        visited.add(real_curr)
        try:
            with os.scandir(curr) as it:
                for entry in it:
                    if entry.is_dir():
                        if not any(ex in entry.name for ex in EXCLUDE_FOLDERS) and not entry.name.startswith('.'):
                            stack.append(entry.path)
                    elif entry.is_file() and entry.name.lower().endswith(VIDEO_EXTS):
                        all_files.append(nfc(entry.path))
        except: pass

    conn = get_db()
    cursor = conn.cursor()
    cursor.execute('SELECT id, series_path FROM episodes WHERE series_path LIKE ?', (f"{category}/%",))
    db_data = {row['id']: row['series_path'] for row in cursor.fetchall()}
    current_ids = set()
    total = len(all_files)

    for idx, fp in enumerate(all_files):
        mid = hashlib.md5(fp.encode()).hexdigest()
        current_ids.add(mid)
        rel = nfc(os.path.relpath(fp, base))
        name = os.path.splitext(os.path.basename(fp))[0]
        spath = f"{category}/{rel}"

        cursor.execute('INSERT OR IGNORE INTO series (path, category, name) VALUES (?, ?, ?)', (spath, category, name))
        if mid not in db_data:
            cursor.execute('INSERT OR REPLACE INTO episodes (id, series_path, title, videoUrl, thumbnailUrl) VALUES (?, ?, ?, ?, ?)', (mid, spath, os.path.basename(fp), f"/video_serve?type={prefix}&path={urllib.parse.quote(rel)}", f"/thumb_serve?type={prefix}&id={mid}&path={urllib.parse.quote(rel)}"))
        elif db_data[mid] != spath:
            cursor.execute('UPDATE episodes SET series_path = ? WHERE id = ?', (spath, mid))

        if (idx + 1) % 2000 == 0:
            conn.commit()
            log("SCAN", f"ì§„í–‰ ì¤‘... ({idx+1}/{total})")

    for rid in (set(db_data.keys()) - current_ids):
        cursor.execute('DELETE FROM episodes WHERE id = ?', (rid,))
    cursor.execute('DELETE FROM series WHERE path NOT IN (SELECT DISTINCT series_path FROM episodes) AND category = ?', (category,))
    conn.commit()
    conn.close()
    log("SCAN", f"'{category}' ìŠ¤ìº” ì™„ë£Œ ({total}ê°œ)")

def perform_full_scan():
    log("SYSTEM", f"ì „ì²´ ìŠ¤ìº” ì‹œì‘ (v{CACHE_VERSION})")
    pk = [("ì˜í™”", "movies"), ("ì™¸êµ­TV", "foreigntv"), ("êµ­ë‚´TV", "koreantv"), ("ì• ë‹ˆë©”ì´ì…˜", "animations_all"), ("ë°©ì†¡ì¤‘", "air")]
    conn = get_db()
    rows = conn.execute("SELECT key FROM server_config WHERE key LIKE 'scan_done_%' AND value = 'true'").fetchall()
    done = [r['key'].replace('scan_done_', '') for r in rows]
    conn.close()

    for label, ck in pk:
        if ck in done: continue
        path, prefix = PATH_MAP[label]
        if os.path.exists(path):
            scan_recursive_to_db(path, prefix, ck)
            conn = get_db()
            conn.execute("INSERT OR REPLACE INTO server_config (key, value) VALUES (?, 'true')", (f'scan_done_{ck}',))
            conn.commit()
            conn.close()
            build_all_caches()

    conn = get_db()
    conn.execute('INSERT OR REPLACE INTO server_config (key, value) VALUES (?, ?)', ('last_scan_version', CACHE_VERSION))
    conn.execute("DELETE FROM server_config WHERE key LIKE 'scan_done_%'")
    conn.commit()
    conn.close()
    threading.Thread(target=fetch_metadata_async, daemon=True).start()

def fetch_metadata_async(force_all=False):
    global IS_METADATA_RUNNING
    if IS_METADATA_RUNNING:
        log("METADATA", "ì´ë¯¸ í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    IS_METADATA_RUNNING = True
    log("METADATA", f"ë³‘ë ¬ ë§¤ì¹­ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (force_all={force_all})")
    try:
        conn = get_db()
        if force_all:
            log("METADATA", "ì „ì²´ ì´ˆê¸°í™” ìˆ˜í–‰ ì¤‘ (force_all=True)...")
            conn.execute('UPDATE series SET posterPath=NULL, tmdbId=NULL, failed=0')
            conn.commit()

        log("METADATA", "DBì—ì„œ ë§¤ì¹­ë˜ì§€ ì•Šì€ ì‘í’ˆ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        all_names_rows = conn.execute('SELECT name FROM series WHERE tmdbId IS NULL AND failed = 0').fetchall()
        conn.close()

        if not all_names_rows:
            log("METADATA", "ë§¤ì¹­ ëŒ€ìƒ ì—†ìŒ (ëª¨ë“  ì‘í’ˆì´ ì´ë¯¸ ë§¤ì¹­ë˜ì—ˆê±°ë‚˜ ì‹¤íŒ¨ ì²˜ë¦¬ë¨)")
            IS_METADATA_RUNNING = False
            build_all_caches()
            return

        log("METADATA", f"ì´ {len(all_names_rows)}ê°œ íŒŒì¼ì— ëŒ€í•´ ì œëª© ì •ì œ ë° ê·¸ë£¹í™” ì‹œì‘...")
        name_groups = {}
        for row in all_names_rows:
            orig_name = row['name']
            ct, year = clean_title_complex(orig_name)
            if not ct: continue
            key = (ct, year)
            name_groups.setdefault(key, set()).add(orig_name)

        tasks = [{'clean_title': ct, 'year': year, 'sample_name': list(names)[0], 'orig_names': list(names)} for (ct, year), names in name_groups.items()]
        total = len(tasks)
        log("METADATA", f"ê·¸ë£¹í™” ì™„ë£Œ: {total}ê°œì˜ ê³ ìœ  ì‘í’ˆ ì‹ë³„ë¨")

        def process_one(task):
            info = get_tmdb_info_server(task['sample_name'], ignore_cache=force_all)
            return (task, info)

        batch_size = 50
        total_success = 0
        total_fail = 0
        for i in range(0, total, batch_size):
            batch = tasks[i:i+batch_size]
            log("METADATA", f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ({i+1}~{min(i+batch_size, total)} / {total})")
            results = []
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_task = {executor.submit(process_one, t): t for t in batch}
                for future in as_completed(future_to_task):
                    results.append(future.result())

            log("METADATA", f"ë°°ì¹˜ {i//batch_size + 1} ê²°ê³¼ DB ë°˜ì˜ ì¤‘...")
            conn = get_db()
            cursor = conn.cursor()
            batch_success = 0
            batch_fail = 0
            for task, info in results:
                orig_names = task['orig_names']
                if info.get('failed'):
                    batch_fail += 1
                    cursor.executemany('UPDATE series SET failed=1 WHERE name=?', [(n,) for n in orig_names])
                else:
                    batch_success += 1
                    up = (
                        info.get('posterPath'), info.get('year'), info.get('overview'),
                        info.get('rating'), info.get('seasonCount'),
                        json.dumps(info.get('genreIds', [])),
                        json.dumps(info.get('genreNames', []), ensure_ascii=False),
                        info.get('director'),
                        json.dumps(info.get('actors', []), ensure_ascii=False),
                        info.get('tmdbId')
                    )
                    cursor.executemany('UPDATE series SET posterPath=?, year=?, overview=?, rating=?, seasonCount=?, genreIds=?, genreNames=?, director=?, actors=?, tmdbId=?, failed=0 WHERE name=?', [(*up, name) for name in orig_names])

                    if 'seasons_data' in info:
                        eps_to_update = []
                        for name in orig_names:
                            cursor.execute('SELECT id, title FROM episodes WHERE series_path IN (SELECT path FROM series WHERE name = ?)', (name,))
                            eps_to_update.extend(cursor.fetchall())

                        ep_batch = []
                        for ep_row in eps_to_update:
                            sn, EN = extract_episode_numbers(ep_row['title'])
                            if EN:
                                ei = info['seasons_data'].get(f"{sn}_{EN}")
                                if ei:
                                    ep_batch.append((ei.get('overview'), ei.get('air_date'), sn, EN, ep_row['id']))
                        if ep_batch:
                            cursor.executemany('UPDATE episodes SET overview=?, air_date=?, season_number=?, episode_number=? WHERE id=?', ep_batch)

            conn.commit()
            conn.close()
            total_success += batch_success
            total_fail += batch_fail

            log("METADATA", f"ğŸ“ˆ ì§„í–‰ ìƒí™©: ì„±ê³µ {total_success} / ì‹¤íŒ¨ {total_fail} (ì§„í–‰ë¥ : {round((min(i+batch_size, total))/total*100, 1)}%)")

            if (i // batch_size) % 10 == 0:
                log("METADATA", "ì¤‘ê°„ ìºì‹œ ê°±ì‹  ì¤‘...")
                build_all_caches()

        build_all_caches()
        log("METADATA", f"ğŸŠ ìµœì¢… ì™„ë£Œ: ì´ {total_success}ê°œ ì„±ê³µ, {total_fail}ê°œ ì‹¤íŒ¨")
    except:
        log("METADATA", f"ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ: {traceback.format_exc()}")
    finally:
        IS_METADATA_RUNNING = False
        log("METADATA", "ë³‘ë ¬ ë§¤ì¹­ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")

# --- [í’ì„±í•œ ì„¹ì…˜í™” API] ---
def get_sections_for_category(cat, kw=None):
    cache_key = f"sections_{cat}_{kw}"
    if cache_key in _SECTION_CACHE:
        return _SECTION_CACHE[cache_key]
    base_list = _FAST_CATEGORY_CACHE.get(cat, [])
    if not base_list: return []
    if kw and kw not in ["ì „ì²´", "All"]:
        search_kw = kw.strip().lower()
        target_list = [i for i in base_list if search_kw in i['path'].lower() or search_kw in i['name'].lower()]
    else:
        target_list = base_list
    if not target_list: return []
    sections = [{"title": "ì „ì²´ ëª©ë¡", "items": target_list[:400]}]
    _SECTION_CACHE[cache_key] = sections
    return sections

@app.route('/category_sections')
def get_category_sections():
    cat = request.args.get('cat', 'movies')
    kw = request.args.get('kw')
    return gzip_response(get_sections_for_category(cat, kw))

@app.route('/home')
def get_home():
    return gzip_response(HOME_RECOMMEND)

@app.route('/list')
def get_list():
    p = request.args.get('path', '')
    m = {"ì˜í™”": "movies", "ì™¸êµ­TV": "foreigntv", "êµ­ë‚´TV": "koreantv", "ì• ë‹ˆë©”ì´ì…˜": "animations_all", "ë°©ì†¡ì¤‘": "air", "movies": "movies", "foreigntv": "foreigntv", "koreantv": "koreantv", "animations_all": "animations_all", "air": "air"}
    cat = "movies"
    for label, code in m.items():
        if label in p:
            cat = code
            break
    bl = _FAST_CATEGORY_CACHE.get(cat, [])
    kw = request.args.get('keyword')
    lim = int(request.args.get('limit', 1000))
    off = int(request.args.get('offset', 0))
    res = [item for item in bl if not kw or nfc(kw).lower() in item['path'].lower() or nfc(kw).lower() in item['name'].lower()] if kw and kw not in ["ì „ì²´", "All"] else bl
    return gzip_response(res[off:off+lim])

@app.route('/api/series_detail')
def get_series_detail_api():
    path = request.args.get('path')
    if not path: return gzip_response([])
    for c_path, data in _DETAIL_CACHE:
        if c_path == path: return gzip_response(data)
    conn = get_db()
    row = conn.execute('SELECT * FROM series WHERE path = ?', (path,)).fetchone()
    if not row:
        conn.close()
        return gzip_response([])
    series = dict(row)
    for col in ['genreIds', 'genreNames', 'actors']:
        if series.get(col):
            try: series[col] = json.loads(series[col])
            except: series[col] = []
    if series.get('tmdbId'):
        cursor = conn.execute("SELECT e.* FROM episodes e JOIN series s ON e.series_path = s.path WHERE s.tmdbId = ?", (series['tmdbId'],))
    else:
        cursor = conn.execute("SELECT * FROM episodes WHERE series_path = ?", (path,))
    eps = []
    seen = set()
    for r in cursor.fetchall():
        if r['videoUrl'] not in seen:
            eps.append(dict(r))
            seen.add(r['videoUrl'])
    series['movies'] = sorted(eps, key=lambda x: natural_sort_key(x['title']))
    conn.close()
    _DETAIL_CACHE.append((path, series))
    return gzip_response(series)

@app.route('/search')
def search_videos():
    q = request.args.get('q', '').lower()
    if not q: return gzip_response([])
    conn = get_db()
    cursor = conn.execute('SELECT s.*, e.id as ep_id, e.videoUrl, e.thumbnailUrl, e.title FROM series s LEFT JOIN episodes e ON s.path = e.series_path WHERE (s.path LIKE ? OR s.name LIKE ?) GROUP BY s.path ORDER BY s.name ASC', (f'%{q}%', f'%{q}%'))
    rows = []
    for row in cursor.fetchall():
        item = dict(row)
        item['movies'] = [{"id": item.pop('ep_id'), "videoUrl": item.pop('videoUrl'), "thumbnailUrl": item.pop('thumbnailUrl'), "title": item.pop('title')}] if item.get('ep_id') else []
        for col in ['genreIds', 'genreNames', 'actors']:
            if item.get(col):
                try: item[col] = json.loads(item[col])
                except: item[col] = []
        rows.append(item)
    conn.close()
    return gzip_response(rows)

@app.route('/rescan_broken')
def rescan_broken():
    threading.Thread(target=perform_full_scan, daemon=True).start()
    return jsonify({"status": "success"})

@app.route('/rematch_metadata')
def rescan_metadata():
    if IS_METADATA_RUNNING:
        return jsonify({"status": "error", "message": "Metadata process is already running."})
    threading.Thread(target=fetch_metadata_async, args=(True,), daemon=True).start()
    return jsonify({"status": "success", "message": "Full metadata rematch started in background."})

@app.route('/retry_failed_metadata')
def retry_failed_metadata():
    if IS_METADATA_RUNNING:
        return jsonify({"status": "error", "message": "Metadata process is already running."})
    conn = get_db()
    conn.execute('UPDATE series SET failed = 0 WHERE failed = 1')
    conn.commit()
    conn.close()
    threading.Thread(target=fetch_metadata_async, daemon=False).start()
    return jsonify({"status": "success", "message": "Retrying failed metadata with improved regex."})

@app.route('/video_serve')
def video_serve():
    path, prefix = request.args.get('path'), request.args.get('type')
    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
        return send_file(get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path)))), conditional=True)
    except:
        return "Not Found", 404

@app.route('/thumb_serve')
def thumb_serve():
    path, prefix, tid, t = request.args.get('path'), request.args.get('type'), request.args.get('id'), request.args.get('t', default="300")
    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
        vp = get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path))))
        tp = os.path.join(DATA_DIR, f"seek_{tid}_{t}.jpg")
        if not os.path.exists(tp):
            with THUMB_SEMAPHORE:
                subprocess.run([FFMPEG_PATH, "-y", "-skip_frame", "nokey", "-ss", t, "-i", vp, "-frames:v", "1", "-an", "-sn", "-map", "0:v:0", "-q:v", "6", "-vf", "scale=320:-1:flags=fast_bilinear", "-threads", "1", tp], timeout=8, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        return send_file(tp, mimetype='image/jpeg') if os.path.exists(tp) else ("Not Found", 404)
    except:
        return "Not Found", 404

def build_all_caches():
    global _SECTION_CACHE
    _SECTION_CACHE = {}
    _rebuild_fast_memory_cache()
    build_home_recommend()

def _rebuild_fast_memory_cache():
    global _FAST_CATEGORY_CACHE
    temp = {}
    conn = get_db()
    log("CACHE", "ê²½ëŸ‰ ë©”ëª¨ë¦¬ ìºì‹œ ë¹Œë“œ ì‹œì‘")
    for cat in ["movies", "foreigntv", "koreantv", "animations_all", "air"]:
        rows_dict = {}
        all_rows = conn.execute('SELECT path, name, posterPath, year, rating, genreNames, director, tmdbId FROM series WHERE category = ? ORDER BY name ASC', (cat,)).fetchall()
        for row in all_rows:
            path, name, poster, year, rating, g_names, director, t_id = row
            ct, yr = clean_title_complex(name)
            group_key = f"tmdb:{t_id}" if t_id else f"name:{ct}_{yr}"
            if group_key not in rows_dict:
                try: genre_list = json.loads(g_names) if g_names else []
                except: genre_list = []
                rows_dict[group_key] = {
                    "path": path, "name": name, "posterPath": poster,
                    "year": year, "rating": rating, "genreNames": genre_list,
                    "director": director, "tmdbId": t_id, "movies": []
                }
        temp[cat] = list(rows_dict.values())
    conn.close()
    _FAST_CATEGORY_CACHE = temp
    log("CACHE", "ê²½ëŸ‰ ë©”ëª¨ë¦¬ ìºì‹œ ë¹Œë“œ ì™„ë£Œ")

def build_home_recommend():
    global HOME_RECOMMEND
    try:
        m = _FAST_CATEGORY_CACHE.get('movies', [])
        k = _FAST_CATEGORY_CACHE.get('koreantv', [])
        a = _FAST_CATEGORY_CACHE.get('air', [])
        combined = m + k
        unique_map = {}
        for item in combined:
            uid = item.get('tmdbId') or item.get('path')
            if uid not in unique_map:
                unique_map[uid] = item
        unique_hot_list = list(unique_map.values())
        hot_picks = random.sample(unique_hot_list, min(100, len(unique_hot_list))) if unique_hot_list else []
        seen_ids = { (p.get('tmdbId') or p.get('path')) for p in hot_picks }
        airing_picks = []
        for item in a:
            uid = item.get('tmdbId') or item.get('path')
            if uid not in seen_ids:
                airing_picks.append(item)
                if len(airing_picks) >= 100: break
        HOME_RECOMMEND = [
            {"title": "ì§€ê¸ˆ ê°€ì¥ í•«í•œ ì¸ê¸°ì‘", "items": hot_picks},
            {"title": "ì‹¤ì‹œê°„ ë°©ì˜ ì¤‘", "items": airing_picks}
        ]
        log("CACHE", f"í™ˆ ì¶”ì²œ ë¹Œë“œ ì™„ë£Œ ({len(hot_picks)} / {len(airing_picks)})")
    except:
        traceback.print_exc()

def report_db_status():
    try:
        conn = get_db()
        eps = conn.execute("SELECT COUNT(*) FROM episodes").fetchone()[0]
        ser = conn.execute("SELECT COUNT(*) FROM series").fetchone()[0]
        mtch = conn.execute("SELECT COUNT(*) FROM series WHERE tmdbId IS NOT NULL").fetchone()[0]
        log("DB", f"STATUS: ì—í”¼ì†Œë“œ {eps} / ì‹œë¦¬ì¦ˆ {ser} / ë§¤ì¹­ ì„±ê³µ {mtch} ({round(mtch/ser*100, 1)}%)")
        conn.close()
    except: pass

def background_init_tasks():
    report_db_status()
    build_all_caches()

@app.route('/api/status')
def get_server_status():
    try:
        conn = get_db()
        eps = conn.execute("SELECT COUNT(*) FROM episodes").fetchone()[0]
        ser = conn.execute("SELECT COUNT(*) FROM series").fetchone()[0]
        mtch = conn.execute("SELECT COUNT(*) FROM series WHERE tmdbId IS NOT NULL").fetchone()[0]
        fail = conn.execute("SELECT COUNT(*) FROM series WHERE failed = 1").fetchone()[0]
        conn.close()
        return jsonify({
            "total_episodes": eps,
            "total_series": ser,
            "matched_series": mtch,
            "failed_series": fail,
            "success_rate": f"{round(mtch/ser*100, 1)}%" if ser > 0 else "0%"
        })
    except:
        return jsonify({"error": traceback.format_exc()})

if __name__ == '__main__':
    init_db()
    threading.Thread(target=background_init_tasks, daemon=True).start()
    app.run(host='0.0.0.0', port=5000, threaded=True)
