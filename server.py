import os, subprocess, hashlib, urllib.parse, unicodedata, threading, time, json, re, sys, traceback, shutil, requests, \
    random, mimetypes, sqlite3, gzip
from flask import Flask, jsonify, send_from_directory, request, Response, redirect, send_file, make_response
from flask_cors import CORS
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from collections import deque
from io import BytesIO

app = Flask(__name__)
CORS(app)

# MIME íƒ€ì… ì¶”ê°€ ë“±ë¡ (ê°•ì œ ì ìš©)
mimetypes.add_type('video/x-matroska', '.mkv')
mimetypes.add_type('video/mp2t', '.ts')
mimetypes.add_type('video/mp2t', '.tp')


# --- [ìµœì í™”: Gzip ì••ì¶• í•¨ìˆ˜ ì¶”ê°€] ---
def gzip_response(data):
    content = gzip.compress(json.dumps(data, ensure_ascii=False).encode('utf-8'))
    response = make_response(content)
    response.headers['Content-Encoding'] = 'gzip'
    response.headers['Content-Type'] = 'application/json'
    response.headers['Content-Length'] = len(content)
    return response


# --- [1. ì„¤ì • ë° ê²½ë¡œ] ---
MY_IP = "192.168.0.2"
DATA_DIR = "/volume2/video/thumbnails"
DB_FILE = "/volume2/video/video_metadata.db"
TMDB_CACHE_DIR = "/volume2/video/tmdb_cache"
HLS_ROOT = "/dev/shm/videoplayer_hls"
SUBTITLE_DIR = "/volume2/video/subtitles"  # ìë§‰ ì €ì¥ ê²½ë¡œ
CACHE_VERSION = "137.31"  # ì—í”¼ì†Œë“œ ì‹¤ì‹œê°„ ë¡œê¹… ê°•í™” ë²„ì „

# [ìˆ˜ì •] ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ìƒì„± ë³´ì¥
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
FAILURE_LOG_PATH = os.path.join(SCRIPT_DIR, "metadata_failures.txt")
SUCCESS_LOG_PATH = os.path.join(SCRIPT_DIR, "metadata_success.txt")

TMDB_MEMORY_CACHE = {}
TMDB_API_KEY = "eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiI3OGNiYWQ0ZjQ3NzcwYjYyYmZkMTcwNTA2NDIwZDQyYyIsIm5iZiI6MTY1MzY3NTU4MC45MTUsInN1YiI6IjYyOTExNjNjMTI0MjVjMDA1MjI0ZGQzNCIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.3YU0WuIx_WDo6nTRKehRtn4N5I4uCgjI1tlpkqfsUhk".strip()
TMDB_BASE_URL = "https://api.themoviedb.org/3"

# [ì¶”ê°€] ë§¤ì¹­ ì§„ë‹¨ìš© ì „ì—­ ë³€ìˆ˜
MATCH_DIAGNOSTICS = {}

# --- [ì¶”ê°€] ì‹¤ì‹œê°„ UI ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì „ì—­ ìƒíƒœ ê´€ë¦¬ ---
UPDATE_STATE = {
    "is_running": False,
    "task_name": "ëŒ€ê¸° ì¤‘",
    "total": 0,
    "current": 0,
    "success": 0,
    "fail": 0,
    "current_item": "-",
    "logs": deque(maxlen=300)  # ìµœê·¼ 300ê°œì˜ ë¡œê·¸ë§Œ ìœ ì§€í•˜ì—¬ ë©”ëª¨ë¦¬ ìµœì í™”
}
UPDATE_LOCK = threading.Lock()


def set_update_state(is_running=None, task_name=None, total=None, current=None, success=None, fail=None,
                     current_item=None, clear_logs=False):
    with UPDATE_LOCK:
        if is_running is not None: UPDATE_STATE["is_running"] = is_running
        if task_name is not None: UPDATE_STATE["task_name"] = task_name
        if total is not None: UPDATE_STATE["total"] = total
        if current is not None: UPDATE_STATE["current"] = current
        if success is not None: UPDATE_STATE["success"] = success
        if fail is not None: UPDATE_STATE["fail"] = fail
        if current_item is not None: UPDATE_STATE["current_item"] = current_item
        if clear_logs: UPDATE_STATE["logs"].clear()


def emit_ui_log(msg, log_type='info'):
    timestamp = datetime.now().strftime("%H:%M:%S")
    with UPDATE_LOCK:
        UPDATE_STATE["logs"].append({"time": timestamp, "msg": msg, "type": log_type})


# -----------------------------------------------------------

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(TMDB_CACHE_DIR, exist_ok=True)
os.makedirs(SUBTITLE_DIR, exist_ok=True)  # ìë§‰ í´ë” ìƒì„±
if os.path.exists(HLS_ROOT): shutil.rmtree(HLS_ROOT, ignore_errors=True)
os.makedirs(HLS_ROOT, exist_ok=True)

PARENT_VIDEO_DIR = "/volume2/video/GDS3/GDRIVE/VIDEO"
PATH_MAP = {
    "ì™¸êµ­TV": (os.path.join(PARENT_VIDEO_DIR, "ì™¸êµ­TV"), "ftv"),
    "êµ­ë‚´TV": (os.path.join(PARENT_VIDEO_DIR, "êµ­ë‚´TV"), "ktv"),
    "ì˜í™”": (os.path.join(PARENT_VIDEO_DIR, "ì˜í™”"), "movie"),
    "ì• ë‹ˆë©”ì´ì…˜": (os.path.join(PARENT_VIDEO_DIR, "ì¼ë³¸ ì• ë‹ˆë©”ì´ì…˜"), "anim_all"),
    "ë°©ì†¡ì¤‘": (os.path.join(PARENT_VIDEO_DIR, "ë°©ì†¡ì¤‘"), "air")
}

EXCLUDE_FOLDERS = ["ì„±ì¸", "19ê¸ˆ", "Adult", "@eaDir", "#recycle"]
VIDEO_EXTS = ('.mp4', '.mkv', '.avi', '.wmv', '.flv', '.ts', '.tp', '.m4v', '.m2ts', '.mov')

# [ê°œì„ ] ë” ë§ì€ FFmpeg ê²½ë¡œ íƒìƒ‰ (ì‹œë†€ë¡œì§€ í™˜ê²½ ê³ ë ¤)
FFMPEG_PATH = "ffmpeg"
for p in ["/usr/local/bin/ffmpeg", "/var/packages/ffmpeg/target/bin/ffmpeg", "/usr/bin/ffmpeg",
          "/var/packages/VideoStation/target/bin/ffmpeg", "/var/packages/CodecPack/target/bin/ffmpeg"]:
    if os.path.exists(p):
        FFMPEG_PATH = p
        break

# [ì¶”ê°€] FFprobe ê²½ë¡œ ì„¤ì • (ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„±ìš©)
FFPROBE_PATH = "ffprobe"
for p in ["/usr/local/bin/ffprobe", "/var/packages/ffmpeg/target/bin/ffprobe", "/usr/bin/ffprobe",
          "/var/packages/VideoStation/target/bin/ffprobe"]:
    if os.path.exists(p):
        FFPROBE_PATH = p
        break

HOME_RECOMMEND = []
IS_METADATA_RUNNING = False
_FAST_CATEGORY_CACHE = {}
_SECTION_CACHE = {}  # ì¹´í…Œê³ ë¦¬ ì„¹ì…˜ ê²°ê³¼ ìºì‹œ ì¶”ê°€
_DETAIL_CACHE = deque(maxlen=200)

THUMB_SEMAPHORE = threading.Semaphore(4)
STORYBOARD_SEMAPHORE = threading.Semaphore(2)  # [ì¶”ê°€] ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„±ìš© ì„¸ë§ˆí¬ì–´
THUMB_EXECUTOR = ThreadPoolExecutor(max_workers=8)
SUBTITLE_EXECUTOR = ThreadPoolExecutor(max_workers=2)  # ìë§‰ ì¶”ì¶œ ì „ìš© ëŒ€ê¸°ì—´ (ìµœëŒ€ 2ê°œ ë™ì‹œ ì²˜ë¦¬)

ACTIVE_EXTRACTIONS = set()
EXTRACTION_LOCK = threading.Lock()

def log(tag, msg):
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] [{tag}] {msg}", flush=True)


def log_matching_failure(orig, cleaned, reason):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with open(FAILURE_LOG_PATH, "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] [FAIL] ORIG: {orig} | CLEANED: {cleaned} | REASON: {reason}\n")
    except Exception as e:
        log("LOG_ERROR", f"ì‹¤íŒ¨ ë¡œê·¸ íŒŒì¼ ì“°ê¸° ì¤‘ ì—ëŸ¬: {str(e)}")


def log_matching_success(orig, cleaned, matched, tmdb_id):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with open(SUCCESS_LOG_PATH, "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] [OK] ORIG: {orig} | CLEANED: {cleaned} | MATCHED: {matched} | ID: {tmdb_id}\n")
    except:
        pass


def nfc(text):
    return unicodedata.normalize('NFC', text) if text else ""


def nfd(text):
    return unicodedata.normalize('NFD', text) if text else ""


# --- [DB ê´€ë¦¬] ---
def get_db():
    # ì—°ê²° ëŒ€ê¸° ì‹œê°„ì„ 60ì´ˆë¡œ ì„¤ì • (ê¸°ë³¸ê°’ë³´ë‹¤ ê¸¸ê²Œ)
    conn = sqlite3.connect(DB_FILE, timeout=60)
    conn.row_factory = sqlite3.Row

    # ë™ì‹œì„± í–¥ìƒì„ ìœ„í•´ WAL ëª¨ë“œ í™œì„±í™” ì‹œë„
    try:
        # WAL ëª¨ë“œ ì„¤ì • ì‹œ ë½ì´ ê±¸ë ¤ë„ ì „ì²´ í”„ë¡œì„¸ìŠ¤ê°€ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡ í•¨
        conn.execute('PRAGMA journal_mode=WAL')
        # busy_timeoutì„ í•œë²ˆ ë” ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • (ë°€ë¦¬ì´ˆ ë‹¨ìœ„, 30000ms = 30ì´ˆ)
        conn.execute('PRAGMA busy_timeout = 30000')
        # [ì¶”ê°€] temp_storeë¥¼ ë©”ëª¨ë¦¬ë¡œ ë³€ê²½í•˜ì—¬ ë””ìŠ¤í¬ I/O ìµœì í™” ë° ë””ìŠ¤í¬ í’€ë¦¼ í˜„ìƒ ì™„í™”
        conn.execute('PRAGMA temp_store = MEMORY')
    except sqlite3.OperationalError as e:
        log("DB_ERROR", f"WAL ëª¨ë“œ ì„¤ì • ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
    except Exception as e:
        log("DB_ERROR", f"ê¸°íƒ€ DB ì„¤ì • ì˜¤ë¥˜: {e}")

    return conn


def init_db():
    try:
        conn = get_db()
        cursor = conn.cursor()
        cursor.execute(
            'CREATE TABLE IF NOT EXISTS series (path TEXT PRIMARY KEY, category TEXT, name TEXT, posterPath TEXT, year TEXT, overview TEXT, rating TEXT, seasonCount INTEGER, genreIds TEXT, genreNames TEXT, director TEXT, actors TEXT, failed INTEGER DEFAULT 0, tmdbId TEXT)')
        cursor.execute(
            'CREATE TABLE IF NOT EXISTS episodes (id TEXT PRIMARY KEY, series_path TEXT, title TEXT, videoUrl TEXT, thumbnailUrl TEXT, overview TEXT, air_date TEXT, season_number INTEGER, episode_number INTEGER, FOREIGN KEY (series_path) REFERENCES series (path) ON DELETE CASCADE)')
        cursor.execute('CREATE TABLE IF NOT EXISTS tmdb_cache (h TEXT PRIMARY KEY, data TEXT)')
        cursor.execute('CREATE TABLE IF NOT EXISTS server_config (key TEXT PRIMARY KEY, value TEXT)')

        cursor.execute('CREATE INDEX IF NOT EXISTS idx_series_category ON series(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_series_name ON series(name)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_series_tmdbId ON series(tmdbId)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_episodes_series ON episodes(series_path)')

        def add_col_if_missing(table, col, type):
            cursor.execute(f"PRAGMA table_info({table})")
            cols = [c[1] for c in cursor.fetchall()]
            if col not in cols:
                log("DB", f"ì»¬ëŸ¼ ì¶”ê°€: {table}.{col}")
                cursor.execute(f"ALTER TABLE {table} ADD COLUMN {col} {type}")

        add_col_if_missing('series', 'tmdbId', 'TEXT')
        add_col_if_missing('series', 'genreNames', 'TEXT')
        add_col_if_missing('series', 'director', 'TEXT')
        add_col_if_missing('series', 'actors', 'TEXT')
        add_col_if_missing('series', 'cleanedName', 'TEXT')
        add_col_if_missing('series', 'yearVal', 'TEXT')

        add_col_if_missing('episodes', 'overview', 'TEXT')
        add_col_if_missing('episodes', 'air_date', 'TEXT')
        add_col_if_missing('episodes', 'season_number', 'INTEGER')
        add_col_if_missing('episodes', 'episode_number', 'INTEGER')

        cursor.execute('CREATE INDEX IF NOT EXISTS idx_series_cleanedName ON series(cleanedName)')

        conn.commit()
        conn.close()
        log("DB", "ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ìµœì í™” ì™„ë£Œ")
    except sqlite3.OperationalError as e:
        log("DB", f"ì´ˆê¸°í™” ì¤‘ ë½ ë°œìƒ: {e}. ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")


# --- [ìœ í‹¸ë¦¬í‹°] ---
def get_real_path(path):
    if not path or os.path.exists(path): return path
    if os.path.exists(nfc(path)): return nfc(path)
    if os.path.exists(nfd(path)): return nfd(path)
    return path


def migrate_json_to_db():
    if not os.path.exists(TMDB_CACHE_DIR): return
    conn = get_db()
    row = conn.execute("SELECT value FROM server_config WHERE key = 'json_migration_done'").fetchone()
    if row and row['value'] == 'true':
        conn.close()
        return
    files = [f for f in os.listdir(TMDB_CACHE_DIR) if f.endswith(".json")]
    if not files:
        conn.close()
        return
    log("MIGRATE", f"JSON ìºì‹œ {len(files)}ê°œ DB ì´ê´€ ì¤‘...")
    for idx, f in enumerate(files):
        h = f.replace(".json", "")
        try:
            with open(os.path.join(TMDB_CACHE_DIR, f), 'r', encoding='utf-8') as file:
                data = json.load(file)
                conn.execute('INSERT OR REPLACE INTO tmdb_cache (h, data) VALUES (?, ?)', (h, json.dumps(data)))
        except:
            pass
        if (idx + 1) % 2000 == 0:
            conn.commit()
            log("MIGRATE", f"ì§„í–‰ ì¤‘... ({idx + 1}/{len(files)})")
    conn.execute("INSERT OR REPLACE INTO server_config (key, value) VALUES ('json_migration_done', 'true')")
    conn.commit()
    conn.close()
    log("MIGRATE", "ì´ê´€ ì™„ë£Œ")


# --- [ì •ê·œì‹ ë° í´ë¦¬ë‹ ëŒ€í­ ê°•í™”] ---
REGEX_EXT = re.compile(r'\.[a-zA-Z0-9]{2,4}$')
REGEX_YEAR = re.compile(r'\((19|20)\d{2}\)|(?<!\d)(19|20)\d{2}(?!\d)')
REGEX_CH_PREFIX = re.compile(
    r'^\[(?:KBS|SBS|MBC|tvN|JTBC|OCN|Mnet|TVì¡°ì„ |ì±„ë„A|MBN|ENA|KBS2|KBS1|CH\d+|TV|Netflix|Disney\+|AppleTV|NET|Wavve|Tving|Coupang)\]\s*')
# [ê°œì„ ] ê¸°ìˆ ì  íƒœê·¸: í•œê¸€ ë‹¨ì–´ ì¼ë¶€ë¥¼ íƒœê·¸ë¡œ ì˜¤í•´í•˜ì§€ ì•Šë„ë¡ ê²½ê³„ ì¡°ê±´ ê°•í™”
REGEX_TECHNICAL_TAGS = re.compile(
    r'(?i)[.\s_-](?!(?:\d+\b))(\d{3,4}p|2160p|FHD|QHD|UHD|4K|Bluray|Blu-ray|WEB-DL|WEBRip|HDRip|BDRip|DVDRip|H\.?26[45]|x26[45]|HEVC|AVC|AAC\d?|DTS-?H?D?|AC3|DDP\d?|DD\+\d?|Dual|Atmos|REPACK|10bit|REMUX|FLAC|xvid|DivX|MKV|MP4|AVI|HDR(?:10)?(?:\+)?|Vision|Dolby|NF|AMZN|HMAX|DSNP|AppleTV?|Disney|PCOK|playWEB|ATVP|HULU|HDTV|HD|KBS|SBS|MBC|TVN|JTBC|NEXT|ST|SW|KL|YT|MVC|KN|FLUX|hallowed|PiRaTeS|Jadewind|Movie|pt\s*\d+|KOREAN|KOR|ITALIAN|JAPANESE|JPN|CHINESE|CHN|ENGLISH|ENG|USA|HK|TW|FRENCH|GERMAN|SPANISH|THAI|VIETNAMESE|WEB|DL|TVRip|HDR10Plus|IMAX|Unrated|REMASTERED|Criterion|NonDRM|BRRip|1080i|720i|êµ­ì–´|Mandarin|Cantonese|FanSub|VFQ|VF|2CH|5\.1CH|8m|2398|PROPER|PROMO|LIMITED|RM4K|DC|THEATRICAL|EXTENDED|FINAL|DUB|KORDUB|JAPDUB|ENGDUB|ARROW|EDITION|SPECIAL|COLLECTION|RETAIL|TVING|WAVVE|Coupang|CP|B-Global|TrueHD|E-AC3|EAC3|DV|Dual-Audio|Multi-Audio|Multi-Sub)(?:\b|[.\s_-]|$)')

# [ê°œì„ ] ì—í”¼ì†Œë“œ ë²ˆí˜¸ ì¶”ì¶œ íŒ¨í„´ ëŒ€í­ ê°•í™” (í™”/íšŒ/ê¸°/ë¶€/è©± ë’¤ì— ë°”ë¡œ ì˜¤ëŠ” ê²½ìš°ë„ í—ˆìš©)
REGEX_EP_MARKER_STRICT = re.compile(
    r'(?i)(?:(?<=[\uac00-\ud7af\u3040-\u30ff\u4e00-\u9fff])|[.\s_-]|^)(?:ç¬¬?\s*S(\d+)E(\d+)(?:[-~]E?\d+)?(?:[í™”íšŒê¸°ë¶€è©±])?|ç¬¬?\s*S(\d+)|ç¬¬?\s*E(\d+)(?:[-~]\d+)?(?:[í™”íšŒê¸°ë¶€è©±])?|(\d+)\s*(?:í™”|íšŒ|ê¸°|ë¶€|è©±)|Season\s*(\d+)|Episode\s*(\d+)|ì‹œì¦Œ\s*(\d+))(?:\b|[.\s_-]|$)')

REGEX_DATE_YYMMDD = re.compile(r'(?<!\d)\d{6}(?!\d)')
REGEX_FORBIDDEN_CONTENT = re.compile(
    r'(?i)(Storyboard|Behind the Scenes|Making of|Deleted Scenes|Alternate Scenes|Gag Reel|Gag Menu|Digital Hits|Trailer|Bonus|Extras|Gallery|Production|Visual Effects|VFX|ë“±ê¸‰ê³ ì§€|ì˜ˆê³ í¸|ê°œë´‰ë²„ì „|ì¸í„°ë·°|ì‚­ì œì¥ë©´|(?<!\S)[ìƒí•˜](?!\S))')
REGEX_FORBIDDEN_TITLE = re.compile(
    r'(?i)^\s*(Season\s*\d+|Part\s*\d+|EP\s*\d+|\d+í™”|\d+íšŒ|\d+ê¸°|ì‹œì¦Œ\s*\d+|S\d+|E\d+|Disk\s*\d+|Disc\s*\d+|CD\s*\d+|Specials?|Extras?|Bonus|ë¯¸ë¶„ë¥˜|ê¸°íƒ€|ìƒˆ\s*í´ë”|VIDEO|GDS3|GDRIVE|NAS|share|ì˜í™”|ì™¸êµ­TV|êµ­ë‚´TV|ì• ë‹ˆë©”ì´ì…˜|ë°©ì†¡ì¤‘|ì œëª©|UHD|ìµœì‹ |ìµœì‹ ì‘|ìµœì‹ ì˜í™”|4K|1080P|720P)\s*$',
    re.I)

REGEX_BRACKETS = re.compile(
    r'\[.*?(?:\]|$)|\(.*?(?:\)|$)|\{.*?(?:\)|$)|\ã€.*?(?:\ã€‘|$)|\ã€.*?(?:\ã€|$)|\ã€Œ.*?(?:\ã€|$)|\ï¼ˆ.*?(?:\ï¼‰|$)')
REGEX_TMDB_HINT = re.compile(r'\{tmdb[\s-]*(\d+)\}')
# [ì¶”ê°€] ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì¶”ê°€ (í•œêµ­ì–´ë”ë¹™, íë ˆì´ì…˜, ë‹¨í¸ ë“±)
REGEX_JUNK_KEYWORDS = re.compile(
    r'(?i)\s*(?:ë”ë¹™|ìë§‰|í•œêµ­ì–´|ê·¹ì¥íŒ|BD|TV|Web|OAD|OVA|ONA|Full|ë¬´ì‚­ì œ|ê°ë…íŒ|í™•ì¥íŒ|ìµìŠ¤í…ë””ë“œ|ë“±ê¸‰ê³ ì§€|ì˜ˆê³ í¸|(?<!\S)[ìƒí•˜](?!\S)|ê·¹ì¥íŒ\s*\d+ê¸°|íŠ¹ì§‘\s*ë‹¤í|\d+ë¶€ì‘|íë ˆì´ì…˜|ë‹¨í¸|ë“œë¼ë§ˆ)\s*')

# [ìˆ˜ì •] íŠ¹ìˆ˜ë¬¸ì ì œê±° ì‹œ í•˜ì´í”ˆ(-)ê³¼ ì½œë¡ (:)ì€ ì œì™¸í•˜ì—¬ ë¶€ì œ ë¶„ë¦¬ì— ì‚¬ìš© (ë³„í‘œ ì¶”ê°€)
REGEX_SPECIAL_CHARS = re.compile(r'[\[\]()_\.!#@*â€»Ã—,~;ã€ã€‘ã€ã€ã€Œã€"\'ï¼ˆï¼‰â˜†â˜…]')
REGEX_LEADING_INDEX = re.compile(r'^\s*(\d{1,5}(?:\s+|[.\s_-]+|(?=[ê°€-í£a-zA-Z])))|^\s*(\d{1,5}\. )')
REGEX_SPACES = re.compile(r'\s+')


def clean_title_complex(title):
    if not title: return "", None
    orig_title = nfc(title)

    if REGEX_FORBIDDEN_CONTENT.search(orig_title):
        return "", None

    cleaned = REGEX_EXT.sub('', orig_title)
    cleaned = REGEX_CH_PREFIX.sub('', cleaned)
    cleaned = REGEX_TMDB_HINT.sub('', cleaned)

    # [ìˆ˜ì •] ì—°ë„ ì •ë³´ ë¯¸ë¦¬ ì¶”ì¶œ (ë¸Œë˜í‚· ì œê±° ì „)
    year_match = REGEX_YEAR.search(cleaned)
    year = year_match.group().replace('(', '').replace(')', '') if year_match else None

    # [ìˆ˜ì •] ë§ˆì»¤ í™•ì¸ ì‹œ ì œëª©ì´ ë„ˆë¬´ ë§ì´ ì˜ë ¤ë‚˜ê°€ëŠ” ê²ƒì„ ë°©ì§€
    ep_match = REGEX_EP_MARKER_STRICT.search(cleaned)
    if ep_match:
        # [ê°œì„ ] EP ë§ˆì»¤ ì•ë¶€ë¶„ì´ 2ì ë¯¸ë§Œì´ê±°ë‚˜ í•œê¸€ì´ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ ë’·ë¶€ë¶„ì„ ì·¨í•¨
        pre = cleaned[:ep_match.start()].strip()
        if len(pre) >= 2 and not REGEX_FORBIDDEN_TITLE.match(pre):
            cleaned = pre
        elif len(pre) >= 1 and any('\uac00' <= c <= '\ud7af' for c in pre):
            cleaned = pre
        else:
            # ì•ë¶€ë¶„ì´ ì˜ë¯¸ ì—†ìœ¼ë©´ ë§ˆì»¤ ì´í›„ë¥¼ ë³´ë˜, ì´í›„ë„ ë„ˆë¬´ ì§§ìœ¼ë©´ ì›ë³¸ ì œëª© í™œìš© ê³ ë ¤
            post = cleaned[ep_match.end():].strip()
            if len(post) >= 2:
                cleaned = post
            else:
                cleaned = pre if pre else post

    tech_match = REGEX_TECHNICAL_TAGS.search(cleaned)
    if tech_match:
        # [ê°œì„ ] ê¸°ìˆ  íƒœê·¸ì— ì˜í•´ ì œëª©ì´ ë„ˆë¬´ ì§§ì•„ì§€ë©´(1ì ì´í•˜) ìë¥´ì§€ ì•ŠìŒ
        pre_tech = cleaned[:tech_match.start()].strip()
        if len(pre_tech) >= 2:
            cleaned = pre_tech

    # [ê°œì„ ] ìˆ«ì ë¶„ë¦¬ ë¡œì§: ë‚ ì§œë‚˜ ì—°ë„ê°€ ê¹¨ì§€ì§€ ì•Šë„ë¡ í•œê¸€/ì˜ì–´ ê²½ê³„ë§Œ ì²˜ë¦¬
    cleaned = re.sub(r'([ê°€-í£\u3040-\u30ff\u4e00-\u9fff])([a-zA-Z])', r'\1 \2', cleaned)
    cleaned = re.sub(r'([a-zA-Z])([ê°€-í£\u3040-\u30ff\u4e00-\u9fff])', r'\1 \2', cleaned)

    cleaned = REGEX_DATE_YYMMDD.sub(' ', cleaned)
    cleaned = REGEX_YEAR.sub(' ', cleaned)

    # ì •ì œ í›„ ë„ˆë¬´ ì§§ì•„ì§„ ê²½ìš° ë¸Œë˜í‚· ë‚´ë¶€ì—ì„œ ëŒ€ì²´ ì œëª© ì°¾ê¸°
    if len(cleaned.strip()) < 2:
        brackets = re.findall(r'\[(.*?)\]|\((.*?)\)|ï¼ˆ(.*?)ï¼‰', orig_title)
        for b in brackets:
            inner = (b[0] or b[1] or b[2] or "").strip()
            if len(inner) >= 2 and not REGEX_TECHNICAL_TAGS.search(inner) and not REGEX_FORBIDDEN_TITLE.match(inner):
                cleaned = inner
                break

    cleaned = REGEX_BRACKETS.sub(' ', cleaned)
    cleaned = cleaned.replace("(ìë§‰)", "").replace("(ë”ë¹™)", "").replace("[ìë§‰]", "").replace("[ë”ë¹™]", "").replace("ï¼ˆìë§‰ï¼‰",
                                                                                                              "").replace(
        "ï¼ˆë”ë¹™ï¼‰", "")
    cleaned = REGEX_JUNK_KEYWORDS.sub(' ', cleaned)

    # [ìˆ˜ì •] ì (.)ì„ ë¬´ì¡°ê±´ ì œê±°í•˜ê¸° ì „ì— ê³µë°±ìœ¼ë¡œ ë³€í™˜ (ìˆ«ì ë³´í˜¸ ìœ„í•´ íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ì—ì„œ ë‹¤ë£¸)
    cleaned = REGEX_SPECIAL_CHARS.sub(' ', cleaned)
    cleaned = REGEX_LEADING_INDEX.sub('', cleaned)
    cleaned = REGEX_SPACES.sub(' ', cleaned).strip()

    if len(cleaned) < 1:
        # ìµœì¢… ì •ì œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì œëª©ì—ì„œ í™•ì¥ìë§Œ ë–¼ê³  ë°˜í™˜ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        return nfc(os.path.splitext(orig_title)[0]), year
    return nfc(cleaned), year


def extract_episode_numbers(filename):
    match = REGEX_EP_MARKER_STRICT.search(filename)
    if match:
        # S01E05 í˜•ì‹
        if match.group(1) and match.group(2): return int(match.group(1)), int(match.group(2))
        # S01 í˜•ì‹
        if match.group(3): return int(match.group(3)), 1
        # E05 í˜•ì‹
        if match.group(4): return 1, int(match.group(4))
        # 13í™”, 13íšŒ í˜•ì‹
        if match.group(5): return 1, int(match.group(5))
        # Season 2, Episode 3, ì‹œì¦Œ 2 í˜•ì‹
        if match.group(6): return int(match.group(6)), 1
        if match.group(7): return 1, int(match.group(7))
        if match.group(8): return int(match.group(8)), 1
    return 1, None


def natural_sort_key(s):
    if s is None: return []
    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\d+)', nfc(str(s)))]


def extract_tmdb_id(title):
    match = REGEX_TMDB_HINT.search(nfc(title))
    return int(match.group(1)) if match else None


def simple_similarity(s1, s2):
    s1, s2 = s1.lower().replace(" ", ""), s2.lower().replace(" ", "")
    if s1 == s2: return 1.0
    if s1 in s2 or s2 in s1: return 0.8
    return 0.0


# --- [TMDB API ë³´ì™„: ì§€ëŠ¥í˜• ì¬ê²€ìƒ‰ ë° ë­í‚¹ ì‹œìŠ¤í…œ] ---
def get_tmdb_info_server(title, category=None, ignore_cache=False):  # category ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
    if not title: return {"failed": True}
    hint_id = extract_tmdb_id(title)
    ct, year = clean_title_complex(title)
    if not ct or REGEX_FORBIDDEN_TITLE.match(ct):
        return {"failed": True, "forbidden": True}

    # ì¹´í…Œê³ ë¦¬ì— ë”°ë¥¸ ì„ í˜¸ íƒ€ì… ê²°ì • (Taxi Driver ë“± ë™ëª… íƒ€ì´í‹€ ì˜¤ë§¤ì¹­ ë°©ì§€)
    pref_mtype = 'movie' if (category == 'movies' or 'ê·¹ì¥íŒ' in title) else 'tv' if category in ['koreantv', 'foreigntv',
                                                                                               'air',
                                                                                               'animations_all'] else None

    cache_key = f"{ct}_{year}_{category}" if year else f"{ct}_{category}"
    h = hashlib.md5(nfc(cache_key).encode()).hexdigest()

    if not ignore_cache and h in TMDB_MEMORY_CACHE:
        return TMDB_MEMORY_CACHE[h]

    if not ignore_cache:
        try:
            conn = get_db()
            row = conn.execute('SELECT data FROM tmdb_cache WHERE h = ?', (h,)).fetchone()
            conn.close()
            if row:
                data = json.loads(row['data'])
                TMDB_MEMORY_CACHE[h] = data
                return data
        except:
            pass

    log("TMDB", f"ğŸ” ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹œì‘: '{ct}'" + (f" ({year})" if year else "") + (f" [Cat: {category}]" if category else ""))
    headers = {"Authorization": f"Bearer {TMDB_API_KEY}"}
    base_params = {"include_adult": "true", "region": "KR"}

    def perform_search(query, lang=None, m_type='multi', search_year=None):
        if not query or len(query) < 1: return []
        params = {**base_params, "query": query}
        if lang: params["language"] = lang
        if search_year:
            params['year' if m_type == 'movie' else 'first_air_date_year' if m_type == 'tv' else 'year'] = search_year
            if m_type == 'multi': params['year'] = search_year
        try:
            r = requests.get(f"{TMDB_BASE_URL}/search/{m_type}", params=params, headers=headers, timeout=10)
            return r.json().get('results', []) if r.status_code == 200 else []
        except:
            return []

    def rank_results(results, target_title, target_year, pref_type=None):
        if not results: return None, []
        scored = []
        for res in results:
            if res.get('media_type') == 'person': continue
            m_type = res.get('media_type') or ('movie' if res.get('title') else 'tv')
            score = 0
            res_title = res.get('title') or res.get('name') or ""
            res_year = (res.get('release_date') or res.get('first_air_date') or "").split('-')[0]

            sim = simple_similarity(target_title, res_title)
            score += sim * 60

            if target_year and res_year:
                if target_year == res_year:
                    score += 30
                elif abs(int(target_year) - int(res_year)) <= 1:
                    score += 15
            elif not target_year:
                score += 10

            score += min(res.get('popularity', 0) / 10, 10)
            if res.get('poster_path'): score += 5

            # [ì¶”ê°€] ì„ í˜¸í•˜ëŠ” íƒ€ì…(ì˜í™”/TV)ê³¼ ì¼ì¹˜í•  ê²½ìš° í° ê°€ì¤‘ì¹˜ ë¶€ì—¬
            if pref_type and m_type == pref_type:
                score += 40

            scored.append((score, res))

        scored.sort(key=lambda x: x[0], reverse=True)

        # [ì¶”ê°€] ì§„ë‹¨ ë°ì´í„° ìˆ˜ì§‘
        candidates = []
        for s, r in scored[:3]:
            candidates.append({
                "title": r.get('title') or r.get('name'),
                "year": (r.get('release_date') or r.get('first_air_date') or "").split('-')[0],
                "score": round(s, 1),
                "type": r.get('media_type') or ('movie' if r.get('title') else 'tv')
            })

        # [ìˆ˜ì •] ë°˜í™˜ ì‹œ í•­ìƒ ë‘ ê°œì˜ ê°’ì„ ë°˜í™˜í•˜ë„ë¡ ë³´ì¥
        best = scored[0][1] if scored and scored[0][0] > 35 else None
        return best, candidates

    try:
        best_match = None
        all_candidates = []

        if hint_id:
            log("TMDB", f"ğŸ’¡ íŒíŠ¸ ID ì‚¬ìš©: {hint_id}")
            for mt in ['movie', 'tv']:
                resp = requests.get(f"{TMDB_BASE_URL}/{mt}/{hint_id}", params={"language": "ko-KR", **base_params},
                                    headers=headers, timeout=10)
                if resp.status_code == 200:
                    best_match = resp.json()
                    best_match['media_type'] = mt
                    break

        if not best_match:
            results = perform_search(ct, "ko-KR", "multi", year)
            best_match, all_candidates = rank_results(results, ct, year, pref_mtype)

            if not best_match and year:
                log("TMDB", f"ğŸ”„ ì—°ë„ ì œì™¸ ì¬ê²€ìƒ‰: '{ct}'")
                results = perform_search(ct, "ko-KR", "multi", None)
                best_match, all_candidates = rank_results(results, ct, year, pref_mtype)

            if not best_match:
                # [ìˆ˜ì •] ê´„í˜¸ ì•ˆì˜ ì›ì–´/ì˜ì–´ ì œëª© ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ ì‹œë„
                alt_titles = re.findall(r'[\(\[\{ã€ã€ã€Œï¼ˆ](.*?)[\)\]\}ã€‘ã€ã€ï¼‰]', title)
                for alt in alt_titles:
                    alt = alt.strip()
                    if len(alt) >= 2 and not REGEX_TECHNICAL_TAGS.search(alt):
                        log("TMDB", f"ğŸ”„ ëŒ€ì²´ ì œëª© ê²€ìƒ‰: '{alt}'")
                        results = perform_search(alt, None, "multi", year)
                        best_match, all_candidates = rank_results(results, alt, year, pref_mtype)
                        if best_match: break

            if not best_match:
                # [ê°œì„ ] í•œê¸€ ì œëª©ë§Œ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ (íŠ¹ìˆ˜ë¬¸ì/ì˜ì–´ ì œì™¸)
                ko_only = "".join(re.findall(r'[ê°€-í£\s]+', ct)).strip()
                if ko_only and ko_only != ct and len(ko_only) >= 2:
                    log("TMDB", f"ğŸ”„ í•œê¸€ ë¶€ë¶„ ì¬ê²€ìƒ‰: '{ko_only}'")
                    results = perform_search(ko_only, "ko-KR", "multi", year)
                    best_match, all_candidates = rank_results(results, ko_only, year, pref_mtype)

            if not best_match:
                # [ìˆ˜ì •] ì›ì–´(ì¼ì–´/í•œì) ë¶€ë¶„ ì¶”ì¶œ ê²€ìƒ‰ ì¶”ê°€
                cjk_parts = re.findall(r'[\u3040-\u30ff\u4e00-\u9fff]+', title)
                for part in cjk_parts:
                    if len(part) >= 2:
                        log("TMDB", f"ğŸ”„ ì›ì–´ ë¶€ë¶„ ê²€ìƒ‰: '{part}'")
                        results = perform_search(part, None, "multi", year)
                        best_match, all_candidates = rank_results(results, part, year, pref_mtype)
                        if best_match: break

            if not best_match:
                # [ìˆ˜ì •] í•˜ì´í”ˆ(-)ì´ë‚˜ ì½œë¡ (:)ìœ¼ë¡œ êµ¬ë¶„ëœ ë¶€ë¶„ ê²€ìƒ‰ ì‹œë„
                parts = re.split(r'[-:ï½]', ct)
                if len(parts) > 1:
                    for p in parts:
                        sub_title = p.strip()
                        if len(sub_title) >= 2 and not REGEX_FORBIDDEN_TITLE.match(sub_title):
                            log("TMDB", f"ğŸ”„ ë¶€ë¶„ ì œëª© ê²€ìƒ‰: '{sub_title}'")
                            results = perform_search(sub_title, "ko-KR", "multi", year)
                            best_match, all_candidates = rank_results(results, sub_title, year, pref_mtype)
                            if best_match: break

        if best_match:
            m_type, t_id = best_match.get('media_type') or (
                'movie' if best_match.get('title') else 'tv'), best_match.get('id')
            log("TMDB", f"âœ… ë§¤ì¹­ ì„±ê³µ: '{ct}' -> {m_type}:{t_id}")
            log_matching_success(title, ct, best_match.get('title') or best_match.get('name'), f"{m_type}:{t_id}")
            d_resp = requests.get(
                f"{TMDB_BASE_URL}/{m_type}/{t_id}?language=ko-KR&append_to_response=content_ratings,credits",
                headers=headers, timeout=10).json()

            yv = (d_resp.get('release_date') or d_resp.get('first_air_date') or "").split('-')[0]
            rating = None
            if 'content_ratings' in d_resp:
                res_r = d_resp['content_ratings'].get('results', [])
                kr = next((r['rating'] for r in res_r if r.get('iso_3166_1') == 'KR'), None)
                if kr: rating = f"{kr}+" if kr.isdigit() else kr

            genre_names = [g['name'] for g in d_resp.get('genres', [])] if d_resp.get('genres') else []
            cast_data = d_resp.get('credits', {}).get('cast', [])
            actors = [{"name": c['name'], "profile": c['profile_path'], "role": c['character']} for c in cast_data[:10]]
            crew_data = d_resp.get('credits', {}).get('crew', [])
            director = next((c['name'] for c in crew_data if c.get('job') == 'Director'), "")

            info = {
                "tmdbId": f"{m_type}:{t_id}",
                "genreIds": [g['id'] for g in d_resp.get('genres', [])] if d_resp.get('genres') else [],
                "genreNames": genre_names,
                "director": director,
                "actors": actors,
                "posterPath": d_resp.get('poster_path'),
                "year": yv,
                "overview": d_resp.get('overview'),
                "rating": rating,
                "seasonCount": d_resp.get('number_of_seasons'),
                "failed": False
            }

            if m_type == 'tv':
                info['seasons_data'] = {}
                s_count = d_resp.get('number_of_seasons') or 1
                for s_num in range(1, s_count + 1):
                    s_resp = requests.get(f"{TMDB_BASE_URL}/tv/{t_id}/season/{s_num}?language=ko-KR", headers=headers,
                                          timeout=10).json()
                    if 'episodes' in s_resp:
                        for ep in s_resp['episodes']:
                            info['seasons_data'][f"{s_num}_{ep['episode_number']}"] = {
                                "overview": ep.get('overview'),
                                "air_date": ep.get('air_date'),
                                "still_path": ep.get('still_path')
                            }

            TMDB_MEMORY_CACHE[h] = info
            try:
                conn = get_db()
                conn.execute('INSERT OR REPLACE INTO tmdb_cache (h, data) VALUES (?, ?)', (h, json.dumps(info)))
                conn.commit()
                conn.close()
            except:
                pass
            return info
        else:
            log("TMDB", f"âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: '{ct}'")
            # [ì¶”ê°€] ì§„ë‹¨ ì •ë³´ ì €ì¥
            MATCH_DIAGNOSTICS[title] = {
                "cleaned": ct,
                "year": year,
                "category": category,
                "candidates": all_candidates,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            log_matching_failure(title, ct, "NOT_FOUND_IN_TMDB")
            try:
                conn = get_db()
                conn.execute('INSERT OR REPLACE INTO tmdb_cache (h, data) VALUES (?, ?)',
                             (h, json.dumps({"failed": True})))
                conn.commit()
                conn.close()
            except:
                pass
    except:
        log("TMDB", f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {traceback.format_exc()}")
        log_matching_failure(title, ct, f"API_ERROR: {str(sys.exc_info()[1])}")
    return {"failed": True}


# --- [ìŠ¤ìº” ë° íƒìƒ‰] ---
def scan_recursive_to_db(bp, prefix, category):
    log("SCAN", f"ğŸ“‚ '{category}' íƒìƒ‰ ì¤‘: {bp}")
    base = nfc(get_real_path(bp))
    all_files = []
    stack = [base]
    visited = set()
    while stack:
        curr = stack.pop()
        real_curr = os.path.realpath(curr)
        if real_curr in visited: continue
        visited.add(real_curr)
        try:
            with os.scandir(curr) as it:
                for entry in it:
                    if entry.is_dir():
                        if not any(ex in entry.name for ex in EXCLUDE_FOLDERS) and not entry.name.startswith('.'):
                            stack.append(entry.path)
                    elif entry.is_file() and entry.name.lower().endswith(VIDEO_EXTS):
                        all_files.append(nfc(entry.path))
        except:
            pass

    conn = get_db()
    cursor = conn.cursor()
    cursor.execute('SELECT id, series_path FROM episodes WHERE series_path LIKE ?', (f"{category}/%",))
    db_data = {row['id']: row['series_path'] for row in cursor.fetchall()}
    current_ids = set()
    total = len(all_files)

    set_update_state(is_running=True, task_name=f"ìŠ¤ìº” ({category})", total=total, current=0, success=0, fail=0,
                     clear_logs=True)

    for idx, fp in enumerate(all_files):
        mid = hashlib.md5(fp.encode()).hexdigest()
        current_ids.add(mid)
        rel = nfc(os.path.relpath(fp, base))
        name = os.path.splitext(os.path.basename(fp))[0]
        spath = f"{category}/{rel}"

        with UPDATE_LOCK:
            UPDATE_STATE["current"] += 1
            UPDATE_STATE["current_item"] = name

        ct, yr = clean_title_complex(name)
        cursor.execute(
            'INSERT OR IGNORE INTO series (path, category, name, cleanedName, yearVal) VALUES (?, ?, ?, ?, ?)',
            (spath, category, name, ct, yr))

        if mid not in db_data:
            cursor.execute(
                'INSERT OR REPLACE INTO episodes (id, series_path, title, videoUrl, thumbnailUrl) VALUES (?, ?, ?, ?, ?)',
                (mid, spath, os.path.basename(fp), f"/video_serve?type={prefix}&path={urllib.parse.quote(rel)}",
                 f"/thumb_serve?type={prefix}&id={mid}&path={urllib.parse.quote(rel)}"))
            emit_ui_log(f"ì‹ ê·œ ì¶”ê°€: '{name}'", 'success')
            with UPDATE_LOCK:
                UPDATE_STATE["success"] += 1
        elif db_data[mid] != spath:
            cursor.execute('UPDATE episodes SET series_path = ? WHERE id = ?', (spath, mid))
            emit_ui_log(f"ê²½ë¡œ ê°±ì‹ : '{name}'", 'info')
            with UPDATE_LOCK:
                UPDATE_STATE["success"] += 1
        else:
            with UPDATE_LOCK:
                UPDATE_STATE["success"] += 1  # ì´ë¯¸ ì¡´ì¬

        if (idx + 1) % 2000 == 0:
            conn.commit()

    for rid in (set(db_data.keys()) - current_ids):
        cursor.execute('DELETE FROM episodes WHERE id = ?', (rid,))
    cursor.execute('DELETE FROM series WHERE path NOT IN (SELECT DISTINCT series_path FROM episodes) AND category = ?',
                   (category,))
    conn.commit()
    conn.close()

    set_update_state(is_running=False, current_item="ì‘ì—… ì™„ë£Œ")
    log("SCAN", f"âœ… '{category}' ìŠ¤ìº” ì™„ë£Œ ({total}ê°œ)")


def perform_full_scan():
    log("SYSTEM", f"ğŸš€ ì „ì²´ ìŠ¤ìº” ì‹œì‘ (v{CACHE_VERSION})")
    pk = [("ì˜í™”", "movies"), ("ì™¸êµ­TV", "foreigntv"), ("êµ­ë‚´TV", "koreantv"), ("ì• ë‹ˆë©”ì´ì…˜", "animations_all"), ("ë°©ì†¡ì¤‘", "air")]
    conn = get_db()
    rows = conn.execute("SELECT key FROM server_config WHERE key LIKE 'scan_done_%' AND value = 'true'").fetchall()
    done = [r['key'].replace('scan_done_', '') for r in rows]
    conn.close()

    for label, ck in pk:
        if ck in done: continue
        path, prefix = PATH_MAP[label]
        if os.path.exists(path):
            scan_recursive_to_db(path, prefix, ck)
            conn = get_db()
            conn.execute("INSERT OR REPLACE INTO server_config (key, value) VALUES (?, 'true')", (f'scan_done_{ck}',))
            conn.commit()
            conn.close()
            build_all_caches()

    conn = get_db()
    conn.execute('INSERT OR REPLACE INTO server_config (key, value) VALUES (?, ?)',
                 ('last_scan_version', CACHE_VERSION))
    conn.execute("DELETE FROM server_config WHERE key LIKE 'scan_done_%'")
    conn.commit()
    conn.close()
    threading.Thread(target=fetch_metadata_async, daemon=True).start()


def fetch_metadata_async(force_all=False):
    global IS_METADATA_RUNNING
    if IS_METADATA_RUNNING:
        log("METADATA", "ì´ë¯¸ í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    IS_METADATA_RUNNING = True
    log("METADATA", f"âš™ï¸ ë³‘ë ¬ ë§¤ì¹­ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (force_all={force_all})")
    try:
        conn = get_db()
        # [ë¡œê·¸ ê°•í™”] ì‘ì—… ì‹œì‘ ì „ í˜„ì¬ DB í†µê³„ íŒŒì•…
        cursor = conn.cursor()
        t_all = cursor.execute("SELECT COUNT(*) FROM series").fetchone()[0]
        t_ok = cursor.execute("SELECT COUNT(*) FROM series WHERE tmdbId IS NOT NULL").fetchone()[0]
        t_fail = cursor.execute("SELECT COUNT(*) FROM series WHERE failed = 1").fetchone()[0]
        t_wait = cursor.execute("SELECT COUNT(*) FROM series WHERE tmdbId IS NULL AND failed = 0").fetchone()[0]
        t_rate = (t_ok / t_all * 100) if t_all > 0 else 0

        log("METADATA_STATS",
            f"ğŸ“Š [ì‘ì—… ì „ í†µê³„] ì „ì²´: {t_all} | ì„±ê³µ: {t_ok} | ì‹¤íŒ¨: {t_fail} | ëŒ€ê¸°: {t_wait} | ì„±ê³µë¥ : {round(t_rate, 2)}%")

        if force_all:
            log("METADATA", "ğŸ§¹ ì‹¤íŒ¨í•œ í•­ëª© ì´ˆê¸°í™” ë° ì¬ì‹œë„ ì¤€ë¹„ (ì„±ê³µ í•­ëª© ë³´ì¡´)...")
            conn.execute('UPDATE series SET failed=0 WHERE tmdbId IS NULL')
            conn.commit()

        uncleaned_names_rows = conn.execute(
            'SELECT name FROM series WHERE cleanedName IS NULL AND tmdbId IS NULL AND failed = 0 GROUP BY name').fetchall()
        if uncleaned_names_rows:
            log("METADATA", f"ğŸ§ª ëˆ„ë½ëœ í•­ëª© ì œëª© ì •ì œ ì¤‘ ({len(uncleaned_names_rows)}ê°œ ê³ ìœ  ì œëª©)...")
            cursor = conn.cursor()
            for idx, r in enumerate(uncleaned_names_rows):
                name = r['name']
                ct, yr = clean_title_complex(name)
                cursor.execute('UPDATE series SET cleanedName=?, yearVal=? WHERE name=? AND cleanedName IS NULL',
                               (ct, yr, name))
                if (idx + 1) % 2000 == 0: conn.commit()
            conn.commit()

        # [ìˆ˜ì •] ì´ë¯¸ ë§¤ì¹­ëœ ì‹œë¦¬ì¦ˆë¼ë„ ì—í”¼ì†Œë“œ ì •ë³´(ì‹œì¦Œ ë²ˆí˜¸ ë“±)ê°€ ì—†ëŠ” ê²½ìš° í¬í•¨í•˜ë„ë¡ ì¿¼ë¦¬ ìˆ˜ì •
        all_names_rows = conn.execute('''
            SELECT name, category FROM series
            WHERE failed = 0
            AND (tmdbId IS NULL OR path IN (SELECT series_path FROM episodes WHERE season_number IS NULL))
        ''').fetchall()

        if not all_names_rows:
            conn.close()
            log("METADATA", "âœ… ë§¤ì¹­ ëŒ€ìƒ ì—†ìŒ (ëª¨ë“  ì‘í’ˆì´ ì´ë¯¸ ë§¤ì¹­ë˜ì—ˆê±°ë‚˜ ì‹¤íŒ¨ ì²˜ë¦¬ë¨)")
            IS_METADATA_RUNNING = False
            build_all_caches()
            return

        group_rows = conn.execute('''
            SELECT cleanedName, yearVal, category, MIN(name) as sample_name, GROUP_CONCAT(name, '|') as orig_names
            FROM series
            WHERE failed = 0
            AND (tmdbId IS NULL OR path IN (SELECT series_path FROM episodes WHERE season_number IS NULL))
            AND cleanedName IS NOT NULL
            GROUP BY cleanedName, yearVal, category
        ''').fetchall()
        conn.close()

        tasks = []
        for gr in group_rows:
            tasks.append({
                'clean_title': gr['cleanedName'],
                'year': gr['yearVal'],
                'category': gr['category'],
                'sample_name': gr['sample_name'],
                'orig_names': gr['orig_names'].split('|')
            })

        total = len(tasks)
        log("METADATA", f"ğŸ“Š ê·¸ë£¹í™” ì™„ë£Œ: {total}ê°œì˜ ê³ ìœ  ì‘í’ˆ ì‹ë³„ë¨")

        def process_one(task):
            # [ìˆ˜ì •] ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ ë„˜ê²¨ì£¼ì–´ TV/ì˜í™” êµ¬ë¶„ ë§¤ì¹­
            info = get_tmdb_info_server(task['sample_name'], category=task['category'], ignore_cache=force_all)
            return (task, info)

        batch_size = 50
        total_success = 0
        total_fail = 0
        for i in range(0, total, batch_size):
            batch = tasks[i:i + batch_size]
            log("METADATA", f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ({i + 1}~{min(i + batch_size, total)} / {total})")
            results = []
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_task = {executor.submit(process_one, t): t for t in batch}
                for future in as_completed(future_to_task):
                    results.append(future.result())

            log("METADATA", f"ğŸ’¾ ë°°ì¹˜ {i // batch_size + 1} ê²°ê³¼ DB ë°˜ì˜ ì¤‘...")
            conn = get_db()
            cursor = conn.cursor()
            batch_success = 0
            batch_fail = 0
            for task, info in results:
                orig_names = task['orig_names']
                if info.get('failed'):
                    batch_fail += 1
                    cursor.executemany('UPDATE series SET failed=1 WHERE name=?', [(n,) for n in orig_names])
                else:
                    batch_success += 1
                    up = (
                        info.get('posterPath'), info.get('year'), info.get('overview'),
                        info.get('rating'), info.get('seasonCount'),
                        json.dumps(info.get('genreIds', [])),
                        json.dumps(info.get('genreNames', []), ensure_ascii=False),
                        info.get('director'),
                        json.dumps(info.get('actors', []), ensure_ascii=False),
                        info.get('tmdbId')
                    )
                    cursor.executemany(
                        'UPDATE series SET posterPath=?, year=?, overview=?, rating=?, seasonCount=?, genreIds=?, genreNames=?, director=?, actors=?, tmdbId=?, failed=0 WHERE name=?',
                        [(*up, name) for name in orig_names])

                    if 'seasons_data' in info:
                        eps_to_update = []
                        for name in orig_names:
                            cursor.execute(
                                'SELECT id, title FROM episodes WHERE series_path IN (SELECT path FROM series WHERE name = ?)',
                                (name,))
                            eps_to_update.extend(cursor.fetchall())

                        ep_batch = []
                        for ep_row in eps_to_update:
                            sn, EN = extract_episode_numbers(ep_row['title'])
                            if EN:
                                ei = info['seasons_data'].get(f"{sn}_{EN}")
                                if ei:
                                    # [ìˆ˜ì •] TMDB Still ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ thumbnailUrlì„ í•´ë‹¹ URLë¡œ ì—…ë°ì´íŠ¸
                                    still_url = f"https://image.tmdb.org/t/p/w500{ei.get('still_path')}" if ei.get(
                                        'still_path') else None
                                    ep_batch.append(
                                        (ei.get('overview'), ei.get('air_date'), sn, EN, still_url, ep_row['id']))
                        if ep_batch:
                            cursor.executemany(
                                'UPDATE episodes SET overview=?, air_date=?, season_number=?, episode_number=?, thumbnailUrl=COALESCE(?, thumbnailUrl) WHERE id=?',
                                ep_batch)
                            # [ê°œì„ ] ì–´ë–¤ ì‘í’ˆì˜ ì—í”¼ì†Œë“œê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆëŠ”ì§€ ì´ë¦„ê³¼ ê°œìˆ˜ë¥¼ ëª…í™•íˆ ë¡œê¹…
                            log("METADATA",
                                f"ğŸ“º '{task['sample_name']}' ì—í”¼ì†Œë“œ {Log(len(ep_batch))}ê°œ ì •ë³´ ë° Still ì´ë¯¸ì§€ ì ìš© ì™„ë£Œ")
            conn.commit()
            conn.close()
            total_success += batch_success
            total_fail += batch_fail

            log("METADATA",
                f"ğŸ“ˆ ì§„í–‰ ìƒí™©: âœ… ì´ë²ˆ ë°°ì¹˜ ì„±ê³µ {batch_success} / âŒ ì‹¤íŒ¨ {batch_fail} (ëˆ„ì : âœ… {total_success} / âŒ {total_fail})")

            if (i // batch_size) % 10 == 0:
                log("METADATA", "â™»ï¸ ì¤‘ê°„ ìºì‹œ ê°±ì‹  ì¤‘...")
                build_all_caches()

        build_all_caches()

        # [ë¡œê·¸ ê°•í™”] ì‘ì—… ì¢…ë£Œ í›„ ìµœì¢… í†µê³„ ì¶œë ¥
        conn = get_db()
        cursor = conn.cursor()
        f_ok = cursor.execute("SELECT COUNT(*) FROM series WHERE tmdbId IS NOT NULL").fetchone()[0]
        f_rate = (f_ok / t_all * 100) if t_all > 0 else 0
        conn.close()

        log("METADATA_STATS", f"ğŸŠ ì‘ì—… ì¢…ë£Œ! ì„±ê³µ: {t_ok} -> {f_ok} (ê°œì„ : +{f_ok - t_ok}) | ìµœì¢… ì„±ê³µë¥ : {round(f_rate, 2)}%")
        log("METADATA", f"ğŸ ìµœì¢… ì™„ë£Œ: ì´ âœ… {total_success}ê°œ ì‹ ê·œ ë§¤ì¹­, âŒ {total_fail}ê°œ ì‹¤íŒ¨ ìœ ì§€")
    except:
        log("METADATA", f"âš ï¸ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ: {traceback.format_exc()}")
    finally:
        IS_METADATA_RUNNING = False
        log("METADATA", "ğŸ ë³‘ë ¬ ë§¤ì¹­ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")


def get_sections_for_category(cat, kw=None):
    cache_key = f"sections_{cat}_{kw}"
    if cache_key in _SECTION_CACHE:
        return _SECTION_CACHE[cache_key]

    base_list = _FAST_CATEGORY_CACHE.get(cat, [])
    if not base_list: return []

    # Filter by keyword if provided (e.g., "ë¼í”„í…”", "ì œëª©", "ë“œë¼ë§ˆ")
    target_list = base_list
    is_search = False
    if kw and kw not in ["ì „ì²´", "All", "ì œëª©"]:
        search_kw = kw.strip().lower()
        target_list = [i for i in base_list if search_kw in i['path'].lower() or search_kw in i['name'].lower()]
        is_search = True

    if not target_list: return []

    # ë°©ì†¡ì¤‘(air) ì¹´í…Œê³ ë¦¬ëŠ” ê¸°ì¡´ì²˜ëŸ¼ ì „ì²´ ëª©ë¡ ìœ ì§€
    if cat == 'air':
        sections = [{"title": "ì‹¤ì‹œê°„ ë°©ì˜ ì¤‘", "items": target_list}]
    else:
        sections = []

        # 1. ì˜¤ëŠ˜ì˜ ì¶”ì²œ (ëœë¤ 40ê°œ)
        if len(target_list) > 20:
            random_picks = random.sample(target_list, min(40, len(target_list)))
            sections.append({"title": f"{kw if is_search else ''} ì˜¤ëŠ˜ì˜ ì¶”ì²œ".strip(), "items": random_picks})

        # 2. ìµœì‹  ê³µê°œì‘ (2024ë…„ ì´í›„)
        recent_items = [i for i in target_list if i.get('year') and i['year'] >= '2024']
        if len(recent_items) >= 5:
            sections.append({"title": f"{kw if is_search else ''} ìµœì‹  ê³µê°œì‘".strip(), "items": recent_items[:100]})

        # 3. ì¥ë¥´ë³„ ì„¹ì…˜ (ë°ì´í„° ê¸°ë°˜ ìë™ íë ˆì´ì…˜)
        genre_map = {}
        for item in target_list:
            for g in item.get('genreNames', []):
                if g not in genre_map: genre_map[g] = []
                genre_map[g].append(item)

        # ì•„ì´í…œì´ ë§ì€ ìˆœì„œëŒ€ë¡œ ì¥ë¥´ ì •ë ¬
        sorted_genres = sorted(genre_map.keys(), key=lambda x: len(genre_map[x]), reverse=True)
        # ë„ˆë¬´ í¬ê´„ì ì¸ ì¥ë¥´ëŠ” ì œì™¸í•˜ê³  ìƒìœ„ 3ê°œ ì„ íƒ
        display_genres = [g for g in sorted_genres if g not in ["TV ì˜í™”", "ì• ë‹ˆë©”ì´ì…˜"] or cat != 'animations_all'][:3]

        for g in display_genres:
            g_items = genre_map[g]
            if len(g_items) >= 5:
                title = f"{kw if is_search else ''} ì¸ê¸° {g}".strip()
                # í•´ë‹¹ ì¥ë¥´ ë‚´ì—ì„œë„ ëœë¤í•˜ê²Œ ë…¸ì¶œ
                sections.append({"title": title, "items": random.sample(g_items, min(60, len(g_items)))})

        # 4. ë§ˆì§€ë§‰ì— ì „ì²´ ëª©ë¡ ì¶”ê°€
        sections.append({"title": "ì „ì²´ ëª©ë¡", "items": target_list[:800]})

    _SECTION_CACHE[cache_key] = sections
    return sections


@app.route('/category_sections')
def get_category_sections():
    cat = request.args.get('cat', 'movies')
    kw = request.args.get('kw')
    return gzip_response(get_sections_for_category(cat, kw))


@app.route('/home')
def get_home():
    return gzip_response(HOME_RECOMMEND)


@app.route('/list')
def get_list():
    p = request.args.get('path', '')
    m = {"ì˜í™”": "movies", "ì™¸êµ­TV": "foreigntv", "êµ­ë‚´TV": "koreantv", "ì• ë‹ˆë©”ì´ì…˜": "animations_all", "ë°©ì†¡ì¤‘": "air",
         "movies": "movies", "foreigntv": "foreigntv", "koreantv": "koreantv", "animations_all": "animations_all",
         "air": "air"}
    cat = "movies"
    for label, code in m.items():
        if label in p:
            cat = code
            break
    bl = _FAST_CATEGORY_CACHE.get(cat, [])
    kw = request.args.get('keyword')
    lim = int(request.args.get('limit', 1000))
    off = int(request.args.get('offset', 0))
    res = [item for item in bl if not kw or nfc(kw).lower() in item['path'].lower() or nfc(kw).lower() in item[
        'name'].lower()] if kw and kw not in ["ì „ì²´", "All"] else bl
    return gzip_response(res[off:off + lim])


@app.route('/api/series_detail')
def get_series_detail_api():
    path = request.args.get('path')
    if not path: return gzip_response([])
    for c_path, data in _DETAIL_CACHE:
        if c_path == path: return gzip_response(data)
    conn = get_db()
    row = conn.execute('SELECT * FROM series WHERE path = ?', (path,)).fetchone()
    if not row:
        conn.close()
        return gzip_response([])
    series = dict(row)
    cat = series.get('category')
    for col in ['genreIds', 'genreNames', 'actors']:
        if series.get(col):
            try:
                series[col] = json.loads(series[col])
            except:
                series[col] = []

    # [ìˆ˜ì •] TMDB IDê°€ ê°™ë”ë¼ë„ 'ê°™ì€ ì¹´í…Œê³ ë¦¬' ë‚´ì˜ ì—í”¼ì†Œë“œë§Œ ê°€ì ¸ì˜¤ë„ë¡ ë³€ê²½ (ë“œë¼ë§ˆ/ì˜í™” ì„ì„ ë°©ì§€)
    if series.get('tmdbId'):
        cursor = conn.execute(
            "SELECT e.* FROM episodes e JOIN series s ON e.series_path = s.path WHERE s.tmdbId = ? AND s.category = ?",
            (series['tmdbId'], cat))
    else:
        cursor = conn.execute("SELECT * FROM episodes WHERE series_path = ?", (path,))

    eps = []
    seen = set()
    for r in cursor.fetchall():
        if r['videoUrl'] not in seen:
            eps.append(dict(r))
            seen.add(r['videoUrl'])
    series['movies'] = sorted(eps, key=lambda x: natural_sort_key(x['title']))
    conn.close()

    # [ìµœì í™”] ìƒì„¸ í˜ì´ì§€ ì§„ì… ì‹œë§ˆë‹¤ ë°±ê·¸ë¼ìš´ë“œì—ì„œ FFmpegì„ ëŒë¦¬ë˜ ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
    # ì´ì œ TMDB ìŠ¤í‹¸ ì´ë¯¸ì§€ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ë¯€ë¡œ ë¬´ê±°ìš´ ìƒì„±ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
    # for ep in series['movies']:
    #     THUMB_EXECUTOR.submit(pre_generate_individual_task, ep['thumbnailUrl'])

    _DETAIL_CACHE.append((path, series))
    return gzip_response(series)


def pre_generate_individual_task(ep_thumb_url):
    try:
        u = urllib.parse.urlparse(ep_thumb_url)
        q = urllib.parse.parse_qs(u.query)
        if 'path' in q and 'type' in q and 'id' in q:
            _generate_thumb_file(q['path'][0], q['type'][0], q['id'][0], q.get('t', ['300'])[0], q.get('w', ['320'])[0])
    except:
        pass


@app.route('/search')
def search_videos():
    q = request.args.get('q', '').lower()
    if not q: return jsonify([])
    conn = get_db()
    cursor = conn.execute(
        'SELECT s.*, e.id as ep_id, e.videoUrl, e.thumbnailUrl, e.title FROM series s LEFT JOIN episodes e ON s.path = e.series_path WHERE (s.path LIKE ? OR s.name LIKE ?) GROUP BY s.path ORDER BY s.name ASC',
        (f'%{q}%', f'%{q}%'))
    rows = []
    for row in cursor.fetchall():
        item = dict(row)
        item['movies'] = [
            {"id": item.pop('ep_id'), "videoUrl": item.pop('videoUrl'), "thumbnailUrl": item.pop('thumbnailUrl'),
             "title": item.pop('title')}] if item.get('ep_id') else []
        for col in ['genreIds', 'genreNames', 'actors']:
            if item.get(col):
                try:
                    item[col] = json.loads(item[col])
                except:
                    item[col] = []
        rows.append(item)
    conn.close()
    return gzip_response(rows)


@app.route('/rescan_broken')
def rescan_broken():
    threading.Thread(target=perform_full_scan, daemon=True).start()
    return jsonify({"status": "success"})


@app.route('/rematch_metadata')
def rescan_metadata():
    if IS_METADATA_RUNNING:
        return jsonify({"status": "error", "message": "Metadata process is already running."})
    threading.Thread(target=fetch_metadata_async, args=(True,), daemon=True).start()
    return jsonify({"status": "success", "message": "Scanning for new or failed metadata in background."})


@app.route('/retry_failed_metadata')
def retry_failed_metadata():
    if IS_METADATA_RUNNING:
        return jsonify({"status": "error", "message": "Metadata process is already running."})

    # [ì¶”ê°€] ì¬ì‹œë„ ì‹œ ì´ì „ ì§„ë‹¨ ë¡œê·¸ ì´ˆê¸°í™” (Admin í˜ì´ì§€ ë¡œë”© ë¬¸ì œ í•´ê²°)
    MATCH_DIAGNOSTICS.clear()

    conn = get_db()
    conn.execute('UPDATE series SET failed = 0 WHERE failed = 1')
    conn.commit()
    conn.close()
    # [ìˆ˜ì •] ê°•ì œ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ daemon=Falseë¡œ ì‹¤í–‰í•˜ì—¬ í™•ì‹¤íˆ ì™„ë£Œë˜ë„ë¡ í•¨
    threading.Thread(target=fetch_metadata_async, args=(False,), daemon=False).start()
    return jsonify(
        {"status": "success", "message": "Retrying failed metadata and updating matched series with stills."})


@app.route('/backup_metadata')
def backup_metadata():
    try:
        log("BACKUP", "ë©”íƒ€ë°ì´í„° ë°±ì—… ì‹œì‘...")
        conn = get_db()
        # tmdbIdê°€ ìˆëŠ”(ì„±ê³µí•œ) í•­ëª©ë§Œ ì¡°íšŒ
        cursor = conn.execute('SELECT * FROM series WHERE tmdbId IS NOT NULL')
        rows = cursor.fetchall()
        conn.close()

        backup_data = []
        for row in rows:
            item = dict(row)
            # JSON ë¬¸ìì—´ë¡œ ì €ì¥ëœ í•„ë“œë“¤ì„ ì‹¤ì œ ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (ê°€ë…ì„±/ì¬ì‚¬ìš©ì„± ìœ„í•´)
            for key in ['genreIds', 'genreNames', 'actors']:
                if item.get(key):
                    try:
                        item[key] = json.loads(item[key])
                    except:
                        pass
            backup_data.append(item)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"metadata_backup_{timestamp}.json"
        save_path = os.path.join(SCRIPT_DIR, filename)

        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2)

        log("BACKUP", f"ë°±ì—… ì™„ë£Œ: {save_path} ({len(backup_data)}ê±´)")
        return jsonify({"status": "success", "file": save_path, "count": len(backup_data)})
    except Exception as e:
        log("BACKUP_ERROR", str(e))
        return jsonify({"status": "error", "message": str(e)})


@app.route('/apply_tmdb_thumbnails')
def apply_tmdb_thumbnails():
    threading.Thread(target=run_apply_thumbnails, daemon=True).start()
    return jsonify({"status": "success", "message": "Background task started: Applying TMDB thumbnails to episodes."})


def run_apply_thumbnails():
    log("THUMB_SYNC", "ğŸ”„ TMDB ì¸ë„¤ì¼ ì¼ê´„ ì ìš© ì‹œì‘ (ë¯¸ì²˜ë¦¬ í•­ëª©ë§Œ ìŠ¤ë§ˆíŠ¸ í•„í„°ë§)")

    # 1. ì²˜ìŒë¶€í„° ëª¨ë“  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ì•Šê³ , ì•„ì§ http ì¸ë„¤ì¼ì´ ì•„ë‹Œ ì—í”¼ì†Œë“œë¥¼ ê°€ì§„ ì‹œë¦¬ì¦ˆë§Œ ë½‘ì•„ëƒ…ë‹ˆë‹¤.
    conn = get_db()
    query = """
        SELECT DISTINCT s.path, s.name, s.category, s.tmdbId
        FROM series s
        JOIN episodes e ON s.path = e.series_path
        WHERE s.tmdbId IS NOT NULL
        AND (e.thumbnailUrl IS NULL OR e.thumbnailUrl NOT LIKE 'http%')
    """
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë¦¬ìŠ¤íŠ¸ì— ì™„ì „íˆ ì ì¬
    series_rows = [dict(r) for r in conn.execute(query).fetchall()]
    conn.close()  # ë©”ì¸ ì»¤ë„¥ì…˜ ì¦‰ì‹œ ì¢…ë£Œ (DB Lock ì™„ì „ ì°¨ë‹¨)

    total = len(series_rows)
    log("THUMB_SYNC", f"ğŸ¯ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ ì‹¤ì œ ì‘í’ˆ ìˆ˜: {total}ê°œ (ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª©ì€ ì™„ì „íˆ ìŠ¤í‚µë¨)")

    # [ìˆ˜ì •] UI ìƒíƒœ ì´ˆê¸°í™” (ëŒ€ì‹œë³´ë“œ ì‹œì‘)
    set_update_state(is_running=True, task_name="TMDB ì¸ë„¤ì¼ ì¼ê´„ êµì²´", total=total, current=0, success=0, fail=0,
                     clear_logs=True)

    if total == 0:
        log("THUMB_SYNC", "âœ… ëª¨ë“  ì—í”¼ì†Œë“œ ì¸ë„¤ì¼ì´ ì´ë¯¸ TMDB ì´ë¯¸ì§€ë¡œ ì ìš©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        emit_ui_log("ëª¨ë“  ì—í”¼ì†Œë“œ ì¸ë„¤ì¼ì´ ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.", "success")
        set_update_state(is_running=False, current_item="ì‘ì—… ì™„ë£Œ")
        return

    updated_count = 0

    # 2. í•„í„°ë§ëœ ì§„ì§œ ëŒ€ìƒë“¤ë§Œ ë°˜ë³µ ì²˜ë¦¬
    for idx, s_row in enumerate(series_rows):
        path = s_row['path']
        name = s_row['name']

        # [ìˆ˜ì •] í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ í•­ëª© UI ì „ì†¡
        with UPDATE_LOCK:
            UPDATE_STATE["current"] += 1
            UPDATE_STATE["current_item"] = name

        try:
            tmdb_id_full = s_row['tmdbId']
            if tmdb_id_full and ':' in tmdb_id_full:
                t_id = tmdb_id_full.split(':')[1]
                hint_name = f"{{tmdb-{t_id}}} {name}"

                # ë©”ëª¨ë¦¬/DB ìºì‹œì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (DBë¥¼ ì ê¹ ì½ê³  ë‹«ìœ¼ë¯€ë¡œ ì•ˆì „)
                info = get_tmdb_info_server(hint_name, category=s_row['category'], ignore_cache=False)

                if info and 'seasons_data' in info:
                    # ì“°ê¸°/ì½ê¸°ìš© ì»¤ë„¥ì…˜ì„ í•„ìš”í•  ë•Œë§Œ ì§§ê²Œ ì—½ë‹ˆë‹¤.
                    u_conn = get_db()
                    eps = u_conn.execute(
                        "SELECT id, title, thumbnailUrl FROM episodes WHERE series_path = ? AND (thumbnailUrl IS NULL OR thumbnailUrl NOT LIKE 'http%')",
                        (path,)).fetchall()

                    ep_batch = []
                    for ep in eps:
                        sn, en = extract_episode_numbers(ep['title'])
                        if en:
                            key = f"{sn}_{en}"
                            if key in info['seasons_data']:
                                still = info['seasons_data'][key].get('still_path')
                                if still:
                                    new_url = f"https://image.tmdb.org/t/p/w500{still}"
                                    ep_batch.append((new_url, ep['id']))

                    if ep_batch:
                        u_conn.executemany("UPDATE episodes SET thumbnailUrl = ? WHERE id = ?", ep_batch)
                        u_conn.commit()
                        updated_count += len(ep_batch)
                        with UPDATE_LOCK:
                            UPDATE_STATE["success"] += len(ep_batch)
                        emit_ui_log(f"'{name}' ì—í”¼ì†Œë“œ {len(ep_batch)}ê°œ ì¸ë„¤ì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ", "success")
                    else:
                        # [ì¶”ê°€] ë³€ê²½ ì‚¬í•­ì´ ì—†ì„ ë•Œ ìŠ¤í‚µ ë¡œê·¸ ì¶œë ¥
                        emit_ui_log(f"'{name}' ê±´ë„ˆëœ€ (TMDBì— ìŠ¤í‹¸ì»· ì—†ìŒ ë˜ëŠ” ì´ë¯¸ ì ìš©ë¨)", "info")

                    u_conn.close()  # ë³¼ì¼ì´ ëë‚˜ë©´ ì¦‰ì‹œ ë‹«ìŒ
                else:
                    # [ì¶”ê°€] TMDBì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ëª» ê°€ì ¸ì™”ì„ ë•Œ
                    emit_ui_log(f"'{name}' ê±´ë„ˆëœ€ (TMDBì—ì„œ ì‹œì¦Œ/ì—í”¼ì†Œë“œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ)", "warning")
            else:
                emit_ui_log(f"'{name}' ê±´ë„ˆëœ€ (ìœ íš¨í•œ TMDB ID ì—†ìŒ)", "info")

                with UPDATE_LOCK:
                    UPDATE_STATE["current"] = UPDATE_STATE["success"] + UPDATE_STATE["fail"]
                    UPDATE_STATE["current_item"] = name

        except Exception as e:
            log("THUMB_SYNC", f"Error processing {name}: {e}")
            with UPDATE_LOCK:
                UPDATE_STATE["fail"] += 1
            emit_ui_log(f"'{name}' ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}", "error")

        if (idx + 1) % 50 == 0:
            log("THUMB_SYNC", f"ì§„í–‰ ì¤‘... ({idx + 1}/{total}) - ì´ë²ˆ ì‘ì—…ìœ¼ë¡œ ì—…ë°ì´íŠ¸ëœ ì¸ë„¤ì¼: {updated_count}ê°œ")

    log("THUMB_SYNC", f"âœ… ì™„ë£Œ. ì´ {updated_count}ê°œ ì—í”¼ì†Œë“œ ì¸ë„¤ì¼ ì‹ ê·œ ì—…ë°ì´íŠ¸ ë¨.")
    # [ìˆ˜ì •] ì‘ì—… ì¢…ë£Œ ì²˜ë¦¬
    set_update_state(is_running=False, current_item=f"ì‘ì—… ì™„ë£Œ (ì´ {updated_count}ê°œ êµì²´ë¨)")
    emit_ui_log(f"ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {updated_count}ê°œ êµì²´ë¨)", "success")


FFMPEG_PROCS = {}


def kill_old_processes(sid):
    if sid in FFMPEG_PROCS:
        try:
            FFMPEG_PROCS[sid].terminate()
            FFMPEG_PROCS[sid].wait()
        except:
            pass
        del FFMPEG_PROCS[sid]
    sdir = os.path.join(HLS_ROOT, sid)
    if os.path.exists(sdir):
        try:
            shutil.rmtree(sdir)
        except:
            pass


@app.route('/video_serve')
def video_serve():
    path, prefix = request.args.get('path'), request.args.get('type')
    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
        full_path = get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path))))
        if not os.path.exists(full_path):
            log("VIDEO", f"File not found: {full_path}")
            return "Not Found", 404

        # [iOS HLS Logic]
        ua = request.headers.get('User-Agent', '').lower()
        is_ios = any(x in ua for x in ['iphone', 'ipad', 'apple', 'avfoundation'])

        # [ìˆ˜ì •] ì•ˆë“œë¡œì´ë“œë‚˜ ì—ë®¬ë ˆì´í„°(ExoPlayer)ì—ì„œ 'apple' í‚¤ì›Œë“œë¡œ ì¸í•´ iOSë¡œ ì˜¤íŒë˜ëŠ” í˜„ìƒ ë°©ì§€
        if 'android' in ua or 'exoplayer' in ua:
            is_ios = False

        if is_ios and not full_path.lower().endswith(('.mp4', '.m4v', '.mov')):
            sid = hashlib.md5(full_path.encode()).hexdigest()
            kill_old_processes(sid)

            sdir = os.path.join(HLS_ROOT, sid)
            os.makedirs(sdir, exist_ok=True)
            video_m3u8 = os.path.join(sdir, "video.m3u8")

            if not os.path.exists(video_m3u8):
                cmd = [FFMPEG_PATH, '-y', '-i', full_path, '-c:v', 'libx264', '-preset', 'ultrafast', '-crf', '28',
                       '-sn', '-c:a', 'aac', '-f', 'hls', '-hls_time', '6', '-hls_list_size', '0', video_m3u8]
                FFMPEG_PROCS[sid] = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                for _ in range(40):
                    if os.path.exists(video_m3u8): break
                    time.sleep(0.5)

            return redirect(f"http://{MY_IP}:5000/hls/{sid}/video.m3u8")

        return send_file(full_path, conditional=True)
    except:
        log("VIDEO", f"Error serving video: {traceback.format_exc()}")
        return "Internal Server Error", 500


@app.route('/hls/<sid>/<filename>')
def serve_hls(sid, filename):
    return send_from_directory(os.path.join(HLS_ROOT, sid), filename)


# --- [ìƒˆë¡œìš´ ê³ ì† ë¯¸ë¦¬ë³´ê¸° ì—”ë“œí¬ì¸íŠ¸] ---
@app.route('/preview_serve')
def preview_serve():
    """
    ì˜ìƒ ë¯¸ë¦¬ë³´ê¸°ìš© ê³ ì† ìŠ¤íŠ¸ë¦¬ë°:
    ì›ë³¸ ì˜ìƒì˜ íŠ¹ì • ì§€ì ë¶€í„° ì €í•´ìƒë„(480p)ë¡œ ë¹ ë¥´ê²Œ ì¸ì½”ë”©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
    """
    path_raw, prefix = request.args.get('path'), request.args.get('type')
    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
        vp = get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path_raw))))
        if not os.path.exists(vp): return "Not Found", 404

        # ë¯¸ë¦¬ë³´ê¸° ì‹œì‘ ì§€ì  ì„¤ì • (íŒŒì¼ í¬ê¸°ë‚˜ ê¸¸ì´ì— ë”°ë¼ ì¡°ì ˆ ê°€ëŠ¥, ì—¬ê¸°ì„  1ë¶„ ì§€ì  ì„ í˜¸)
        start_time = "60"

        # FFmpegì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ë‹¤ìš´ìŠ¤ì¼€ì¼ë§ ìŠ¤íŠ¸ë¦¬ë°
        # -ssë¥¼ ì•ì— ë‘ì–´ ê³ ì† íƒìƒ‰, -t 30ìœ¼ë¡œ 30ì´ˆë§Œ ì¶”ì¶œ
        cmd = [
            FFMPEG_PATH, "-ss", start_time, "-i", vp,
            "-t", "30", "-vf", "scale=640:-2",
            "-vcodec", "libx264", "-preset", "ultrafast", "-tune", "zerolatency",
            "-crf", "28", "-an", "-f", "matroska", "pipe:1"
        ]

        def generate():
            proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
            try:
                while True:
                    chunk = proc.stdout.read(65536)
                    if not chunk: break
                    yield chunk
            finally:
                proc.kill()

        return Response(generate(), mimetype='video/x-matroska')
    except:
        return "Error", 500


# --- [ê´€ë¦¬ì ë° ì§„ë‹¨ ë¡œì§ ì¶”ê°€] ---
@app.route('/admin')
def admin_page():
    return """
    <html>
    <head>
        <title>NAS Player Admin - Metadata Failures</title>
        <style>
            body { font-family: sans-serif; background: #141414; color: white; padding: 20px; }
            table { width: 100%; border-collapse: collapse; margin-top: 20px; }
            th, td { padding: 12px; text-align: left; border-bottom: 1px solid #333; }
            th { background: #222; }
            .candidate { font-size: 0.85em; color: #aaa; margin-bottom: 5px; }
            .score { color: #46D369; font-weight: bold; }
            input { padding: 5px; border-radius: 4px; border: 1px solid #444; background: #222; color: white; }
            button { padding: 5px 10px; background: #E50914; color: white; border: none; border-radius: 4px; cursor: pointer; }
            button:hover { background: #b20710; }
            .pagination { margin-top: 20px; text-align: center; }
            .pagination button { margin: 0 5px; }
        </style>
    </head>
    <body>
        <h1>ë©”íƒ€ë°ì´í„° ë§¤ì¹­ ì‹¤íŒ¨ ì§„ë‹¨ ë° ìˆ˜ë™ ìˆ˜ì •</h1>
        <div id="content">ë¡œë”© ì¤‘...</div>
        <div class="pagination">
            <button onclick="prevPage()">ì´ì „</button>
            <span id="pageInfo" style="margin: 0 10px;"></span>
            <button onclick="nextPage()">ë‹¤ìŒ</button>
        </div>
        <script>
            let currentOffset = 0;
            const LIMIT = 50;
            let totalCount = 0;

            async function loadFailures() {
                const resp = await fetch(`/api/admin/diagnostics?offset=${currentOffset}&limit=${LIMIT}`);
                const data = await resp.json();
                totalCount = data.total;

                let html = '<table><tr><th>ì›ë³¸ íŒŒì¼ëª…</th><th>ì •ì œëœ ì œëª©</th><th>TMDB í›„ë³´êµ° (ì ìˆ˜)</th><th>ìˆ˜ë™ ë§¤ì¹­ (Type:ID)</th></tr>';
                for (const [orig, info] of Object.entries(data.items)) {
                    let candHtml = info.candidates.map(c =>
                        `<div class="candidate">${c.title} (${c.year}) - <span class="score">${c.score}ì </span> [${c.type}]</div>`
                    ).join('') || 'í›„ë³´ ì—†ìŒ';

                    html += `<tr>
                        <td>${orig}</td>
                        <td>${info.cleaned} (${info.year || ''})</td>
                        <td>${candHtml}</td>
                        <td>
                            <input type="text" id="id_${btoa(orig)}" placeholder="movie:123 or tv:456">
                            <button onclick="manualMatch('${orig}')">ì ìš©</button>
                        </td>
                    </tr>`;
                }
                html += '</table>';
                document.getElementById('content').innerHTML = html;
                document.getElementById('pageInfo').innerText = `${currentOffset + 1} ~ ${Math.min(currentOffset + LIMIT, totalCount)} / ì´ ${totalCount}ê±´`;
            }

            function prevPage() {
                if (currentOffset - LIMIT >= 0) {
                    currentOffset -= LIMIT;
                    loadFailures();
                }
            }

            function nextPage() {
                if (currentOffset + LIMIT < totalCount) {
                    currentOffset += LIMIT;
                    loadFailures();
                }
            }

            async function manualMatch(orig) {
                const val = document.getElementById('id_' + btoa(orig)).value;
                if (!val.includes(':')) { alert('í˜•ì‹ ì˜¤ë¥˜! movie:ID ë˜ëŠ” tv:ID ë¡œ ì…ë ¥í•˜ì„¸ìš”.'); return; }
                const [type, id] = val.split(':');
                const resp = await fetch('/api/admin/manual_match', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({orig_name: orig, type: type, tmdb_id: id})
                });
                const res = await resp.json();
                if (res.status === 'success') { alert('ìˆ˜ì • ì™„ë£Œ!'); loadFailures(); }
                else { alert('ì—ëŸ¬: ' + res.message); }
            }
            loadFailures();
        </script>
    </body>
    </html>
    """


@app.route('/api/admin/diagnostics')
def get_diagnostics():
    offset = int(request.args.get('offset', 0))
    limit = int(request.args.get('limit', 50))

    all_items = list(MATCH_DIAGNOSTICS.items())
    total_count = len(all_items)
    paged_items = all_items[offset: offset + limit]

    return jsonify({
        "total": total_count,
        "items": dict(paged_items),
        "offset": offset,
        "limit": limit
    })


@app.route('/api/admin/manual_match', methods=['POST'])
def manual_match():
    data = request.json
    orig_name = data.get('orig_name')
    m_type = data.get('type')
    t_id = data.get('tmdb_id')

    if not all([orig_name, m_type, t_id]):
        return jsonify({"status": "error", "message": "Missing data"})

    try:
        headers = {"Authorization": f"Bearer {TMDB_API_KEY}"}
        d_resp = requests.get(
            f"{TMDB_BASE_URL}/{m_type}/{t_id}?language=ko-KR&append_to_response=content_ratings,credits",
            headers=headers, timeout=10).json()

        if 'id' not in d_resp:
            return jsonify({"status": "error", "message": "TMDB ID not found"})

        yv = (d_resp.get('release_date') or d_resp.get('first_air_date') or "").split('-')[0]
        rating = None
        if 'content_ratings' in d_resp:
            res_r = d_resp['content_ratings'].get('results', [])
            kr = next((r['rating'] for r in res_r if r.get('iso_3166_1') == 'KR'), None)
            if kr: rating = f"{kr}+" if kr.isdigit() else kr

        genre_names = [g['name'] for g in d_resp.get('genres', [])]
        cast_data = d_resp.get('credits', {}).get('cast', [])
        actors = [{"name": c['name'], "profile": c['profile_path'], "role": c['character']} for c in cast_data[:10]]
        crew_data = d_resp.get('credits', {}).get('crew', [])
        director = next((c['name'] for c in crew_data if c.get('job') == 'Director'), "")

        info = {
            "tmdbId": f"{m_type}:{t_id}",
            "genreIds": [g['id'] for g in d_resp.get('genres', [])],
            "genreNames": genre_names,
            "director": director,
            "actors": actors,
            "posterPath": d_resp.get('poster_path'),
            "year": yv,
            "overview": d_resp.get('overview'),
            "rating": rating,
            "seasonCount": d_resp.get('number_of_seasons'),
            "failed": False
        }

        conn = get_db()
        cursor = conn.cursor()
        up = (
            info['posterPath'], info['year'], info['overview'],
            info['rating'], info['seasonCount'],
            json.dumps(info['genreIds']),
            json.dumps(info['genreNames'], ensure_ascii=False),
            info['director'],
            json.dumps(info['actors'], ensure_ascii=False),
            info['tmdbId']
        )
        cursor.execute(
            'UPDATE series SET posterPath=?, year=?, overview=?, rating=?, seasonCount=?, genreIds=?, genreNames=?, director=?, actors=?, tmdbId=?, failed=0 WHERE name=?',
            (*up, orig_name))
        conn.commit()
        conn.close()

        if orig_name in MATCH_DIAGNOSTICS:
            del MATCH_DIAGNOSTICS[orig_name]

        build_all_caches()
        return jsonify({"status": "success"})
    except Exception as e:
        return jsonify({"status": "error", "message": "Missing data"})


def _generate_thumb_file(path_raw, prefix, tid, t, w):
    tp = os.path.join(DATA_DIR, f"seek_{tid}_{t}_{w}.jpg")
    if os.path.exists(tp): return tp

    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
        vp = get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path_raw))))
        if not os.path.exists(vp):
            log("THUMB", f"Video not found for thumb: {vp}")
            return None

        with THUMB_SEMAPHORE:
            if os.path.exists(tp): return tp
            log("THUMB", f"Generating thumb: {os.path.basename(vp)} at {t}s")

            try:
                # [ìˆ˜ì •] íƒ€ì„ì•„ì›ƒì„ 30ì´ˆë¡œ ëŠ˜ë¦¬ê³  ì˜ˆì™¸ ì²˜ë¦¬ë¥¼ ì¶”ê°€í•˜ì—¬ ì„œë²„ ì•ˆì •ì„± í™•ë³´
                result = subprocess.run([
                    FFMPEG_PATH, "-y",
                    "-lowres", "1",  # 1/2 í•´ìƒë„ ë””ì½”ë”© (ì†ë„ íšê¸°ì  í–¥ìƒ)
                    "-ss", str(t),
                    "-noaccurate_seek",
                    "-i", vp,
                    "-frames:v", "1",
                    "-map", "0:v:0",
                    "-an", "-sn",
                    "-q:v", "8",  # í’ˆì§ˆë³´ë‹¤ ì†ë„ ìš°ì„ 
                    "-vf", f"scale={w}:-1:flags=fast_bilinear",
                    "-threads", "1",
                    tp
                ], timeout=30, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)

                if result.returncode != 0:
                    log("THUMB_ERROR", f"FFmpeg failed: {result.stderr.decode()}")
            except subprocess.TimeoutExpired:
                log("THUMB_TIMEOUT", f"FFmpeg timed out for: {os.path.basename(vp)} (GDRIVE ì§€ì—° ì˜ì‹¬)")
            except Exception as e:
                log("THUMB_ERROR", f"Unexpected error: {str(e)}")

        return tp if os.path.exists(tp) else None
    except:
        log("THUMB_ERROR", f"Exception: {traceback.format_exc()}")
        return None


@app.route('/thumb_serve')
def thumb_serve():
    path, prefix, tid = request.args.get('path'), request.args.get('type'), request.args.get('id')
    t = request.args.get('t', default="300")
    w = request.args.get('w', default="480")

    tp = _generate_thumb_file(path, prefix, tid, t, w)
    if tp and os.path.exists(tp):
        resp = make_response(send_file(tp, mimetype='image/jpeg'))
        resp.headers['Cache-Control'] = 'public, max-age=31536000, immutable'
        return resp
    return "Not Found", 404


# --- [ë³µì›ëœ ê¸°ëŠ¥: ìŠ¤í‚µ ë„¤ë¹„ê²Œì´ì…˜ìš© ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„±] ---
def get_video_duration(path):
    try:
        result = subprocess.run([FFPROBE_PATH, "-v", "error", "-show_entries", "format=duration", "-of",
                                 "default=noprint_wrappers=1:nokey=1", path], stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE)
        return float(result.stdout)
    except:
        return 0


@app.route('/storyboard')
def gen_seek_thumbnails():
    """
    ë¹„ë””ì˜¤ì˜ ì „ì²´ êµ¬ê°„ì„ ë¯¸ë¦¬ë³¼ ìˆ˜ ìˆëŠ” ìŠ¤í”„ë¼ì´íŠ¸ ì‹œíŠ¸(ìŠ¤í† ë¦¬ë³´ë“œ)ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    ExoPlayer ë“± í´ë¼ì´ì–¸íŠ¸ì—ì„œ íƒìƒ‰ ë°”(seek bar) ì´ë™ ì‹œ ì¸ë„¤ì¼ì„ í‘œì‹œí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    path_raw = request.args.get('path')
    prefix = request.args.get('type')
    if not path_raw or not prefix: return "Bad Request", 400

    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
        vp = get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path_raw))))
        if not os.path.exists(vp): return "Not Found", 404

        # íŒŒì¼ëª… ê¸°ë°˜ í•´ì‹œ ìƒì„± (ìºì‹œìš©)
        file_hash = hashlib.md5(vp.encode()).hexdigest()
        sb_path = os.path.join(DATA_DIR, f"sb_{file_hash}.jpg")

        # ì´ë¯¸ ìƒì„±ëœ ìŠ¤í† ë¦¬ë³´ë“œê°€ ìˆìœ¼ë©´ ë°˜í™˜
        if os.path.exists(sb_path):
            resp = make_response(send_file(sb_path, mimetype='image/jpeg'))
            resp.headers['Cache-Control'] = 'public, max-age=31536000, immutable'
            return resp

        # ìƒì„± ì¤‘ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ ë½(Lock)
        with STORYBOARD_SEMAPHORE:
            if os.path.exists(sb_path):  # ë½ íšë“ í›„ ë‹¤ì‹œ í™•ì¸
                return send_file(sb_path, mimetype='image/jpeg')

            duration = get_video_duration(vp)
            if duration == 0: return "Duration Error", 500

            # 10x10 ê·¸ë¦¬ë“œ, 100ê°œì˜ ì¸ë„¤ì¼ ìƒì„±
            interval = duration / 100

            # FFmpeg ëª…ë ¹ì–´ë¡œ íƒ€ì¼(Sprite Sheet) ìƒì„±
            # fps=1/interval: interval ì´ˆë§ˆë‹¤ 1í”„ë ˆì„ ì¶”ì¶œ
            # scale=160:-1: ë„ˆë¹„ 160pxë¡œ ë¦¬ì‚¬ì´ì§• (ë†’ì´ ë¹„ìœ¨ ìœ ì§€)
            # tile=10x10: 10í–‰ 10ì—´ë¡œ í•©ì¹˜ê¸°
            cmd = [
                FFMPEG_PATH, "-y",
                "-i", vp,
                "-vf", f"fps=1/{interval},scale=160:-1,tile=10x10",
                "-frames:v", "1",
                "-q:v", "5",  # JPEG í’ˆì§ˆ
                sb_path
            ]

            log("STORYBOARD", f"ì¸ë„¤ì¼ ìƒì„± ì‹œì‘: {os.path.basename(vp)} (Duration: {duration}s)")
            subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, timeout=60)

            if os.path.exists(sb_path):
                log("STORYBOARD", f"ìƒì„± ì™„ë£Œ: {os.path.basename(sb_path)}")
                return send_file(sb_path, mimetype='image/jpeg')
            else:
                log("STORYBOARD", "ìƒì„± ì‹¤íŒ¨")
                return "Generation Failed", 500
    except Exception as e:
        log("STORYBOARD", f"ì—ëŸ¬ ë°œìƒ: {str(e)}")
        return "Internal Server Error", 501


@app.route('/api/status')
def get_server_status():
    try:
        conn = get_db()
        eps = conn.execute("SELECT COUNT(*) FROM episodes").fetchone()[0]
        ser = conn.execute("SELECT COUNT(*) FROM series").fetchone()[0]
        mtch = conn.execute("SELECT COUNT(*) FROM series WHERE tmdbId IS NOT NULL").fetchone()[0]
        fail = conn.execute("SELECT COUNT(*) FROM series WHERE failed = 1").fetchone()[0]

        # --- [ì¶”ê°€/ìˆ˜ì •ëœ ë¶€ë¶„: ì„œë¸Œì¹´í…Œê³ ë¦¬ í¬í•¨ ìŠ¤í‹¸ì»· í˜„í™©] ---
        # 1. ì¸ë„¤ì¼ì´ ë³€ê²½ëœ(http) ì—í”¼ì†Œë“œë§Œ ìš°ì„  ì¡°íšŒ
        applied_eps = conn.execute("""
            SELECT series_path, COUNT(id) as tmdb_eps
            FROM episodes
            WHERE thumbnailUrl LIKE 'http%'
            GROUP BY series_path
        """).fetchall()

        applied_map = {row['series_path']: row['tmdb_eps'] for row in applied_eps}

        # 2. TMDB ë§¤ì¹­ëœ ì‘í’ˆì˜ ê¸°ë³¸ ì •ë³´(ì „ì²´ íšŒì°¨ìˆ˜, ì´ë¦„, ì¹´í…Œê³ ë¦¬, ìƒì„¸ê²½ë¡œ) ì¡°íšŒ
        series_info = conn.execute("""
            SELECT s.path, s.name, s.category, COUNT(e.id) as total_eps
            FROM series s
            JOIN episodes e ON s.path = e.series_path
            WHERE s.tmdbId IS NOT NULL
            GROUP BY s.path
        """).fetchall()

        stills_applied = []
        for row in series_info:
            path = row['path']
            tmdb_eps = applied_map.get(path, 0)
            if tmdb_eps > 0:
                # ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ë¡œì§ (ì˜ˆ: air/ë¼í”„í…” ì• ë‹ˆë©”ì´ì…˜/ë‚˜í˜¼ë ™ -> ë°©ì†¡ì¤‘ > ë¼í”„í…” ì• ë‹ˆë©”ì´ì…˜)
                # category_name ë§¤í•‘
                cat_map = {"movies": "ì˜í™”", "foreigntv": "ì™¸êµ­TV", "koreantv": "êµ­ë‚´TV", "animations_all": "ì• ë‹ˆë©”ì´ì…˜",
                           "air": "ë°©ì†¡ì¤‘"}
                main_cat_ko = cat_map.get(row['category'], row['category'])

                parts = path.split('/')
                sub_cat = parts[1] if len(parts) > 2 else "ì¼ë°˜"
                display_cat = f"{main_cat_ko} > {sub_cat}"

                stills_applied.append({
                    "name": row['name'],
                    "category": display_cat,
                    "applied": f"{tmdb_eps}/{row['total_eps']}"
                })

        # ë³´ê¸° ì¢‹ê²Œ ì¹´í…Œê³ ë¦¬, ì´ë¦„ìˆœìœ¼ë¡œ ì •ë ¬
        stills_applied.sort(key=lambda x: (x['category'], x['name']))
        # --- [ì¶”ê°€ëœ ë¶€ë¶„ ë] ---

        conn.close()
        return jsonify({
            "total_episodes": eps,
            "total_series": ser,
            "matched_series": mtch,
            "failed_series": fail,
            "success_rate": f"{round(mtch / ser * 100, 1)}%" if ser > 0 else "0%",
            "stills_applied_count": len(stills_applied),
            "stills_applied_series": stills_applied
        })
    except:
        return jsonify({"error": traceback.format_exc()})

# --- [ìë§‰ ê¸°ëŠ¥: ë¹„ë™ê¸° ì¶”ì¶œ, ê²€ìƒ‰ ë¡œì§ ê°œì„ , íƒ€ì„ì•„ì›ƒ í•´ê²° ë²„ì „] ---
def run_subtitle_extraction(vp, rel_path, sub_to_extract):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìë§‰ì„ ì¶”ì¶œí•˜ê³ , ì™„ë£Œ í›„ ì‘ì—… ëª©ë¡ì—ì„œ ì œê±°í•©ë‹ˆë‹¤."""
    with EXTRACTION_LOCK:
        ACTIVE_EXTRACTIONS.add(rel_path)
    try:
        stream_index = sub_to_extract['index']
        lang = sub_to_extract.get('tags', {}).get('language', 'und').lower()
        lang_suffix = 'ko' if lang in ['ko', 'kor'] else 'en' if lang in ['en', 'eng'] else 'und'

        video_rel_hash = hashlib.md5(rel_path.encode()).hexdigest()
        subtitle_filename = f"{video_rel_hash}.{lang_suffix}.srt"
        subtitle_full_path = os.path.join(SUBTITLE_DIR, subtitle_filename)

        if not os.path.exists(subtitle_full_path):
            log("SUBTITLE", f"ë°±ê·¸ë¼ìš´ë“œ ì¶”ì¶œ ì‹œì‘: ìŠ¤íŠ¸ë¦¼ #{stream_index} ({lang}) -> {subtitle_filename}")
            extract_cmd = [FFMPEG_PATH, "-y", "-nostdin", "-i", vp, "-vn", "-an", "-map", f"0:{stream_index}", "-c:s",
                           "srt", subtitle_full_path]
            try:
                subprocess.run(extract_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,
                               timeout=300)
                log("SUBTITLE", f"ë°±ê·¸ë¼ìš´ë“œ ì¶”ì¶œ ì„±ê³µ: {subtitle_filename}")
            except subprocess.CalledProcessError as e:
                log("SUBTITLE_FAIL", f"ì¶”ì¶œ ì‹¤íŒ¨. FFmpeg Code: {e.returncode}")
            except subprocess.TimeoutExpired:
                log("SUBTITLE_FAIL", f"ì¶”ì¶œ ì‹œê°„ ì´ˆê³¼ (300ì´ˆ): {subtitle_filename}")
    except Exception as e:
        log("SUBTITLE_ERROR", f"ë°±ê·¸ë¼ìš´ë“œ ìë§‰ ì¶”ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {traceback.format_exc()}")
    finally:
        # ì‘ì—…ì´ ì„±ê³µí•˜ë“  ì‹¤íŒ¨í•˜ë“ , ëª©ë¡ì—ì„œ ì œê±°
        with EXTRACTION_LOCK:
            ACTIVE_EXTRACTIONS.discard(rel_path)
            log("SUBTITLE", f"ì¶”ì¶œ ì‘ì—… ì™„ë£Œ ë° ì •ë¦¬: {os.path.basename(rel_path)}")


@app.route('/api/subtitle_info')
def get_subtitle_info_api():
    path_raw, prefix = request.args.get('path'), request.args.get('type')
    try:
        base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
        rel_path = nfc(urllib.parse.unquote(path_raw))

        # [ìˆ˜ì •] ì§„í–‰ ì¤‘ì¸ ì‘ì—…ì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
        with EXTRACTION_LOCK:
            if rel_path in ACTIVE_EXTRACTIONS:
                log("SUBTITLE", f"ì¶”ì¶œ ì§„í–‰ ì¤‘... í´ë¼ì´ì–¸íŠ¸ì— ëŒ€ê¸° ì‹ í˜¸ ì „ì†¡: {os.path.basename(rel_path)}")
                return jsonify({"external": [], "embedded": [], "extraction_triggered": True})

        vp = get_real_path(os.path.join(base, rel_path))
        video_filename = os.path.basename(vp)
        video_name_no_ext = os.path.splitext(video_filename)[0]

        log("SUBTITLE", f"ìë§‰ ì¡°íšŒ ì‹œì‘: {video_filename}")
        external, embedded = [], []

        parent_dir = os.path.dirname(vp)
        if os.path.exists(parent_dir):
            for f in os.listdir(parent_dir):
                f_nfc = nfc(f)
                if f_nfc.lower().endswith(('.srt', '.smi', '.ass', '.vtt')):
                    sub_name_no_ext = os.path.splitext(f_nfc)[0]
                    if video_name_no_ext.startswith(sub_name_no_ext) or sub_name_no_ext.startswith(video_name_no_ext):
                        if f_nfc != video_filename:
                            external.append(
                                {"name": f_nfc, "path": nfc(os.path.join(os.path.dirname(rel_path), f_nfc))})

        video_rel_hash = hashlib.md5(rel_path.encode()).hexdigest()
        if os.path.exists(SUBTITLE_DIR):
            for f in os.listdir(SUBTITLE_DIR):
                if f.startswith(video_rel_hash):
                    display_name = f.replace(f"{video_rel_hash}.", f"{video_name_no_ext}.")
                    external.append({"name": display_name, "path": f"__SUBTITLE_DIR__/{f}"})

        if not external:
            try:
                cmd = [FFPROBE_PATH, "-v", "error", "-show_streams", "-of", "json", vp]
                result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=True,
                                        timeout=60)
                all_streams = json.loads(result.stdout).get('streams', [])
                embedded = [s for s in all_streams if
                            s.get('codec_type') == 'subtitle' or 'sub' in s.get('codec_name', '').lower()]
            except Exception as e:
                log("SUBTITLE_ERROR", f"ë‚´ì¥ ìë§‰ ë¶„ì„ ì‹¤íŒ¨: {e}")

        final_subs = sorted(list({sub['path']: sub for sub in external}.values()), key=lambda x: x['name'])
        log("SUBTITLE", f"íƒìƒ‰ ì™„ë£Œ - ìµœì¢… ì™¸ë¶€ ìë§‰: {len(final_subs)}ê°œ, ë‚´ì¥ ìë§‰: {len(embedded)}ê°œ")

        extraction_started = False
        if not final_subs and embedded:
            sub_to_extract = min(embedded, key=lambda s: {'ko': 1, 'kor': 1, 'en': 2, 'eng': 2}.get(
                s.get('tags', {}).get('language', 'und').lower(), 99))
            if sub_to_extract:
                with EXTRACTION_LOCK:
                    if rel_path not in ACTIVE_EXTRACTIONS:
                        log("SUBTITLE", "ì™¸ë¶€ ìë§‰ ì—†ìŒ. ë‚´ì¥ ìë§‰ ë°±ê·¸ë¼ìš´ë“œ ì¶”ì¶œì„ ì˜ˆì•½í•©ë‹ˆë‹¤.")
                        SUBTITLE_EXECUTOR.submit(run_subtitle_extraction, vp, rel_path, sub_to_extract)
                extraction_started = True

        return jsonify({"external": final_subs, "embedded": embedded, "extraction_triggered": extraction_started})
    except Exception as e:
        log("SUBTITLE_ERROR", f"ìë§‰ ì •ë³´ ì¡°íšŒ ì¤‘ ì—ëŸ¬: {traceback.format_exc()}")
        return jsonify({"error": str(e), "external": [], "embedded": [], "extraction_triggered": False})

@app.route('/api/subtitle_extract')
def subtitle_extract():
    path_raw = request.args.get('path')
    prefix = request.args.get('type')
    index = request.args.get('index')
    sub_path = request.args.get('sub_path')

    try:
        if sub_path:
            full_sub_path = None
            if sub_path.startswith("__SUBTITLE_DIR__/"):
                filename = sub_path.replace("__SUBTITLE_DIR__/", "")
                full_sub_path = get_real_path(os.path.join(SUBTITLE_DIR, filename))
            else:
                base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
                full_sub_path = get_real_path(os.path.join(base, nfc(urllib.parse.unquote(sub_path))))

            if full_sub_path and os.path.exists(full_sub_path):
                return send_file(full_sub_path)
            else:
                log("SUBTITLE_ERROR", f"ì™¸ë¶€ ìë§‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {full_sub_path}")
                return "Not Found", 404

        if index is not None:
            base = next(v[0] for k, v in PATH_MAP.items() if v[1] == prefix)
            vp = get_real_path(os.path.join(base, nfc(urllib.parse.unquote(path_raw))))

            cmd = [FFMPEG_PATH, "-y", "-nostdin", "-i", vp, "-map", f"0:{index}", "-f", "srt", "pipe:1"]

            def generate():
                proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
                try:
                    while True:
                        chunk = proc.stdout.read(4096)
                        if not chunk: break
                        yield chunk
                finally:
                    proc.kill()

            return Response(generate(), mimetype='text/plain; charset=utf-8')

        return "Bad Request", 400
    except Exception as e:
        log("SUBTITLE_ERROR", f"ìë§‰ ì „ì†¡ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}\n{traceback.format_exc()}")
        return "Internal Server Error", 500

def pre_extract_movie_subtitles():
    """ì˜í™” ì¹´í…Œê³ ë¦¬ì—ì„œ ìë§‰ì´ ì—†ëŠ” ì˜ìƒë§Œ ëŒ€ìƒìœ¼ë¡œ ìë§‰ ì‚¬ì „ ì¶”ì¶œì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    set_update_state(is_running=True, task_name="ì˜í™” ìë§‰ ì‚¬ì „ ì¶”ì¶œ", total=0, current=0, success=0, fail=0, clear_logs=True)
    log("SUBTITLE_PRE", "ì˜í™” ì¹´í…Œê³ ë¦¬ ìë§‰ ì‚¬ì „ ì¶”ì¶œ ì‹œì‘...")
    emit_ui_log("ì˜í™” ì¹´í…Œê³ ë¦¬ì—ì„œ ìë§‰ì´ ì—†ëŠ” í•­ëª©ì„ ì°¾ì•„ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.", "info")

    try:
        conn = get_db()
        # [ìˆ˜ì •] 'episodes' í…Œì´ë¸”ì—ì„œ 'path'ê°€ ì•„ë‹Œ 'videoUrl' ì»¬ëŸ¼ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
        video_rows = conn.execute("SELECT videoUrl FROM episodes WHERE series_path LIKE 'movies/%'").fetchall()
        conn.close()

        total_videos = len(video_rows)
        set_update_state(total=total_videos)
        log("SUBTITLE_PRE", f"ì‚¬ì „ ì¶”ì¶œ ëŒ€ìƒ ì˜í™” ìˆ˜: {total_videos}ê°œ")
        emit_ui_log(f"ì „ì²´ ì˜í™” {total_videos}ê°œë¥¼ ëŒ€ìƒìœ¼ë¡œ ê²€ì‚¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.", "info")

        base_movie_path = PATH_MAP.get("ì˜í™”", (None, None))[0]
        if not base_movie_path:
            log("SUBTITLE_PRE_ERROR", "ì˜í™” ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            emit_ui_log("ì„¤ì •ì—ì„œ ì˜í™” ì¹´í…Œê³ ë¦¬ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "error")
            return

        extraction_queued_count = 0

        for idx, row in enumerate(video_rows):
            with UPDATE_LOCK:
                UPDATE_STATE["current"] = idx + 1

            # [ìˆ˜ì •] row['path'] -> row['videoUrl']
            video_url = row['videoUrl']
            rel_path = video_url.replace('/video_serve?type=movie&path=', '')
            rel_path = nfc(urllib.parse.unquote(rel_path))

            vp = get_real_path(os.path.join(base_movie_path, rel_path))
            video_filename = os.path.basename(rel_path)

            with UPDATE_LOCK:
                UPDATE_STATE["current_item"] = video_filename

            if not os.path.exists(vp):
                emit_ui_log(f"íŒŒì¼ ì—†ìŒ, ê±´ë„ˆëœ€: {video_filename}", "warning")
                with UPDATE_LOCK: UPDATE_STATE["fail"] += 1
                continue

            # --- [í•µì‹¬ ë¡œì§] ì´ë¯¸ ìë§‰ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸ ---
            parent_dir = os.path.dirname(vp)
            video_name_no_ext = os.path.splitext(video_filename)[0]

            has_external = False

            # 1. ì›ë³¸ í´ë” í™•ì¸
            if os.path.exists(parent_dir):
                for f in os.listdir(parent_dir):
                    if f.lower().endswith(('.srt', '.smi', '.ass', '.vtt')):
                        sub_name_no_ext = os.path.splitext(f)[0]
                        if video_name_no_ext.startswith(sub_name_no_ext) or sub_name_no_ext.startswith(
                                video_name_no_ext):
                            has_external = True;
                            break

            if has_external:
                with UPDATE_LOCK: UPDATE_STATE["success"] += 1
                continue

            # 2. ì„œë²„ ìºì‹œ í´ë” í™•ì¸
            video_rel_hash = hashlib.md5(rel_path.encode()).hexdigest()
            if os.path.exists(SUBTITLE_DIR):
                for f in os.listdir(SUBTITLE_DIR):
                    if f.startswith(video_rel_hash):
                        has_external = True;
                        break

            if has_external:
                with UPDATE_LOCK: UPDATE_STATE["success"] += 1
                continue

            # --- ìë§‰ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì•„ë˜ ë¡œì§ ì‹¤í–‰ ---
            try:
                cmd = [FFPROBE_PATH, "-v", "error", "-show_streams", "-of", "json", vp]
                result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=True,
                                        timeout=60)
                all_streams = json.loads(result.stdout).get('streams', [])
                embedded = [s for s in all_streams if
                            s.get('codec_type') == 'subtitle' or 'sub' in s.get('codec_name', '').lower()]
            except Exception:
                embedded = []

            if embedded:
                sub_to_extract = min(embedded, key=lambda s: {'ko': 1, 'kor': 1, 'en': 2, 'eng': 2}.get(
                    s.get('tags', {}).get('language', 'und').lower(), 99))
                with EXTRACTION_LOCK:
                    if rel_path not in ACTIVE_EXTRACTIONS:
                        SUBTITLE_EXECUTOR.submit(run_subtitle_extraction, vp, rel_path, sub_to_extract)
                        extraction_queued_count += 1
                        emit_ui_log(f"ë‚´ì¥ ìë§‰ ë°œê²¬, ì¶”ì¶œ ì˜ˆì•½: {video_filename}", "success")

            if (idx + 1) % 100 == 0:
                log("SUBTITLE_PRE", f"ì§„í–‰ ìƒí™©: {idx + 1}/{total_videos} ê²€ì‚¬ ì™„ë£Œ, {extraction_queued_count}ê°œ ì¶”ì¶œ ì˜ˆì•½ë¨.")

    except Exception as e:
        log("SUBTITLE_PRE_ERROR", f"ì‚¬ì „ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {traceback.format_exc()}")
        emit_ui_log(f"ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}", "error")
    finally:
        set_update_state(is_running=False, current_item="ì˜í™” ìë§‰ ì‚¬ì „ ì¶”ì¶œ ì™„ë£Œ")
        log("SUBTITLE_PRE", f"ì˜í™” ì¹´í…Œê³ ë¦¬ ìë§‰ ì‚¬ì „ ì¶”ì¶œ ì™„ë£Œ. ì´ {extraction_queued_count}ê°œì˜ ìë§‰ ì¶”ì¶œ ì‘ì—…ì„ ì˜ˆì•½í–ˆìŠµë‹ˆë‹¤.")
        emit_ui_log(f"ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ì‹ ê·œ ì¶”ì¶œ {extraction_queued_count}ê°œ)", "success")


@app.route('/pre_extract_subtitles')
def pre_extract_subtitles_route():
    # ë‹¤ë¥¸ ì‘ì—…ì´ ì‹¤í–‰ ì¤‘ì¼ ë•ŒëŠ” ìƒˆ ì‘ì—…ì„ ì‹œì‘í•˜ì§€ ì•ŠìŒ
    if UPDATE_STATE.get("is_running", False):
        return jsonify({"status": "error", "message": "ë‹¤ë¥¸ ì‘ì—…ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."}), 409

    threading.Thread(target=pre_extract_movie_subtitles, daemon=True).start()
    return jsonify({"status": "success", "message": "ì˜í™” ìë§‰ ì‚¬ì „ ì¶”ì¶œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤."})


@app.route('/admin_stills')
def admin_stills_page():
    return """
    <html>
    <head>
        <title>NAS Player - TMDB ìŠ¤í‹¸ì»· ì ìš© í˜„í™©</title>
        <style>
            body { font-family: sans-serif; background: #141414; color: white; padding: 20px; }
            h1 { margin-bottom: 5px; }
            .summary { font-size: 1.1em; color: #46D369; margin-bottom: 20px; font-weight: bold; }
            table { width: 100%; border-collapse: collapse; margin-top: 20px; font-size: 0.9em; }
            th, td { padding: 10px; text-align: left; border-bottom: 1px solid #333; }
            th { background: #222; position: sticky; top: 0; }
            .tag { display: inline-block; padding: 3px 8px; border-radius: 4px; font-size: 0.85em; font-weight: bold; background: #333; color: #ccc; border: 1px solid #555;}
        </style>
    </head>
    <body>
        <h1>TMDB ìŠ¤í‹¸ì»· ì ìš© ì‘í’ˆ ëª©ë¡</h1>
        <div class="summary" id="summary">ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...</div>
        <div id="content"></div>
        <script>
            async function loadData() {
                try {
                    const resp = await fetch('/api/status');
                    const data = await resp.json();

                    document.getElementById('summary').innerText = `ì´ ${data.stills_applied_count}ê°œì˜ ì‘í’ˆì— ìŠ¤í‹¸ì»·ì´ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤. (ë§¤ì¹­ëœ ì´ ì‘í’ˆ ìˆ˜: ${data.matched_series}ê°œ)`;

                    let html = '<table><tr><th>ì¹´í…Œê³ ë¦¬</th><th>ì‘í’ˆëª…</th><th>ìŠ¤í‹¸ì»· ë°˜ì˜ ì—í”¼ì†Œë“œ ë¹„ìœ¨</th></tr>';
                    data.stills_applied_series.forEach(item => {
                        html += `<tr>
                            <td><span class="tag">${item.category}</span></td>
                            <td>${item.name}</td>
                            <td>${item.applied}</td>
                        </tr>`;
                    });
                    html += '</table>';
                    document.getElementById('content').innerHTML = html;
                } catch (e) {
                    document.getElementById('summary').innerText = "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.";
                }
            }
            loadData();
        </script>
    </body>
    </html>
    """

# --- [UI/ìºì‹œ ë¡œì§ ë³´ì¡´] ---
@app.route('/updater')
def updater_ui():
    return """
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>NAS Player - ë©”íƒ€ë°ì´í„° ëª¨ë‹ˆí„°ë§</title>
        <style>
            body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: #f4f6f9; color: #333; margin: 0; padding: 20px; }
            .container { max-width: 900px; margin: 0 auto; background: white; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); padding: 30px; }
            h1 { font-size: 24px; color: #2c3e50; border-bottom: 2px solid #ecf0f1; padding-bottom: 15px; margin-top: 0; display: flex; align-items: center; gap: 10px; }
            .btn-group { margin-bottom: 25px; display: flex; gap: 10px; flex-wrap: wrap; }
            button { background: #6c757d; color: white; border: none; padding: 12px 20px; border-radius: 6px; cursor: pointer; font-size: 14px; font-weight: bold; transition: opacity 0.2s; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
            button:hover { opacity: 0.8; }
            button.btn-primary { background: #007bff; }
            button.btn-success { background: #28a745; }
            button.btn-warning { background: #ffc107; color: #212529; }
            .status-box { background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 8px; padding: 20px; margin-bottom: 20px; }
            .status-header { font-size: 18px; font-weight: bold; margin-bottom: 15px; color: #34495e; display: flex; justify-content: space-between;}
            .task-badge { background: #e9ecef; color: #495057; padding: 3px 10px; border-radius: 15px; font-size: 13px; font-weight: bold;}
            .progress-container { background: #e9ecef; border-radius: 8px; height: 20px; width: 100%; overflow: hidden; margin-bottom: 15px; }
            .progress-bar { background: #28a745; height: 100%; width: 0%; transition: width 0.3s; }
            .stats { display: flex; justify-content: space-between; font-size: 14px; font-weight: bold; color: #495057; }
            .stats span.success { color: #28a745; }
            .stats span.fail { color: #dc3545; }
            .terminal { background: #1e1e1e; border-radius: 8px; padding: 15px; height: 500px; overflow-y: auto; font-family: 'Consolas', 'Courier New', monospace; font-size: 13px; line-height: 1.6; color: #d4d4d4; box-shadow: inset 0 2px 5px rgba(0,0,0,0.5); }
            .log-success { color: #4CAF50; }
            .log-error { color: #F44336; }
            .log-info { color: #9E9E9E; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>âš™ï¸ ë©”íƒ€ë°ì´í„° ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ëª¨ë‹ˆí„°</h1>

            <div class="btn-group">
                <button class="btn-primary" onclick="triggerTask('/retry_failed_metadata')">â†» ì‹¤íŒ¨ ë©”íƒ€ë°ì´í„° ì¬ë§¤ì¹­</button>
                <button class="btn-success" onclick="triggerTask('/apply_tmdb_thumbnails')">ğŸ–¼ï¸ TMDB ì¸ë„¤ì¼ ì¼ê´„ êµì²´</button>
                <button class ="btn-primary" style="background-color: #343a40;" onclick="triggerTask('/pre_extract_subtitles')"> ğŸ¬ ì˜í™” ìë§‰ ì¼ê´„ ì¶”ì¶œ </button>
                <button class="btn-warning" onclick="triggerTask('/rematch_metadata')">âš ï¸ ì „ì²´ ê°•ì œ ì¬ìŠ¤ìº”</button>
                <button class="btn-info" onclick="window.open('/admin_stills', '_blank')" style="background-color: #17a2b8;">ğŸ“Š ìŠ¤í‹¸ì»· ì ìš© í™•ì¸</button>
            </div>

            <div class="status-box">
                <div class="status-header">
                    <span id="statusText">ì²˜ë¦¬ ì¤‘: ëŒ€ê¸° ì¤‘</span>
                    <span class="task-badge" id="taskName">ëŒ€ê¸° ì¤‘</span>
                </div>

                <div class="progress-container">
                    <div class="progress-bar" id="progressBar"></div>
                </div>

                <div class="stats">
                    <div>
                        <span id="progressCount">0 / 0</span>
                        <span id="progressPercent" style="margin-left: 10px; color: #007bff;">0%</span>
                    </div>
                    <div>
                        <span class="success" id="successCount">ì„±ê³µ: 0</span> &nbsp;|&nbsp;
                        <span class="fail" id="failCount">ì‹¤íŒ¨: 0</span>
                    </div>
                </div>
            </div>

            <div class="terminal" id="terminalBox"></div>
        </div>

        <script>
            async function triggerTask(url) {
                if (confirm('ì‘ì—…ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ë©° ëª¨ë‹ˆí„°ë§ ì°½ì— ë°˜ì˜ë©ë‹ˆë‹¤)')) {
                    await fetch(url);
                }
            }

            async function updateStatus() {
                try {
                    const res = await fetch('/api/updater/status');
                    const data = await res.json();

                    document.getElementById('statusText').innerText = data.is_running ? `ì²˜ë¦¬ ì¤‘: ${data.current_item}` : `ì™„ë£Œë¨: ${data.current_item}`;
                    document.getElementById('taskName').innerText = data.task_name;

                    const percent = data.total > 0 ? Math.round((data.current / data.total) * 100) : 0;
                    document.getElementById('progressBar').style.width = percent + '%';

                    if(!data.is_running && data.total > 0) {
                        document.getElementById('progressBar').style.width = '100%';
                    }

                    document.getElementById('progressCount').innerText = `${data.current.toLocaleString()} / ${data.total.toLocaleString()}`;
                    document.getElementById('progressPercent').innerText = `${percent}%`;
                    document.getElementById('successCount').innerText = `ì„±ê³µ: ${data.success.toLocaleString()}`;
                    document.getElementById('failCount').innerText = `ì‹¤íŒ¨: ${data.fail.toLocaleString()}`;

                    const term = document.getElementById('terminalBox');

                    if (data.logs.length > 0) {
                        let html = '';
                        data.logs.forEach(log => {
                            let cssClass = 'log-info';
                            let icon = 'â„¹ï¸';

                            if (log.type === 'success') { cssClass = 'log-success'; icon = 'âœ…'; }
                            else if (log.type === 'error') { cssClass = 'log-error'; icon = 'âŒ'; }
                            else if (log.type === 'warning') { cssClass = 'log-error'; icon = 'âš ï¸'; }

                            html += `<div class="${cssClass}">${log.time} ${icon} [UPDATE] ${log.msg}</div>`;
                        });

                        const isScrolledToBottom = term.scrollHeight - term.clientHeight <= term.scrollTop + 50;
                        term.innerHTML = html;

                        if (isScrolledToBottom) {
                            term.scrollTop = term.scrollHeight;
                        }
                    }
                } catch (e) {
                    console.error('Failed to fetch status:', e);
                }
            }

            setInterval(updateStatus, 500);
            updateStatus();
        </script>
    </body>
    </html>
    """


@app.route('/api/updater/status')
def get_updater_status():
    with UPDATE_LOCK:
        logs = list(UPDATE_STATE['logs'])
        return jsonify({
            "is_running": UPDATE_STATE['is_running'],
            "task_name": UPDATE_STATE['task_name'],
            "total": UPDATE_STATE['total'],
            "current": UPDATE_STATE['current'],
            "success": UPDATE_STATE['success'],
            "fail": UPDATE_STATE['fail'],
            "current_item": UPDATE_STATE['current_item'],
            "logs": logs
        })


def build_all_caches():
    global _SECTION_CACHE
    _SECTION_CACHE = {}
    _rebuild_fast_memory_cache()
    build_home_recommend()


def _rebuild_fast_memory_cache():
    global _FAST_CATEGORY_CACHE
    temp = {}
    conn = get_db()
    log("CACHE", "âš™ï¸ ê²½ëŸ‰ ë©”ëª¨ë¦¬ ìºì‹œ ë¹Œë“œ ì‹œì‘")
    for cat in ["movies", "foreigntv", "koreantv", "animations_all", "air"]:
        rows_dict = {}
        all_rows = conn.execute(
            'SELECT path, name, posterPath, year, rating, genreIds, genreNames, director, actors, tmdbId, cleanedName, yearVal, overview FROM series WHERE category = ? ORDER BY name ASC',
            (cat,)).fetchall()
        for row in all_rows:
            path, name, poster, year, rating, g_ids, g_names, director, actors, t_id, c_name, y_val, overview = row
            if not poster and cat != 'air': continue
            if c_name is not None:
                ct, yr = c_name, y_val
            else:
                ct, yr = clean_title_complex(name)
            group_key = f"tmdb:{t_id}" if t_id else f"name:{ct}_{yr}"
            if group_key not in rows_dict:
                try:
                    genre_list = json.loads(g_names) if g_names else []
                except:
                    genre_list = []
                try:
                    genre_ids = json.loads(g_ids) if g_ids else []
                except:
                    genre_ids = []
                try:
                    actors_list = json.loads(actors) if actors else []
                except:
                    actors_list = []
                rows_dict[group_key] = {
                    "path": path, "name": name, "posterPath": poster,
                    "year": year, "rating": rating, "genreIds": genre_ids, "genreNames": genre_list,
                    "director": director, "actors": actors_list, "tmdbId": t_id, "overview": overview, "movies": []
                }
        temp[cat] = list(rows_dict.values())
    conn.close()
    _FAST_CATEGORY_CACHE = temp
    log("CACHE", "âœ… ê²½ëŸ‰ ë©”ëª¨ë¦¬ ìºì‹œ ë¹Œë“œ ì™„ë£Œ")


def build_home_recommend():
    global HOME_RECOMMEND
    try:
        m = _FAST_CATEGORY_CACHE.get('movies', [])
        k = _FAST_CATEGORY_CACHE.get('koreantv', [])
        a = _FAST_CATEGORY_CACHE.get('air', [])
        combined = m + k
        unique_map = {}
        for item in combined:
            uid = item.get('tmdbId') or item.get('path')
            if uid not in unique_map: unique_map[uid] = item
        unique_hot_list = list(unique_map.values())
        hot_picks = random.sample(unique_hot_list, min(100, len(unique_hot_list))) if unique_hot_list else []
        seen_ids = {(p.get('tmdbId') or p.get('path')) for p in hot_picks}
        airing_picks = []
        for item in a:
            uid = item.get('tmdbId') or item.get('path')
            if uid not in seen_ids:
                airing_picks.append(item)
                if len(airing_picks) >= 100: break
        HOME_RECOMMEND = [
            {"title": "ì§€ê¸ˆ ê°€ì¥ í•«í•œ ì¸ê¸°ì‘", "items": hot_picks},
            {"title": "ì‹¤ì‹œê°„ ë°©ì˜ ì¤‘", "items": airing_picks}
        ]
        log("CACHE", f"ğŸ  í™ˆ ì¶”ì²œ ë¹Œë“œ ì™„ë£Œ ({len(hot_picks)} / {len(airing_picks)})")
    except:
        traceback.print_exc()


def background_init_tasks():
    build_all_caches()


if __name__ == '__main__':
    init_db()
    threading.Thread(target=background_init_tasks, daemon=True).start()
    app.run(host='0.0.0.0', port=5000, threaded=True)